{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorsiDanilo/big-data-computing-project/blob/main/2_BDC_Project_Bitcoin_price_forecasting_(Model_preparation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e452Ydj7chNV"
      },
      "source": [
        "# Bitcoin price forecasting with PySpark\n",
        "## Big Data Computing final project - A.Y. 2022 - 2023\n",
        "Prof. Gabriele Tolomei\n",
        "\n",
        "MSc in Computer Science\n",
        "\n",
        "La Sapienza, University of Rome\n",
        "\n",
        "### Author\n",
        "Corsi Danilo - corsi.1742375@studenti.uniroma1.it\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFEbOtkdTI0"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The cryptocurrency Bitcoin has attracted the attention of many people in recent years. However, it's\n",
        "price fluctuation can be extremely unpredictable, which makes it difficult to predict when the right\n",
        "time to buy or sell this digital currency will be. In this context, forecasting Bitcoin prices can be a\n",
        "competitive advantage for investors and traders, as it could allow them to make informed decisions\n",
        "on the right time to enter or exit the market. In this project, I will analyze some machine learning\n",
        "techniques to understand, through the processing of historical data, how accurately the price of Bitcoin\n",
        "can be predicted and whether this can provide added value to cryptocurrency investors and traders.\n",
        "### Dataset\n",
        "I chose to use the following dataset from Kaggle Bitcoin Historical Dataset, more specifically those\n",
        "containing minute-by-minute updates of the Bitcoin price from 2017 to 2021 (period for which there\n",
        "were moments of high volatility but also a lot of price lateralisation). The columns (features) contained\n",
        "in it, in addition to the timestamp of each transaction, are the opening, closing, highest and lowest\n",
        "price and the corresponding trading volume in Bitcoin and Dollars.\n",
        "### Methods (TODO: da scegliere per bene)\n",
        "The methods I will test will be Linear Regression (simple and multiple) and Random Forest. Further\n",
        "comparisons with other classification models are planned in the course of development. Moreover, I\n",
        "would also like to try to understand what the differences are between these methods and the imple-\n",
        "mentation of a state-of-the-art neural network such as Long-Short Term Memory.\n",
        "### Evaluation framework (TODO: vedi quali usare in base ai paper/esempi e ai modelli utilizzati)\n",
        "As evaluation framework I will use R-square (R²), Mean Square Error (MSE) and Mean Absolute\n",
        "Error (MAE) to get a complete picture of the performance of the various models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtmWjQUVSvq2"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkQaY6VAf4v_"
      },
      "source": [
        "## Global Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L7-8QQvKf5CR"
      },
      "outputs": [],
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_DATASET_DIR = GDRIVE_DIR + \"/MyDrive/Computer_Science/BDC/project/datasets\"\n",
        "\n",
        "GDRIVE_DATASET_NAME = \"BTC-2017_2021_cleaned\"\n",
        "# GDRIVE_DATASET_NAME = \"BTC-2017_2021_1000000_cleaned\"\n",
        "# GDRIVE_DATASET_NAME = \"BTC-2017_2021_500000_cleaned\"\n",
        "# GDRIVE_DATASET_NAME = \"BTC-2015_2023_cleaned\"\n",
        "# GDRIVE_DATASET_NAME = \"BTC-Hourly_cleaned\"\n",
        "\n",
        "GDRIVE_DATASET_NAME_EXT = \"/\" + GDRIVE_DATASET_NAME + \".csv\"\n",
        "\n",
        "GDRIVE_DATASET = GDRIVE_DATASET_DIR + GDRIVE_DATASET_NAME_EXT\n",
        "\n",
        "SLOW_OPERATION = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRW_JLjSvrA"
      },
      "source": [
        "## Install PySpark and related dependencies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si82CaUYSvrA",
        "outputId": "2949d510-fafc-41b7-ae95-a49a106ba5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=ffab0fe01a6aa089f1971eb5d2f9b03075dd39e38accd5e13a115ae6b58711ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1\n",
        "!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXTGTgzEqE3x"
      },
      "source": [
        "##  Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PX7xDYw4SvrB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvwgAvGSvrB"
      },
      "source": [
        "##  Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fhi5bmOeSvrB"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n",
        "                setAppName(\"BitcoinPriceForecasting\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9prfkJSvrB"
      },
      "source": [
        "##  Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "au2-MqC-SvrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b04425-ee4d-425a-b64f-f9a6afa62c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsgE-WfSvrB"
      },
      "source": [
        "##  Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8X312aarSvrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "7f53afd0-e3e3-4c9d-86de-2d67fc50d09b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f730420a0b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://bb5e7ee7b1d9:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BitcoinPriceForecasting</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GEvgsXRkSvrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f26905-3eb3-49d0-c287-9b27722eb8ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.host', 'bb5e7ee7b1d9'),\n",
              " ('spark.driver.memory', '45G'),\n",
              " ('spark.kryoserializer.buffer.max', '1G'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.app.startTime', '1686326504065'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.name', 'BitcoinPriceForecasting'),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.app.submitTime', '1686326503722'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.app.id', 'local-1686326507343'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.driver.port', '38525'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpaf8RMIcP4a"
      },
      "source": [
        "# **Model preparation**\n",
        "\n",
        "Prepara i dati: Assicurati che il tuo dataset sia in un formato adatto per l'addestramento del modello. Dovresti avere una colonna di etichette di output (variabile di risposta) e le features (variabili indipendenti) in colonne separate.\n",
        "\n",
        "Crea un VectorAssembler: Un VectorAssembler è utilizzato per combinare le features in una singola colonna vettoriale. Questo passaggio è necessario poiché PySpark richiede che le features siano in un unico vettore per l'addestramento del modello Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Agk8Cum2OdQz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "c39cfb98-9595-4e6d-ef4d-45531a28475d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0355d20f986b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load dataset into pyspark dataframe objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df = spark.read.load(GDRIVE_DATASET, \n\u001b[0m\u001b[1;32m      3\u001b[0m                          \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/drive/MyDrive/Computer_Science/BDC/project/datasets/BTC-2017_2021_cleaned.csv."
          ]
        }
      ],
      "source": [
        "# load dataset into pyspark dataframe objects\n",
        "df = spark.read.load(GDRIVE_DATASET, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = df\n",
        "dataframe.printSchema()\n",
        "dataframe.show(5)"
      ],
      "metadata": {
        "id": "pGHOC9sFMZ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5-oVeurnfGf"
      },
      "outputs": [],
      "source": [
        "def model_preparation(dataframe):  \n",
        "  from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "  assembler = VectorAssembler(\n",
        "      inputCols=[\"close\"],  # Colonna del prezzo di chiusura\n",
        "      outputCol=\"features\"  # Colonna vettoriale risultante\n",
        "  )\n",
        "\n",
        "  dataframe = assembler.transform(dataframe)\n",
        "\n",
        "  from pyspark.sql.functions import date_format, to_timestamp\n",
        "\n",
        "  # transform date column into string\n",
        "  dataframe = dataframe.withColumn(\"date_str\", date_format(to_timestamp(\"date\", \"yyyy-MM-dd HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "  # encode the date to a column of label indicies\n",
        "  from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "  label_stringIdx = StringIndexer(inputCol = 'date_str', outputCol = 'labelIndex')\n",
        "  dataframe = label_stringIdx.fit(dataframe).transform(dataframe)\n",
        "\n",
        "  # dividi il dataset in train set e test set\n",
        "  from pyspark.sql.functions import percent_rank\n",
        "  from pyspark.sql import Window\n",
        "\n",
        "  dataframe = dataframe.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"date_str\")))\n",
        "  train_df = dataframe.where(\"rank <= .8\").drop(\"rank\", \"date_str\")\n",
        "  test_df = dataframe.where(\"rank > .8\").drop(\"rank\", \"date_str\")\n",
        "\n",
        "  if(SLOW_OPERATION):\n",
        "    print(\"The shape of the train set is {:d} rows by {:d} columns\".format(train_df.count(), len(train_df.columns)))\n",
        "    train_df.printSchema()\n",
        "    train_df.show(5)\t\n",
        "\n",
        "    print(\"The shape of the test set is {:d} rows by {:d} columns\".format(test_df.count(), len(test_df.columns)))\n",
        "    test_df.printSchema()\n",
        "    test_df.show(5)\t\n",
        "\n",
        "  return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = model_preparation(df)"
      ],
      "metadata": {
        "id": "Fe1u7N0RNTc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJPBT9vs_zRZ"
      },
      "outputs": [],
      "source": [
        "def compute_avg_train_test(dataframe):\n",
        "  dataframe = dataframe.drop(\"features\", \"labelIndex\")\n",
        "  \n",
        "  dataframe = dataframe.withColumn(\"date\", date_format(dataframe.date, \"yyyy-MM-dd\")).groupBy(\"date\").agg(\n",
        "      avg(\"close\").alias(\"avg_close\")\n",
        "  ).sort(\"date\")\n",
        "\n",
        "  dataframe = dataframe.withColumn(\"avg_close\", round(dataframe[\"avg_close\"], 2))\n",
        "\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGlxw9wPW77V"
      },
      "outputs": [],
      "source": [
        "def show_avg_train_test(train_df, test_df):\n",
        "  avg_train_df_pandas = compute_avg_train_test(train_df).toPandas()\n",
        "  avg_test_df_pandas = compute_avg_train_test(test_df).toPandas()\n",
        "\n",
        "  trace1 = go.Scatter(\n",
        "      x = avg_train_df_pandas['date'],\n",
        "      y = avg_train_df_pandas['avg_close'].astype(float),\n",
        "      mode = 'lines',\n",
        "      name = 'Train set'\n",
        "  )\n",
        "\n",
        "  trace2 = go.Scatter(\n",
        "      x = avg_test_df_pandas['date'],\n",
        "      y = avg_test_df_pandas['avg_close'].astype(float),\n",
        "      mode = 'lines',\n",
        "      name = 'Test set'\n",
        "  )\n",
        "  \n",
        "  layout = dict(\n",
        "      title='Train and Test set with the Slider ',\n",
        "      xaxis=dict(\n",
        "          rangeselector=dict(\n",
        "              buttons=list([\n",
        "                  #change the count to desired amount of months.\n",
        "                  dict(count=1,\n",
        "                      label='1m',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=6,\n",
        "                      label='6m',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=12,\n",
        "                      label='1y',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=36,\n",
        "                      label='3y',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(step='all')\n",
        "              ])\n",
        "          ),\n",
        "          rangeslider=dict(\n",
        "              visible = True\n",
        "          ),\n",
        "          type='date'\n",
        "      )\n",
        "  )\n",
        "\n",
        "  data = [trace1,trace2]\n",
        "  fig = dict(data=data, layout=layout)\n",
        "  iplot(fig, filename = \"Train and Test set  with Rangeslider\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uuZJSHkRY12"
      },
      "outputs": [],
      "source": [
        "show_avg_train_test(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the final train and test datasets"
      ],
      "metadata": {
        "id": "4-owrgEPHQ7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output(dataframe):\n",
        "  from pyspark.sql.functions import date_format, to_timestamp, col\n",
        "\n",
        "  # transform date column into string\n",
        "  dataframe = dataframe.withColumn(\"date\", to_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"string\"))\n",
        "\n",
        "  # definition of Vector to String conversion function\n",
        "  vector_to_string = udf(lambda vector: str(vector), StringType())\n",
        "\n",
        "  # applying the function to the features column\n",
        "  dataframe = dataframe.withColumn(\"features\", vector_to_string(dataframe[\"features\"]))\n",
        "\n",
        "  # save the dataset in CSV format\n",
        "  dataframe.repartition(1).write.csv(GDRIVE_DATASET_DIR + '/output', header=True, mode='overwrite')  # Sostituisci 'output.csv' con il percorso e il nome desiderato per il file di output\n",
        "\n",
        "  import os\n",
        "  import glob\n",
        "  import time\n",
        "\n",
        "  while True:\n",
        "      csv_files = glob.glob(os.path.join(GDRIVE_DATASET_DIR + '/output', \"*.csv\"))\n",
        "      if len(csv_files) > 0:\n",
        "          # .csv file found!\n",
        "          file_path = csv_files[0]\n",
        "          break\n",
        "      else:\n",
        "          print(\".csv file not found. I'll try again after 1 second...\")\n",
        "          time.sleep(1)\n",
        "  print(\".csv file found:\", file_path)\n",
        "\n",
        "  new_file_name = GDRIVE_DATASET_NAME + \"_cleaned.csv\"\n",
        "  # rename the file\n",
        "  new_file_path = os.path.join(os.path.dirname(file_path), new_file_name)\n",
        "  os.rename(file_path, new_file_path)\n",
        "\n",
        "  # move the file to the destination folder\n",
        "  new_file_destination = os.path.join(GDRIVE_DATASET_DIR, new_file_name)\n",
        "  os.rename(new_file_path, new_file_destination)\n",
        "\n",
        "  # import shutil\n",
        "\n",
        "  # # remove the output folder\n",
        "  # shutil.rmtree(GDRIVE_DATASET_DIR + '/output')\n",
        "\n",
        "  print(\"File renamed and moved successfully!\")"
      ],
      "metadata": {
        "id": "miO1DZVW1D9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output(train_df, \"train\")\n",
        "output(test_df, \"test\")"
      ],
      "metadata": {
        "id": "G8mHxapmSAC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e452Ydj7chNV",
        "zK1eMlqXWioB"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}