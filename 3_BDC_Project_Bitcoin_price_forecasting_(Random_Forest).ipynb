{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorsiDanilo/big-data-computing-project/blob/main/3_BDC_Project_Bitcoin_price_forecasting_(Random_Forest).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e452Ydj7chNV"
      },
      "source": [
        "# Bitcoin price forecasting with PySpark\n",
        "## Big Data Computing final project - A.Y. 2022 - 2023\n",
        "Prof. Gabriele Tolomei\n",
        "\n",
        "MSc in Computer Science\n",
        "\n",
        "La Sapienza, University of Rome\n",
        "\n",
        "### Author\n",
        "Corsi Danilo - corsi.1742375@studenti.uniroma1.it\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFEbOtkdTI0"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The cryptocurrency Bitcoin has attracted the attention of many people in recent years. However, it's\n",
        "price fluctuation can be extremely unpredictable, which makes it difficult to predict when the right\n",
        "time to buy or sell this digital currency will be. In this context, forecasting Bitcoin prices can be a\n",
        "competitive advantage for investors and traders, as it could allow them to make informed decisions\n",
        "on the right time to enter or exit the market. In this project, I will analyze some machine learning\n",
        "techniques to understand, through the processing of historical data, how accurately the price of Bitcoin\n",
        "can be predicted and whether this can provide added value to cryptocurrency investors and traders.\n",
        "### Dataset\n",
        "I chose to use the following dataset from Kaggle Bitcoin Historical Dataset, more specifically those\n",
        "containing minute-by-minute updates of the Bitcoin price from 2017 to 2021 (period for which there\n",
        "were moments of high volatility but also a lot of price lateralisation). The columns (features) contained\n",
        "in it, in addition to the timestamp of each transaction, are the opening, closing, highest and lowest\n",
        "price and the corresponding trading volume in Bitcoin and Dollars.\n",
        "### Methods (TODO: da scegliere per bene)\n",
        "The methods I will test will be Linear Regression (simple and multiple) and Random Forest. Further\n",
        "comparisons with other classification models are planned in the course of development. Moreover, I\n",
        "would also like to try to understand what the differences are between these methods and the imple-\n",
        "mentation of a state-of-the-art neural network such as Long-Short Term Memory.\n",
        "### Evaluation framework (TODO: vedi quali usare in base ai paper/esempi e ai modelli utilizzati)\n",
        "As evaluation framework I will use R-square (R²), Mean Square Error (MSE) and Mean Absolute\n",
        "Error (MAE) to get a complete picture of the performance of the various models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Constants\n"
      ],
      "metadata": {
        "id": "AkQaY6VAf4v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: da sistemare\n",
        "\n",
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_DATASET_OUTPUT_DIR = GDRIVE_DIR + \"/MyDrive/Computer_Science/BDC/project/datasets/output\"\n",
        "GDRIVE_DATASET_TEMP_DIR = GDRIVE_DIR + \"/MyDrive/Computer_Science/BDC/project/datasets/temp\"\n",
        "\n",
        "GDRIVE_DATASET_NAME_TRAIN = \"800K_cleaned_train\"\n",
        "GDRIVE_DATASET_NAME_TEST = \"800K_cleaned_test\"\n",
        "GDRIVE_DATASET_NAME_EXT_TRAIN  = \"/\" + GDRIVE_DATASET_NAME_TRAIN + \".csv\"\n",
        "GDRIVE_DATASET_NAME_EXT_TEST = \"/\" + GDRIVE_DATASET_NAME_TEST + \".csv\"\n",
        "\n",
        "GDRIVE_DATASET_TRAIN = GDRIVE_DATASET_OUTPUT_DIR + GDRIVE_DATASET_NAME_EXT_TRAIN\n",
        "GDRIVE_DATASET_TEST = GDRIVE_DATASET_OUTPUT_DIR + GDRIVE_DATASET_NAME_EXT_TEST\n",
        "\n",
        "SLOW_OPERATION = False"
      ],
      "metadata": {
        "id": "L7-8QQvKf5CR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Import useful Python packages"
      ],
      "metadata": {
        "id": "IXTGTgzEqE3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PX7xDYw4SvrB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtmWjQUVSvq2"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRW_JLjSvrA"
      },
      "source": [
        "## Install PySpark and related dependencies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Si82CaUYSvrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c56b3e-1c7c-44a6-d10f-2aa5a9d2099d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=7262ef33fc20414e5805f9e65ff420b19c8b5508c2d0786b309f637e3395b311\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1\n",
        "!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvwgAvGSvrB"
      },
      "source": [
        "##  Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fhi5bmOeSvrB"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"BitcoinPriceForecasting\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9prfkJSvrB"
      },
      "source": [
        "##  Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "au2-MqC-SvrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380d54f5-0027-4b43-c9e6-4867c89c0100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsgE-WfSvrB"
      },
      "source": [
        "##  Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8X312aarSvrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "d7dd6427-3c03-416e-fccf-500cdddc271d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f66ef7f0e50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ad6879868ba1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BitcoinPriceForecasting</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GEvgsXRkSvrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e31a82-5375-4bd7-ea75-93e1666c25ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.startTime', '1686410597406'),\n",
              " ('spark.driver.host', 'ad6879868ba1'),\n",
              " ('spark.driver.memory', '45G'),\n",
              " ('spark.app.id', 'local-1686410600414'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.name', 'BitcoinPriceForecasting'),\n",
              " ('spark.app.submitTime', '1686410597157'),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.driver.port', '39327'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMCpzz2fe9Vi"
      },
      "source": [
        "# Random Forest\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.tree.RandomForest.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset into pyspark dataframe objects\n",
        "train_df = spark.read.load(GDRIVE_DATASET_TRAIN, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                    )\n",
        "\n",
        "test_df = spark.read.load(GDRIVE_DATASET_TEST, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "yiF49OXz4-i7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SLOW_OPERATION:\n",
        "  print(\"The shape of the train dataset is {:d} rows by {:d} columns\".format(train_df.count(), len(train_df.columns)))\n",
        "  train_df.show(5)\n",
        "  print(\"The shape of the test dataset is {:d} rows by {:d} columns\".format(test_df.count(), len(test_df.columns)))\n",
        "  test_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAPRhwLH6fbs",
        "outputId": "bbf34c63-2f18-473e-90c0-f761e3c3a2bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the train dataset is 666484 rows by 4 columns\n",
            "+-------------------+--------+----------+----------+\n",
            "|               date|   close|  features|labelIndex|\n",
            "+-------------------+--------+----------+----------+\n",
            "|2021-07-20 00:00:00|30847.56|[30847.56]|       0.0|\n",
            "|2021-07-20 00:01:00|30875.01|[30875.01]|       1.0|\n",
            "|2021-07-20 00:02:00|30825.46|[30825.46]|       2.0|\n",
            "|2021-07-20 00:03:00| 30795.5| [30795.5]|       3.0|\n",
            "|2021-07-20 00:04:00|30787.26|[30787.26]|       4.0|\n",
            "+-------------------+--------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "The shape of the test dataset is 166621 rows by 4 columns\n",
            "+-------------------+--------+----------+----------+\n",
            "|               date|   close|  features|labelIndex|\n",
            "+-------------------+--------+----------+----------+\n",
            "|2022-10-27 03:26:00|20703.24|[20703.24]|  666484.0|\n",
            "|2022-10-27 03:27:00|20688.54|[20688.54]|  666485.0|\n",
            "|2022-10-27 03:28:00|20680.09|[20680.09]|  666486.0|\n",
            "|2022-10-27 03:29:00|20680.35|[20680.35]|  666487.0|\n",
            "|2022-10-27 03:30:00| 20685.8| [20685.8]|  666488.0|\n",
            "+-------------------+--------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- close: double (nullable = true)\n",
            " |-- features: string (nullable = true)\n",
            " |-- labelIndex: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SLOW_OPERATION:\n",
        "  print(\"The shape of the train dataset is {:d} rows by {:d} columns\".format(train_df.count(), len(train_df.columns)))\n",
        "  train_df.show(5)\n",
        "  print(\"The shape of the test dataset is {:d} rows by {:d} columns\".format(test_df.count(), len(test_df.columns)))\n",
        "  test_df.show(5)"
      ],
      "metadata": {
        "id": "-JP_QRGt5Bcd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: da sistemare\n",
        "\n",
        "def random_forest(train, test):\n",
        "  from pyspark.ml.regression import RandomForestRegressor\n",
        "  from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "  # TODO: da risolvere in qualche modo (devo ricalcolarmi il vettore \"features\" perché quando importo i dataset viene visto come una stringa)\n",
        "  train = train.drop(\"features\")\n",
        "  test = test.drop(\"features\")\n",
        "\n",
        "  assembler = VectorAssembler(\n",
        "      inputCols=[\"close\"], \n",
        "      outputCol=\"features\"\n",
        "  )\n",
        "\n",
        "  train = assembler.transform(train)\n",
        "  test = assembler.transform(test)\n",
        "\n",
        "  # Crea un oggetto RandomForestRegressor: Puoi impostare i parametri desiderati per il modello Random Forest come numero di alberi (numTrees), profondità massima degli alberi (maxDepth), numero massimo di bin per il partizionamento delle features (maxBins), ecc.\n",
        "  rf = RandomForestRegressor(\n",
        "      featuresCol=\"features\",  # Colonna vettoriale delle features\n",
        "      labelCol=\"labelIndex\",  # Colonna delle etichette di output\n",
        "  )\n",
        "\n",
        "  # Addestra il modello: Utilizza il metodo fit() per addestrare il modello sulla tua dataset di addestramento.\n",
        "  model = rf.fit(train)\n",
        "\n",
        "  # Effettua le previsioni: Utilizza il modello addestrato per fare previsioni sul tuo dataset di test o su nuovi dati.\n",
        "  predictions = model.transform(test)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "FnagHRV5ZmUL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = random_forest(train_df, test_df)"
      ],
      "metadata": {
        "id": "LLIcmyCDoYtk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(pred):\n",
        "  from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "  evaluator = RegressionEvaluator(\n",
        "      labelCol=\"labelIndex\",  # Colonna delle etichette di output\n",
        "      predictionCol=\"prediction\"  # Colonna delle previsioni\n",
        "  )\n",
        "\n",
        "  mse = evaluator.evaluate(pred, {evaluator.metricName: \"mse\"})\n",
        "  rmse = evaluator.evaluate(pred, {evaluator.metricName: \"rmse\"})\n",
        "  r2 = evaluator.evaluate(pred, {evaluator.metricName: \"r2\"})\n",
        "  mae = evaluator.evaluate(pred, {evaluator.metricName: \"mae\"})\n",
        "\n",
        "  from pyspark.sql.functions import abs, col\n",
        "  from pyspark.sql import functions as F\n",
        "  from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "  # Calcola il MAPE\n",
        "  mape = pred.withColumn(\"error\", abs(col(\"labelIndex\") - col(\"prediction\")) / col(\"labelIndex\")) \\\n",
        "          .selectExpr(\"avg(error) * 100 as mape\") \\\n",
        "          .collect()[0][\"mape\"]\n",
        "          \n",
        "  # adj_r2 \n",
        "  n = pred.count()  # Numero di osservazioni\n",
        "  p = 1  # Numero di predittori nel modello\n",
        "  adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
        "\n",
        "  print(\"MSE = %s\" % (mse)) # deve essere un valore non negativo, dove un valore di 0 indica una perfetta corrispondenza tra i valori predetti e quelli di riferimento\n",
        "  print(\"RMSE = %s\" % (rmse)) # dovresti considerare il valore di RMSE in relazione al range dei valori target nel tuo problema specifico\n",
        "  print(\"R2 = %s\" % (r2)) # piú é vicino ad 1 meglio é\n",
        "  print(\"MAE = %s\" % (mae)) # può essere utile confrontare il valore di MAE con quello di altri modelli o con il range dei valori target per valutare la sua precisione\n",
        "  print(\"MAPE = %s\" % (mape)) # di solito viene utilizzato come misura relativa per confrontare la precisione di modelli diversi\n",
        "  print(\"ADJ R2 = %s\" % (adj_r2)) # piú é vicino ad 1 meglio é"
      ],
      "metadata": {
        "id": "C1-U5RrqtioH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSsgF_D0tgB9",
        "outputId": "ff45f24f-8c8e-4cb0-dffb-3874b3e19388"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE = 8738520842.856455\n",
            "RMSE = 93480.0558560833\n",
            "R2 = 0.7639305889282244\n",
            "MAE = 65860.25448693408\n",
            "MAPE = 550.4254789163781\n",
            "ADJ R2 = 0.7639302347259938\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e452Ydj7chNV",
        "zK1eMlqXWioB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}