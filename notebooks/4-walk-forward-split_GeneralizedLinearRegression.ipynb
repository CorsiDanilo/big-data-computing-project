{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bitcoin price prediction - Walk Forward Split**\n",
    "### Big Data Computing final project - A.Y. 2022 - 2023\n",
    "Prof. Gabriele Tolomei\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "La Sapienza, University of Rome\n",
    "\n",
    "### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Description: perform model's train / validation with hyperparameter tuning and cross validation based on different methods of splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global constants, dependencies, libraries and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main constants\n",
    "LOCAL_RUNNING = True\n",
    "SLOW_OPERATIONS = False # Decide whether or not to use operations that might slow down notebook execution\n",
    "ROOT_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL_RUNNING:\n",
    "    # Point Colaboratory to Google Drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Define GDrive paths\n",
    "    drive.mount(ROOT_DIR, force_remount=True)\n",
    "\n",
    "    # Install Spark and related dependencies\n",
    "    !pip install pyspark\n",
    "    !pip install -U -q PyDrive -qq\n",
    "    !apt install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import my utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set main dir\n",
    "MAIN_DIR = ROOT_DIR + \"\" if LOCAL_RUNNING else ROOT_DIR + \"/MyDrive/BDC/project\"\n",
    "\n",
    "# Utilities dir\n",
    "UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n",
    "\n",
    "# Import my utilities\n",
    "import sys\n",
    "sys.path.append(UTILITIES_DIR)\n",
    "\n",
    "from imports import *\n",
    "import train_validation_utilities\n",
    "from config import *\n",
    "\n",
    "importlib.reload(train_validation_utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = Block Split\n",
    "# WFS = Walk Forward Split\n",
    "# SS = Single Split\n",
    "SPLITTING_METHOD = WFS\n",
    "\n",
    "# LR = LinearRegression \n",
    "# GLR = GeneralizedLinearRegression \n",
    "# RF = RandomForestRegressor \n",
    "# GBTR = GradientBoostingTreeRegressor\n",
    "MODEL_NAME = GLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# --- DATASET --- #\n",
    "###################\n",
    "\n",
    "# Datasets dirs\n",
    "DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n",
    "\n",
    "# Datasets paths\n",
    "DATASET_TRAIN_VALID  = DATASET_OUTPUT_DIR + \"/\" + DATASET_TRAIN_VALID_NAME + \".parquet\"\n",
    "\n",
    "####################\n",
    "# --- FEATURES --- #\n",
    "####################\n",
    "\n",
    "# Features dir\n",
    "FEATURES_DIR = MAIN_DIR + \"/features\"\n",
    "\n",
    "# Features paths\n",
    "FEATURES_CORRELATION = FEATURES_DIR + \"/\" + FEATURES_CORRELATION_LABEL + \".json\"\n",
    "BASE_FEATURES = FEATURES_DIR + \"/\" + BASE_FEATURES_LABEL + \".json\"\n",
    "BASE_AND_MOST_CORR_FEATURES = FEATURES_DIR + \"/\" + BASE_AND_MOST_CORR_FEATURES_LABEL + \".json\"\n",
    "BASE_AND_LEAST_CORR_FEATURES = FEATURES_DIR + \"/\" + BASE_AND_LEAST_CORR_FEATURES_LABEL + \".json\"\n",
    "\n",
    "##################\n",
    "# --- MODELS --- #\n",
    "##################\n",
    "\n",
    "# Model dir\n",
    "MODELS_DIR = MAIN_DIR + \"/models\"\n",
    "\n",
    "# Model path\n",
    "MODEL = MODELS_DIR + \"/\" + MODEL_NAME\n",
    "\n",
    "###################\n",
    "# --- RESULTS --- #\n",
    "###################\n",
    "\n",
    "# Results dir\n",
    "RESULTS_DIR = MAIN_DIR + \"/results/\" + SPLITTING_METHOD\n",
    "\n",
    "# Results path\n",
    "ALL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_all.csv\"\n",
    "REL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_rel.csv\"\n",
    "\n",
    "MODEL_ACCURACY_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pio.renderers.default = 'vscode+colab' # To correctly render plotly plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session\n",
    "conf = SparkConf().\\\n",
    "                set('spark.ui.port', \"4050\").\\\n",
    "                set('spark.executor.memory', '12G').\\\n",
    "                set('spark.driver.memory', '12G').\\\n",
    "                set('spark.driver.maxResultSize', '109G').\\\n",
    "                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n",
    "                setAppName(\"BitcoinPricePrediction\").\\\n",
    "                setMaster(\"local[*]\")\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train / validation set into pyspark dataset objects\n",
    "df = spark.read.load(DATASET_TRAIN_VALID,\n",
    "                         format=\"parquet\",\n",
    "                         sep=\",\",\n",
    "                         inferSchema=\"true\",\n",
    "                         header=\"true\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_utilities.dataset_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base features\n",
    "with open(BASE_FEATURES, \"r\") as f:\n",
    "    BASE_FEATURES = json.load(f)\n",
    "print(BASE_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading currency and additional most correlated features\n",
    "with open(BASE_AND_MOST_CORR_FEATURES, \"r\") as f:\n",
    "    BASE_AND_MOST_CORR_FEATURES = json.load(f)\n",
    "print(BASE_AND_MOST_CORR_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading currency and additional least correlated features\n",
    "with open(BASE_AND_LEAST_CORR_FEATURES, \"r\") as f:\n",
    "    BASE_AND_LEAST_CORR_FEATURES = json.load(f)\n",
    "print(BASE_AND_LEAST_CORR_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train / validation\n",
    "In order to train and validate the model I'll try several approaches:\n",
    "- `Default without normalization:` make predictions using the base model\n",
    "- `Default with normalization:` like the previous one but features are normalized\n",
    "\n",
    "Then the features that gave on average the most satisfactory results (for each model) are chosen and proceeded with:\n",
    "- `Hyperparameter tuning:` finding the best parameters to use. \n",
    "- `Cross Validation:` validate the performance of the model with the chosen parameters (also here using Block split / Walk forward split)\n",
    "\n",
    "If the final results are satisfactory, the model will be trained on the whole train / validation set and saved in order to make predictions on the test set.\n",
    "\n",
    "For each approach the train / validation set will be split according to the chosen splitting method (in order to figure out which one works best for our problem). In this case the `Walk forward time series splits` method will be used: involves using a sliding window approach to create the training and validation sets for each fold. The model is trained on a fixed window of historical data, and then validated on the next observation in the time series. This process is repeated for each subsequent observation, with the window sliding forward one step at a time. \n",
    "\n",
    "<img src=\"https://github.com/CorsiDanilo/big-data-computing-project/blob/main/notebooks/images/walk-forward-splits.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get splitting parameters based on the choosen splitting method\n",
    "splitting_info = train_validation_utilities.get_splitting_params(SPLITTING_METHOD)\n",
    "splitting_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default\n",
    "The train / validation set will be splitted based on the splitting method chosen so that the model performance can be seen without any tuning by using different features (normalized and non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default parameters\n",
    "params = train_validation_utilities.get_defaults_model_params(MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and features type\n",
    "MODEL_TYPE = \"default\"\n",
    "FEATURES_NORMALIZATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base features\n",
    "CHOSEN_FEATURES = BASE_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base features\n",
    "default_train_results_base_features, default_valid_results_base_features, default_train_pred_base_features, default_valid_pred_base_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, SLOW_OPERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base and most additional correlated features\n",
    "CHOSEN_FEATURES = BASE_AND_MOST_CORR_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_AND_MOST_CORR_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base and most additional correlated features\n",
    "default_train_results_base_and_most_corr_features, default_valid_results_base_and_most_corr_features, default_train_pred_base_and_most_corr_features, default_valid_pred_base_and_most_corr_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_base_and_most_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_base_and_most_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base and least additional correlated features\n",
    "CHOSEN_FEATURES = BASE_AND_LEAST_CORR_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_AND_LEAST_CORR_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base and least additional correlated features\n",
    "default_train_results_base_and_least_corr_features, default_valid_results_base_and_least_corr_features, default_train_pred_base_and_least_corr_features, default_valid_pred_base_and_least_corr_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_base_and_least_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_base_and_least_corr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and features type\n",
    "MODEL_TYPE = \"default_norm\"\n",
    "FEATURES_NORMALIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base features\n",
    "CHOSEN_FEATURES = BASE_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base features\n",
    "default_norm_train_results_base_features, default_norm_valid_results_base_features, default_norm_train_pred_base_features, default_norm_valid_pred_base_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base and most additional correlated features\n",
    "CHOSEN_FEATURES = BASE_AND_MOST_CORR_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_AND_MOST_CORR_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base and most additional correlated features\n",
    "default_norm_train_results_base_and_most_corr_features, default_norm_valid_results_base_and_most_corr_features, default_norm_train_pred_base_and_most_corr_features, default_norm_valid_pred_base_and_most_corr_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_base_and_most_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_base_and_most_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose base and least additional correlated features\n",
    "CHOSEN_FEATURES = BASE_AND_LEAST_CORR_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_AND_LEAST_CORR_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using base and least additional correlated features\n",
    "default_norm_train_results_base_and_least_corr_features, default_norm_valid_results_base_and_least_corr_features, default_norm_train_pred_base_and_least_corr_features, default_norm_valid_pred_base_and_least_corr_features = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_base_and_least_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_base_and_least_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model information and evaluators to show\n",
    "model_info = ['Model', 'Type', 'Dataset', 'Splitting', 'Features', 'Parameters']\n",
    "evaluator_lst = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adjusted_R2', 'Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results to show\n",
    "default_comparison_lst = [default_valid_results_base_features, default_valid_results_base_and_most_corr_features, default_valid_results_base_and_least_corr_features, default_norm_valid_results_base_features, default_norm_valid_results_base_and_most_corr_features, default_norm_valid_results_base_and_least_corr_features]\n",
    "\n",
    "# Show the comparison table\n",
    "default_comparison_lst_df = pd.concat([train_validation_utilities.model_comparison(results, model_info, evaluator_lst) for results in default_comparison_lst])\n",
    "default_comparison_lst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best default model results and predicitons\n",
    "best_default_results = pd.concat([train_validation_utilities.model_comparison(results, model_info, evaluator_lst) for results in [default_norm_valid_results_base_and_least_corr_features]])\n",
    "best_default_predictions = default_norm_valid_pred_base_and_least_corr_features\n",
    "best_default_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the choosen features\n",
    "FEATURES_NORMALIZATION = True\n",
    "CHOSEN_FEATURES = BASE_AND_LEAST_CORR_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = BASE_FEATURES_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned\n",
    "Once the features and execution method are selected, the model will undergo hyperparameter tuning and cross validation to find the best configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model grid parameters\n",
    "params = train_validation_utilities.get_model_grid_params(MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the type of feature to be used\n",
    "MODEL_TYPE = \"hyp_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning\n",
    "hyp_res = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)\n",
    "hyp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Since during this stage will be used the Block split / Walk forward split method of the dataset I compute a score for each parameter chosen by each split, assigning weights based on:\n",
    "   * Their `frequency` for each split (if the same parameters are chosen from several splits, these will have greater weight) \n",
    "   * The `split` they belong to (the closer the split is to today's date the more weight they will have)\n",
    "   * Their `RMSE value` for each split (the lower this is, the more weight they will have)\n",
    "   \n",
    "   Then, the overall score will be calculated by putting together these three weights for each parameter and the one with the best score will be the chosen parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameters score\n",
    "grouped_scores, best_params = train_validation_utilities.choose_best_params(hyp_res)\n",
    "grouped_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"cross_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tuned parameters\n",
    "params = train_validation_utilities.get_best_model_params(best_params, MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation\n",
    "cv_train_result, cv_valid_result, cv_train_pred, cv_valid_pred = train_validation_utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL, LOCAL_RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_valid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results to show\n",
    "tuned_comparison_lst = [cv_valid_result]\n",
    "\n",
    "# Show the comparison table\n",
    "tuned_comparison_lst_df = pd.concat([train_validation_utilities.model_comparison(results, model_info, evaluator_lst) for results in tuned_comparison_lst])\n",
    "tuned_comparison_lst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison table\n",
    "Visualization of model performance at various stages of train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate final results into Pandas dataset\n",
    "final_comparison_lst_df = pd.DataFrame(pd.concat([default_comparison_lst_df, tuned_comparison_lst_df], ignore_index=True))\n",
    "final_comparison_lst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model accuracy\n",
    "\n",
    "Since predicting the price accurately is very difficult I also saw how good the models are at predicting whether the price will go up or down in this way:\n",
    "\n",
    "For each prediction let's consider the actual market-price, next-market-price and our predicted next-market-price (prediction).\n",
    "I compute whether the current prediction is correct (1) or not (0):\n",
    "\n",
    "$$ \n",
    "prediction\\_is\\_correct\n",
    "= \n",
    "\\begin{cases}\n",
    "0 \\text{ if [(market-price > next-market-price) and (market-price < prediction)] or [(market-price < next-market-price) and (market-price > prediction)]} \\\\\n",
    "1 \\text{ if [(market-price > next-market-price) and (market-price > prediction)] or [(market-price < next-market-price) and (market-price < prediction)]}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "After that I count the number of correct prediction:\n",
    "$$ \n",
    "correct\\_predictions\n",
    "= \n",
    "\\sum_{i=0}^{total\\_rows} prediction\\_is\\_correct\n",
    "$$\n",
    "\n",
    "Finally I compute the percentage of accuracy of the model:\n",
    "$$\n",
    "\\\\ \n",
    "accuracy \n",
    "= \n",
    "(correct\\_predictions / total\\_rows) \n",
    "* 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas dataset to a PySpark dataset\n",
    "best_default_pred_spark = spark.createDataFrame(best_default_predictions)\n",
    "validated_pred_spark = spark.createDataFrame(cv_valid_pred)\n",
    "\n",
    "# Compute model accuracy\n",
    "default_accuracy = train_validation_utilities.model_accuracy(best_default_pred_spark)\n",
    "validated_accuracy = train_validation_utilities.model_accuracy(validated_pred_spark)\n",
    "\n",
    "# Shows whether features are normalised or not\n",
    "if FEATURES_NORMALIZATION:\n",
    "    NEW_CHOSEN_FEATURES_LABEL = CHOSEN_FEATURES_LABEL + \"_norm\"\n",
    "    CHOSEN_FEATURES_LABEL = NEW_CHOSEN_FEATURES_LABEL\n",
    "    \n",
    "# Saving accuracy data into dataframe\n",
    "accuracy_data = {\n",
    "    'Model': MODEL_NAME,\n",
    "    'Features': CHOSEN_FEATURES_LABEL,\n",
    "    'Splitting': SPLITTING_METHOD,\n",
    "    'Accuracy (default)': default_accuracy,\n",
    "    'Accuracy (tuned)': validated_accuracy\n",
    "}\n",
    "accuracy_data_df = pd.DataFrame(accuracy_data, index=['Model'])\n",
    "\n",
    "print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (default): {default_accuracy:.2f}%\")\n",
    "print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (tuned): {validated_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate default and tuned results\n",
    "default_tuned_results = [best_default_results, cv_valid_result]\n",
    "default_tuned_results_df = pd.concat([train_validation_utilities.model_comparison(results, model_info, evaluator_lst) for results in default_tuned_results])\n",
    "default_tuned_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all final comparison results\n",
    "final_comparison_lst_df.to_csv(ALL_MODEL_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant results (default and tuned results)\n",
    "default_tuned_results_df.to_csv(REL_MODEL_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving accuracy results\n",
    "accuracy_data_df.to_csv(MODEL_ACCURACY_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook in html format (remember to save the notebook and change the model name)\n",
    "if LOCAL_RUNNING:\n",
    "    !jupyter nbconvert --to html 4-walk-forward-split_{MODEL_NAME}.ipynb --output 4-walk-forward-split_{MODEL_NAME} --output-dir='../exports'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
