{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bitcoin price prediction - Walk Forward Split**\n",
    "### Big Data Computing final project - A.Y. 2022 - 2023\n",
    "Prof. Gabriele Tolomei\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "La Sapienza, University of Rome\n",
    "\n",
    "### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Description: executing the chosen model, first with default values, then by choosing the best parameters by performing hyperparameter tuning with cross validation and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global constants, dependencies, libraries and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main constants\n",
    "LOCAL_RUNNING = True\n",
    "SLOW_OPERATIONS = True # Decide whether or not to use operations that might slow down notebook execution\n",
    "ROOT_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL_RUNNING:\n",
    "    # Point Colaboratory to Google Drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Define GDrive paths\n",
    "    drive.mount(ROOT_DIR, force_remount=True)\n",
    "\n",
    "    # Install Spark and related dependencies\n",
    "    !pip install pyspark\n",
    "    !pip install -U -q PyDrive -qq\n",
    "    !apt install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import my utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set main dir\n",
    "MAIN_DIR = ROOT_DIR + \"\" if LOCAL_RUNNING else ROOT_DIR + \"/MyDrive/BDC/project\"\n",
    "\n",
    "# Utilities dir\n",
    "UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n",
    "\n",
    "# Import my utilities\n",
    "import sys\n",
    "sys.path.append(UTILITIES_DIR)\n",
    "\n",
    "from imports import *\n",
    "import utilities, parameters\n",
    "\n",
    "importlib.reload(utilities)\n",
    "importlib.reload(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = Block Split\n",
    "# WFS = Walk Forward Split\n",
    "# SS = Single Split\n",
    "SPLITTING_METHOD = parameters.WFS \n",
    "\n",
    "# LR = LinearRegression \n",
    "# GLR = GeneralizedLinearRegression \n",
    "# RF = RandomForestRegressor \n",
    "# GBTR = GradientBoostingTreeRegressor\n",
    "MODEL_NAME = parameters.GBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# --- DATASET --- #\n",
    "###################\n",
    "\n",
    "# Datasets dirs\n",
    "DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n",
    "\n",
    "# Datasets names\n",
    "DATASET_TRAIN_VALID_NAME = \"bitcoin_blockchain_data_15min_train_valid\"\n",
    "\n",
    "# Datasets paths\n",
    "DATASET_TRAIN_VALID  = DATASET_OUTPUT_DIR + \"/\" + DATASET_TRAIN_VALID_NAME + \".parquet\"\n",
    "\n",
    "####################\n",
    "# --- FEATURES --- #\n",
    "####################\n",
    "\n",
    "# Features dir\n",
    "FEATURES_DIR = MAIN_DIR + \"/features\"\n",
    "\n",
    "# Features labels\n",
    "FEATURES_LABEL = \"features\"\n",
    "TARGET_LABEL = \"next-market-price\"\n",
    "\n",
    "# Features names\n",
    "ALL_FEATURES_NAME = \"all_features\"\n",
    "MOST_REL_FEATURES_NAME = \"most_rel_features\"\n",
    "LEAST_REL_FEATURES_NAME = \"least_rel_features\"\n",
    "\n",
    "# Features paths\n",
    "ALL_FEATURES = FEATURES_DIR + \"/\" + ALL_FEATURES_NAME + \".json\"\n",
    "MOST_REL_FEATURES = FEATURES_DIR + \"/\" + MOST_REL_FEATURES_NAME + \".json\"\n",
    "LEAST_REL_FEATURES = FEATURES_DIR + \"/\" + LEAST_REL_FEATURES_NAME + \".json\"\n",
    "\n",
    "##################\n",
    "# --- MODELS --- #\n",
    "##################\n",
    "\n",
    "# Model dir\n",
    "MODELS_DIR = MAIN_DIR + \"/models\"\n",
    "\n",
    "# Model path\n",
    "MODEL = MODELS_DIR + \"/\" + MODEL_NAME\n",
    "\n",
    "###################\n",
    "# --- RESULTS --- #\n",
    "###################\n",
    "\n",
    "# Results dir\n",
    "RESULTS_DIR = MAIN_DIR + \"/results/\" + SPLITTING_METHOD\n",
    "\n",
    "# Results path\n",
    "ALL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_all.csv\"\n",
    "REL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_rel.csv\"\n",
    "\n",
    "MODEL_ACCURACY_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pio.renderers.default='notebook' # To correctly export the notebook in html format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session\n",
    "conf = SparkConf().\\\n",
    "                set('spark.ui.port', \"4050\").\\\n",
    "                set('spark.executor.memory', '12G').\\\n",
    "                set('spark.driver.memory', '12G').\\\n",
    "                set('spark.driver.maxResultSize', '109G').\\\n",
    "                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n",
    "                setAppName(\"BitcoinPricePrediction\").\\\n",
    "                setMaster(\"local[*]\")\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train / validation set into pyspark dataset objects\n",
    "df = spark.read.load(DATASET_TRAIN_VALID,\n",
    "                         format=\"parquet\",\n",
    "                         sep=\",\",\n",
    "                         inferSchema=\"true\",\n",
    "                         header=\"true\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(dataset):\n",
    "  # Print dataset\n",
    "  dataset.show(3)\n",
    "\n",
    "  # Get the number of rows\n",
    "  num_rows = dataset.count()\n",
    "\n",
    "  # Get the number of columns\n",
    "  num_columns = len(dataset.columns)\n",
    "\n",
    "  # Print the shape of the dataset\n",
    "  print(\"Shape:\", (num_rows, num_columns))\n",
    "\n",
    "  # Print the schema of the dataset\n",
    "  dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SLOW_OPERATIONS:\n",
    "  dataset_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the features\n",
    "with open(ALL_FEATURES, \"r\") as f:\n",
    "    ALL_FEATURES = json.load(f)\n",
    "print(ALL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the most relevant features\n",
    "with open(MOST_REL_FEATURES, \"r\") as f:\n",
    "    MOST_REL_FEATURES = json.load(f)\n",
    "print(MOST_REL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading least relevant features\n",
    "with open(LEAST_REL_FEATURES, \"r\") as f:\n",
    "    LEAST_REL_FEATURES = json.load(f)\n",
    "print(LEAST_REL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train / validation\n",
    "In order to train and validate the model I'll try several approaches:\n",
    "- **Default without normalization:** Make predictions using the chosen base model\n",
    "- **Default with normalization:** Like the previous one but features are normalized\n",
    "\n",
    "At this point, the features that gave on average the most satisfactory results (for each model) are chosen and proceeded with:\n",
    "\n",
    "- **Hyperparameter tuning:** Researching the best parameters to use\n",
    "- **Cross Validation:** Validate the performance of the model with the chosen parameters\n",
    "\n",
    "If the final results are satisfactory, the model will be trained on the whole train / validation set and saved to later make predictions on the test set.\n",
    "\n",
    "For each approach the train / validation set will be split according to the chosen splitting method (in order to figure out which one works best for our problem). In this case the **Walk forward time series splits** method will be used: involves using a sliding window approach to create the training and validation sets for each fold. The model is trained on a fixed window of historical data, and then validated on the next observation in the time series. This process is repeated for each subsequent observation, with the window sliding forward one step at a time. \n",
    "\n",
    "![walk-forward-splits.png](https://drive.google.com/uc?id=1SNdq-kjbv4MXtdBj3EOJ2dmQpbbPStJi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get splitting parameters based on the choosen splitting method\n",
    "splitting_info = parameters.get_splitting_params(SPLITTING_METHOD)\n",
    "splitting_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default\n",
    "The train / validation set will be splitted based on the splitting method chosen so that the model performance can be seen without any tuning by using different features (normalized and non)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and features type\n",
    "MODEL_TYPE = \"default\"\n",
    "FEATURES_NORMALIZATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default parameters\n",
    "params = parameters.get_defaults_model_params(MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using all the features\n",
    "default_train_results_all_features, default_valid_results_all_features, default_train_pred_all_features, default_valid_pred_all_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using the most relevant features\n",
    "default_train_results_most_rel_features, default_valid_results_most_rel_features, default_train_pred_most_rel_features, default_valid_pred_most_rel_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_REL_FEATURES, MOST_REL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_most_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_most_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using the least relevant features\n",
    "default_train_results_least_rel_features, default_valid_results_least_rel_features, default_train_pred_least_rel_features, default_valid_pred_least_rel_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_REL_FEATURES, LEAST_REL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_results_least_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_valid_results_least_rel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and features type\n",
    "MODEL_TYPE = \"default_norm\"\n",
    "FEATURES_NORMALIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using all the features\n",
    "default_norm_train_results_all_features, default_norm_valid_results_all_features, default_norm_train_pred_all_features, default_norm_valid_pred_all_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using the most relevant features\n",
    "default_norm_train_results_most_rel_features, default_norm_valid_results_most_rel_features, default_norm_train_pred_most_rel_features, default_norm_valid_pred_most_rel_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_REL_FEATURES, MOST_REL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_most_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_most_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by using the least relevant features\n",
    "default_norm_train_results_least_rel_features, default_norm_valid_results_least_rel_features, default_norm_train_pred_least_rel_features, default_norm_valid_pred_least_rel_features = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_REL_FEATURES, LEAST_REL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_train_results_least_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm_valid_results_least_rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model information and evaluators to show\n",
    "model_info = ['Model', 'Type', 'Dataset', 'Splitting', 'Features', 'Parameters']\n",
    "evaluator_lst = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adjusted_R2', 'Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results to show\n",
    "default_comparison_lst = [default_valid_results_all_features, default_valid_results_most_rel_features, default_valid_results_least_rel_features, default_norm_valid_results_all_features, default_norm_valid_results_most_rel_features, default_norm_valid_results_least_rel_features]\n",
    "\n",
    "# Show the comparison table\n",
    "default_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in default_comparison_lst])\n",
    "default_comparison_lst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best default model results and predicitons\n",
    "best_default_results = default_valid_results_most_rel_features\n",
    "best_default_predictions = default_valid_pred_most_rel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned\n",
    "Once the features and execution method are selected, the model will undergo hyperparameter tuning and cross validation to find the best configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the type of feature to be used\n",
    "MODEL_TYPE = \"hyp_tuning\"\n",
    "CHOSEN_FEATURES = MOST_REL_FEATURES\n",
    "CHOSEN_FEATURES_LABEL = MOST_REL_FEATURES_NAME\n",
    "FEATURES_NORMALIZATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model grid parameters\n",
    "params = parameters.get_model_grid_params(MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning\n",
    "hyp_res = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n",
    "hyp_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To select the best parameters to be used in the final model I assign a score to each value in the \"Parameters\" column based on the following criteria:\n",
    "* Calculate the frequency of each unique value in the \"Parameters\" column.\n",
    "* Normalize the frequencies to a scale of 0 to 1, where 1 represents the highest frequency.\n",
    "* Calculate the split weight for each value in the \"Parameters\" column, where a higher split number corresponds to a higher weight.\n",
    "* Normalize the split weights to a scale of 0 to 1, where 1 represents the highest split weight.\n",
    "* Calculate the RMSE weight for each value in the \"Parameters\" column, where a lower RMSE value corresponds to a higher weight.\n",
    "* Normalize the RMSE weights to a scale of 0 to 1, where 1 represents the highest RMSE weight.\n",
    "\n",
    "Then calculate the overall score for each value in the \"Parameters\" column by combining the normalized frequency, split weight, and RMSE weight and take into consideration the parameters that have the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameters score\n",
    "grouped_scores, best_params = parameters.choose_best_params(hyp_res)\n",
    "grouped_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"cross_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tuned parameters\n",
    "params = parameters.get_best_model_params(best_params, MODEL_NAME)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation\n",
    "cv_train_result, cv_valid_result, cv_train_pred, cv_valid_pred = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results to show\n",
    "tuned_comparison_lst = [cv_valid_result]\n",
    "\n",
    "# Show the comparison table\n",
    "tuned_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in tuned_comparison_lst])\n",
    "tuned_comparison_lst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison table\n",
    "Visualization of model performance at various stages of train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate final results into Pandas dataset\n",
    "final_comparison_lst_df = pd.DataFrame(pd.concat([default_comparison_lst_df, tuned_comparison_lst_df], ignore_index=True))\n",
    "final_comparison_lst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model accuracy\n",
    "\n",
    "Since predicting the price accurately is very difficult let's se how good the model is at predicting whether the price will go up or down. \n",
    "\n",
    "For each row in our predictions let's consider the actual market-price, next-market-price and our predicted next-market-price (prediction).\n",
    "We compute whether the current prediction is correct (1) or not (0):\n",
    "\n",
    "$$ \n",
    "prediction\\_is\\_correct\n",
    "= \n",
    "\\begin{cases}\n",
    "0 \\text{ if [(market-price > next-market-price) and (market-price < prediction)] or [(market-price < next-market-price) and (market-price > prediction)]} \\\\\n",
    "1 \\text{ if [(market-price > next-market-price) and (market-price > prediction)] or [(market-price < next-market-price) and (market-price < prediction)]}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "After that we count the number of correct prediction:\n",
    "$$ \n",
    "correct\\_predictions\n",
    "= \n",
    "\\sum_{i=0}^{total\\_rows} prediction\\_is\\_correct\n",
    "$$\n",
    "\n",
    "Finally we compute the percentage of accuracy of the model:\n",
    "$$\n",
    "\\\\ \n",
    "accuracy \n",
    "= \n",
    "(correct\\_predictions / total\\_rows) \n",
    "* 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas dataset to a PySpark dataset\n",
    "best_default_pred_spark = spark.createDataFrame(best_default_predictions)\n",
    "validated_pred_spark = spark.createDataFrame(cv_valid_pred)\n",
    "\n",
    "# Compute model accuracy\n",
    "default_accuracy = utilities.model_accuracy(best_default_pred_spark)\n",
    "validated_accuracy = utilities.model_accuracy(validated_pred_spark)\n",
    "\n",
    "# Shows whether features are normalised or not\n",
    "if FEATURES_NORMALIZATION:\n",
    "    NEW_CHOSEN_FEATURES_LABEL = CHOSEN_FEATURES_LABEL + \"_norm\"\n",
    "    CHOSEN_FEATURES_LABEL = NEW_CHOSEN_FEATURES_LABEL\n",
    "\n",
    "# Saving accuracy data into dataframe\n",
    "accuracy_data = {\n",
    "    'Model': MODEL_NAME,\n",
    "    'Features': CHOSEN_FEATURES_LABEL,\n",
    "    'Splitting': SPLITTING_METHOD,\n",
    "    'Accuracy (default)': default_accuracy,\n",
    "    'Accuracy (tuned)': validated_accuracy\n",
    "}\n",
    "accuracy_data_df = pd.DataFrame(accuracy_data, index=['Model'])\n",
    "\n",
    "print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (default): {default_accuracy:.2f}%\")\n",
    "print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (tuned): {validated_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate default and tuned results\n",
    "default_tuned_results = [best_default_results, cv_valid_result]\n",
    "default_tuned_results_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in default_tuned_results])\n",
    "default_tuned_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving final results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all final comparison results\n",
    "final_comparison_lst_df.to_csv(ALL_MODEL_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant results (default and tuned results)\n",
    "default_tuned_results_df.to_csv(REL_MODEL_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving accuracy results\n",
    "accuracy_data_df.to_csv(MODEL_ACCURACY_RESULTS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook in html format (remember to save the notebook and change the model name)\n",
    "!jupyter nbconvert --to html GBTR_4-walk-forward-split.ipynb --output GBTR_4-walk-forward-split_{MODEL_NAME}_most_rel_features_no_norm --output-dir='./exports'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
