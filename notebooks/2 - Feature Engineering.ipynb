{"cells":[{"cell_type":"markdown","metadata":{"id":"oqwl6Bf0Lv9D"},"source":["# **Bitcoin price prediction with PySpark - Feature Engineering**\n","## Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n","\n","\n","---\n","\n","\n","Description: adding useful features regardings the price of Bitcoin, visualizing data and performing feature selection\n"]},{"cell_type":"markdown","metadata":{"id":"FIbEK2pDLv9x"},"source":["# Global constants, dependencies, libraries and tools"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jUpNH3eIQPR4"},"outputs":[],"source":["# Main constants\n","LOCAL_RUNNING = True\n","SLOW_OPERATIONS = False # Decide whether or not to use operations that might slow down notebook execution\n","MAIN_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EKk35k0MLv9y"},"outputs":[],"source":["if not LOCAL_RUNNING: \n","    # Point Colaboratory to Google Drive\n","    from google.colab import drive\n","\n","    # Define GDrive paths\n","    drive.mount(MAIN_DIR, force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wQ8YvlaZLv9x"},"outputs":[],"source":["# Set main dir\n","MAIN_DIR = MAIN_DIR + \"\" if LOCAL_RUNNING else MAIN_DIR + \"/MyDrive/BDC/project\"\n","\n","###################\n","# --- DATASET --- #\n","###################\n","\n","# Datasets dirs\n","DATASET_RAW_DIR = MAIN_DIR + \"/datasets/raw\"\n","DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n","DATASET_TEMP_DIR = MAIN_DIR + \"/datasets/temp\"\n","\n","# Datasets names\n","DATASET_NAME = \"bitcoin_blockchain_data_15min\"\n","\n","# Datasets paths\n","DATASET_RAW = DATASET_RAW_DIR + \"/\" + DATASET_NAME + \".parquet\"\n","\n","####################\n","# --- FEATURES --- #\n","####################\n","\n","# Features dir\n","FEATURES_DIR = MAIN_DIR + \"/features\"\n","\n","# Features names\n","FEATURES_CORRELATION_NAME = \"features_correlation\"\n","ALL_FEATURES_NAME = \"all_features\"\n","MOST_CORR_FEATURES_NAME = \"most_corr_features\"\n","LEAST_CORR_FEATURES_NAME = \"least_corr_features\"\n","\n","# Features paths\n","FEATURES_CORRELATION = FEATURES_DIR + \"/\" + FEATURES_CORRELATION_NAME + \".json\"\n","ALL_FEATURES = FEATURES_DIR + \"/\" + ALL_FEATURES_NAME + \".json\"\n","MOST_CORR_FEATURES = FEATURES_DIR + \"/\" + MOST_CORR_FEATURES_NAME + \".json\"\n","LEAST_CORR_FEATURES = FEATURES_DIR + \"/\" + LEAST_CORR_FEATURES_NAME + \".json\"\n","\n","#####################\n","# --- UTILITIES --- #\n","#####################\n","\n","# Utilities dir\n","UTILITIES_DIR = MAIN_DIR + \"/utilities\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IvKa3zyWLv9y"},"outputs":[],"source":["# Suppression of warnings for better reading\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sifbION-Lv9y"},"outputs":[],"source":["if not LOCAL_RUNNING:\n","    # Install Spark and related dependencies\n","    !pip install pyspark\n","    !pip install -U -q PyDrive -qq\n","    !apt install openjdk-8-jdk-headless -qq"]},{"cell_type":"markdown","metadata":{"id":"jeP_41EsLv9y"},"source":["# Import files"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"5O0SVFUzLv9y"},"outputs":[],"source":["# Import my files\n","import sys\n","sys.path.append(UTILITIES_DIR)\n","\n","from imports import *"]},{"cell_type":"markdown","metadata":{"id":"YUI9DnQVLv9z"},"source":["# Create the pyspark session"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZLwcEWvYLv9z"},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '12G').\\\n","                set('spark.driver.memory', '12G').\\\n","                set('spark.driver.maxResultSize', '109G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPricePrediction\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"F7IjRUGdLv9z"},"source":["# Loading dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ugtTXvpBLv9z"},"outputs":[],"source":["# Load datasets into pyspark dataset objects\n","df = spark.read.load(DATASET_RAW,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    ) \\\n","                     .withColumn(\"id\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))-1) # Adding \"id\" column"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"s76xwenILv9z"},"outputs":[],"source":["def dataset_info(dataset):\n","  # Print dataset\n","  dataset.show(3)\n","\n","  # Get the number of rows\n","  num_rows = dataset.count()\n","\n","  # Get the number of columns\n","  num_columns = len(dataset.columns)\n","\n","  # Print the shape of the dataset\n","  print(\"Shape:\", (num_rows, num_columns))\n","\n","  # Print the schema of the dataset\n","  dataset.printSchema()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TkviD2q8Lv9z"},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(df)"]},{"cell_type":"markdown","metadata":{"id":"h1v6rsUjLv9z"},"source":["# Adding useful features\n","Here I am going to add some features that could help us predict the Bitcoin price:\n","\n","*   **Next market price:** represents the price of Bitcoin for the next day (this will be the target variable on which to make predictions)\n","*   **Rate of change:** indicator that measures the percentage of price changes over a period of time, allows investors to spot security momentum and other trends\n","*   **Simple Moving Averages:** indicators that calculate the average price over a specified number of days. They are commonly used by traders to identify trends and potential buy or sell signals\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"WML0CKelLv9z"},"outputs":[],"source":["# Creation of a new dataset for the new features\n","new_features_df = df.select(\"timestamp\", \"id\", \"market-price\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7j4GFmpHLv9z"},"outputs":[],"source":["# Adding 'tomorrow-market-price' column\n","new_features_df = new_features_df.withColumn(\"next-market-price\", F.lag(\"market-price\", offset=-1) \\\n","        .over(Window.orderBy(\"id\"))) \\\n","        .dropna()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-XLttgNKLv90"},"outputs":[],"source":["# Adding \"rate-of-change\" column\n","new_features_df = new_features_df.withColumn(\"rate-of-change\", (F.col(\"next-market-price\") / F.col(\"market-price\") - 1) * 100)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"G6MPud1wLv90"},"outputs":[],"source":["def simple_moving_average(dataset, period, days, col=\"next-market-price\", orderby=\"id\"):\n","    dataset = dataset.withColumn(f\"sma-{days}-days\", F.avg(col) \\\n","          .over(Window.orderBy(orderby) \\\n","          .rowsBetween(-period,0)))\n","    return dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"0OBOZbvILv90"},"outputs":[],"source":["# Moving averages days (5/7/10/20/50/100)\n","MA5 = 60 * 24 * 5\n","MA7 = 60 * 24 * 7\n","MA10 = 60 * 24 * 10\n","MA20 = 60 * 24 * 20\n","MA50 = 60 * 24 * 50\n","MA100 = 60 * 24 * 100\n","\n","# Computing the SMA\n","new_features_df = simple_moving_average(new_features_df, MA5, 5)\n","new_features_df = simple_moving_average(new_features_df, MA7, 7)\n","new_features_df = simple_moving_average(new_features_df, MA10, 10)\n","new_features_df = simple_moving_average(new_features_df, MA20, 20)\n","new_features_df = simple_moving_average(new_features_df, MA50, 50)\n","new_features_df = simple_moving_average(new_features_df, MA100, 100)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"krKDYR1yLv90"},"outputs":[],"source":["# Drop \"market-price\" column\n","new_features_df = new_features_df.drop(\"market-price\")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"n3exSIGOLv90"},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(new_features_df)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"pL_5JLHyLv90"},"outputs":[],"source":["# Merge original dataset with the one with the new features\n","merged_df = df.join(new_features_df, on=['timestamp','id'], how='inner')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"q74GqOn_Lv90"},"outputs":[],"source":["# Rearranges  columns\n","new_columns = [\"timestamp\", \"id\"] + [col for col in merged_df.columns if col not in [\"timestamp\", \"id\", \"next-market-price\"]] + [\"next-market-price\"]\n","merged_df = merged_df.select(*new_columns)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"g9vp9SeaSaCg"},"outputs":[],"source":["# Set the \"timestamp\" column as the index of the Pandas dataset\n","merged_df.toPandas().set_index(\"timestamp\", inplace=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"d39yWzsbLv90"},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(merged_df)"]},{"cell_type":"markdown","metadata":{"id":"wxb4eZExLv91"},"source":["# Splitting dataset\n","Here we are going to split the dataset into two sets:\n","* **Train / Validation set:** will be used to train the models and validate the performances\n","* **Test set:** will be used to perform price prediction on never-before-seen data (the last 3 months of the original dataset will be used)."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"y5TAxfkk-v8c"},"outputs":[],"source":["# Retrieve the last timestamp value\n","last_value = merged_df.agg(last(\"timestamp\")).collect()[0][0]\n","\n","# Subtract three month from the last timestamp value\n","split_date = last_value - relativedelta(months=3)\n","\n","# Split the dataset based on the desired date\n","train_valid_df = merged_df[merged_df['timestamp'] <= split_date]\n","test_df = merged_df[merged_df['timestamp'] > split_date]"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"KoN-4kCaLv91"},"outputs":[],"source":["def data_visualization(train_valid, test):\n","  trace1 = go.Scatter(\n","      x = train_valid['timestamp'],\n","      y = train_valid[\"market-price\"].astype(float),\n","      mode = 'lines',\n","      name = \"Train / Validation set\"\n","  )\n","\n","  trace2 = go.Scatter(\n","      x = test['timestamp'],\n","      y = test['market-price'].astype(float),\n","      mode = 'lines',\n","      name = \"Test set\"\n","  )\n","\n","  layout = dict(\n","      title=\"Train / Validation and Test sets\",\n","      xaxis=dict(\n","          rangeselector=dict(\n","              buttons=list([\n","                  # Change the count to desired amount of months.\n","                  dict(count=1,\n","                      label='1m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=6,\n","                      label='6m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=12,\n","                      label='1y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=36,\n","                      label='3y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(step='all')\n","              ])\n","          ),\n","          rangeslider=dict(\n","              visible = True\n","          ),\n","          type='date'\n","      )\n","  )\n","\n","  data = [trace1, trace2]\n","  fig = dict(data=data, layout=layout)\n","  iplot(fig, filename = \"Train / Validation and Test sets\")"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"GZ_OPmiwLv91"},"outputs":[],"source":["if SLOW_OPERATIONS:\n","    data_visualization(train_valid_df.toPandas(), test_df.toPandas())"]},{"cell_type":"markdown","metadata":{"id":"pO07pajqLv91"},"source":["# Saving datasets"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"b0s5qokVLv91"},"outputs":[],"source":["def output(dataset, ds_type):\n","  dataset.write.parquet(DATASET_TEMP_DIR, mode='overwrite')\n","\n","  while True:\n","      parquet_files = glob.glob(os.path.join(DATASET_TEMP_DIR, \"part*.parquet\"))\n","      if len(parquet_files) > 0:\n","          # .parquet file found!\n","          file_path = parquet_files[0]\n","          break\n","      else:\n","          print(\".parquet file not found. I'll try again after 1 second...\")\n","          time.sleep(1)\n","\n","  print(\".parquet file found:\", file_path)\n","\n","  new_file_path = DATASET_OUTPUT_DIR + \"/\" + DATASET_NAME + \"_\" + ds_type +\".parquet\"\n","\n","  # Rename and move the file\n","  shutil.move(file_path, new_file_path)\n","\n","  print(\"File renamed and moved successfully!\")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Ss2HR4JpLv91"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o197.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32md:\\Documents\\Repository\\BDC\\project\\notebooks\\2 - Feature Engineering.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Save the train / validation set\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output(train_valid_df, \u001b[39m\"\u001b[39;49m\u001b[39mtrain_valid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","\u001b[1;32md:\\Documents\\Repository\\BDC\\project\\notebooks\\2 - Feature Engineering.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moutput\u001b[39m(dataset, ds_type):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   dataset\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(DATASET_TEMP_DIR, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/2%20-%20Feature%20Engineering.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       parquet_files \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_TEMP_DIR, \u001b[39m\"\u001b[39m\u001b[39mpart*.parquet\u001b[39m\u001b[39m\"\u001b[39m))\n","File \u001b[1;32md:\\Documents\\Repository\\BDC\\project\\.bdc\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n","File \u001b[1;32md:\\Documents\\Repository\\BDC\\project\\.bdc\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32md:\\Documents\\Repository\\BDC\\project\\.bdc\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32md:\\Documents\\Repository\\BDC\\project\\.bdc\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o197.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"]}],"source":["# Save the train / validation set\n","output(train_valid_df, \"train_valid\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtPmzkfwLv96"},"outputs":[],"source":["# Save the test set\n","output(test_df, \"test\")"]},{"cell_type":"markdown","metadata":{"id":"dbFTB-2aLv96"},"source":["# Data visualization\n","\n","Here we are going to display the features taken under consideration according to their categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ui5CtB0sLv96"},"outputs":[],"source":["# Convert the PySpark dataset into Pandas\n","merged_df_pd = merged_df.toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcnp_SJNLv96"},"outputs":[],"source":["# List of features according to categories\n","currency_statistics = {'Market price (USD)':'market-price', 'Market cap (USD)':'market-cap', 'N. total bitcoins':'total-bitcoins', 'Trade volume (USD)':'trade-volume'}\n","block_details = {'Blocks size (MB)':'blocks-size', 'Avg. block size (MB)':'avg-block-size', 'N. total transactions':'n-transactions-total', 'N. transactions per block':'n-transactions-per-block'}\n","mining_information = {'Hash rate (TH/s)':'hash-rate', 'Difficulty (T)':'difficulty', 'Miners revenue (USD)':'miners-revenue', 'Transaction fees (USD)':'transaction-fees-usd'}\n","network_activity = {\"N. unique addresses\":'n-unique-addresses', 'N. transactions':'n-transactions', 'Estimated transaction volume (USD)':'estimated-transaction-volume-usd'}\n","additional_features = {\"Rate of change (%)\":\"rate-of-change\", \"Simple moving avg. (5d)\":\"sma-5-days\", \"Simple moving avg. (7d)\":\"sma-7-days\", \"Simple moving avg. (10d)\":\"sma-10-days\", \"Simple moving avg. (20d)\":\"sma-20-days\", \"Simple moving avg. (50d)\":\"sma-50-days\", \"Simple moving avg. (100d)\":\"sma-100-days\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cEafWckLv96"},"outputs":[],"source":["def data_visualization(dataset, key, value):\n","  trace = go.Scatter(\n","      x = dataset['timestamp'],\n","      y = dataset[value].astype(float),\n","      mode = 'lines',\n","      name = key\n","  )\n","\n","  layout = dict(\n","      title=key,\n","      xaxis=dict(\n","          rangeselector=dict(\n","              buttons=list([\n","                  # Change the count to desired amount of months.\n","                  dict(count=1,\n","                      label='1m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=6,\n","                      label='6m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=12,\n","                      label='1y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=36,\n","                      label='3y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(step='all')\n","              ])\n","          ),\n","          rangeslider=dict(\n","              visible = True\n","          ),\n","          type='date'\n","      )\n","  )\n","\n","  data = [trace]\n","  fig = dict(data=data, layout=layout)\n","  iplot(fig, filename = \"Data visualization with rangeslider\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeBnhpcSLv98"},"outputs":[],"source":["# Currency Statistics\n","if SLOW_OPERATIONS:\n","  for key, value in currency_statistics.items():\n","    data_visualization(merged_df_pd, key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9PPFCl9Lv98"},"outputs":[],"source":["# Block Details\n","if SLOW_OPERATIONS:\n","  for key, value in block_details.items():\n","    data_visualization(merged_df_pd, key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMJYU9RGLv98"},"outputs":[],"source":["# Mining Information\n","if SLOW_OPERATIONS:\n","  for key, value in mining_information.items():\n","    data_visualization(merged_df_pd, key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xb-RqdrfLv98"},"outputs":[],"source":["# Network Activity\n","if SLOW_OPERATIONS:\n","  for key, value in network_activity.items():\n","    data_visualization(merged_df_pd, key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smotu8fBLv98"},"outputs":[],"source":["# Additional Features: Rate of change\n","if SLOW_OPERATIONS:\n","  first_pair = next(iter(additional_features.items()))\n","  data_visualization(merged_df_pd, first_pair[0], first_pair[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA5QQADhLv98"},"outputs":[],"source":["def sma_visualization(dataset, features, title):\n","  trace1 = go.Scatter(\n","      x = dataset['timestamp'],\n","      y = dataset[\"market-price\"].astype(float),\n","      mode = 'lines',\n","      name = \"Market price (usd)\"\n","  )\n","\n","  trace2 = go.Scatter(\n","      x = dataset['timestamp'],\n","      y = dataset[features[0][1]].astype(float),\n","      mode = 'lines',\n","      name = features[0][0]\n","  )\n","\n","  trace3 = go.Scatter(\n","      x = dataset['timestamp'],\n","      y = dataset[features[1][1]].astype(float),\n","      mode = 'lines',\n","      name = features[1][0]\n","  )\n","\n","  trace4 = go.Scatter(\n","      x = dataset['timestamp'],\n","      y = dataset[features[2][1]].astype(float),\n","      mode = 'lines',\n","      name = features[2][0]\n","  )\n","\n","  layout = dict(\n","      title=title,\n","      xaxis=dict(\n","          rangeselector=dict(\n","              buttons=list([\n","                  # Change the count to desired amount of months.\n","                  dict(count=1,\n","                      label='1m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=6,\n","                      label='6m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=12,\n","                      label='1y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=36,\n","                      label='3y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(step='all')\n","              ])\n","          ),\n","          rangeslider=dict(\n","              visible = True\n","          ),\n","          type='date'\n","      )\n","  )\n","\n","  data = [trace1, trace2, trace3, trace4]\n","\n","  fig = dict(data=data, layout=layout)\n","  iplot(fig, filename = title)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVsE5Ed_aZap"},"outputs":[],"source":["# Extract the short term SMA\n","short_term_sma = list(additional_features.items())[1:4]\n","print(short_term_sma)\n","\n","# Extract the long term SMA\n","long_term_sma = list(additional_features.items())[-3:]\n","print(long_term_sma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O45Snn4nLv99"},"outputs":[],"source":["# Additional Features: Short term SMA\n","if SLOW_OPERATIONS:\n","  sma_visualization(merged_df_pd, short_term_sma, \"Short term SMA (usd)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFPGKfPXaXPb"},"outputs":[],"source":["# Additional Features: Long term SMA\n","if SLOW_OPERATIONS:\n","  sma_visualization(merged_df_pd,long_term_sma, \"Long term SMA (usd)\")"]},{"cell_type":"markdown","metadata":{"id":"p1Fc7fl2Lv99"},"source":["#  Feature selection\n","Here we are going to select features based on their correlation with the market price using the Pearson method.\n","\n","They were divided into 3 groups to see the differences according to their use:\n","* **All:** contains all features\n","* **Most correlated:** contains features that have a correlation value > 0.5\n","* **Least correlated:** contains the features that have a correlation value <= 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pG39n7WLv99"},"outputs":[],"source":["# Use the dataset with only the features without any index\n","new_columns = [\"next-market-price\"] + [col for col in merged_df.columns if col not in [\"timestamp\", \"id\", \"next-market-price\"]]\n","merged_df_no_index = merged_df.select(*new_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnD9I1CmLv99"},"outputs":[],"source":["# First group of features: all\n","all_features = merged_df_no_index.columns[1:]\n","print(f\"Number of features: {len(all_features)}\")\n","print(all_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paP6aqjrLv99"},"outputs":[],"source":["# Assemble the features into a vector column\n","assembler = VectorAssembler(inputCols=merged_df_no_index.columns, outputCol=\"features\")\n","df_vector = assembler.transform(merged_df_no_index).select(\"next-market-price\", \"features\")\n","\n","# Calculate the correlation matrix using Pearson method\n","correlation_matrix = Correlation.corr(df_vector, \"features\", method=\"pearson\").head()\n","\n","# Get the correlation values with the \"market-price\" column\n","correlation_with_market_price = correlation_matrix[0].toArray()[0]\n","\n","# Create a dictionary with feature names and their correlation values\n","feature_correlations = dict(zip(merged_df_no_index.columns, correlation_with_market_price))\n","\n","# Sort the features based on their correlation value (from highest to lowest)\n","sorted_features = dict(sorted(feature_correlations.items(), key=lambda x: x[1], reverse=True))\n","\n","# Set the threshold value\n","threshold = 0.5\n","\n","# Show features and their correlation value according to the defined threshold\n","features_dict = {}\n","most_corr_features = []\n","least_corr_features = []\n","for feature, correlation in sorted_features.items():\n","  features_dict[feature] = correlation\n","  if (feature != 'next-market-price'):\n","    if (correlation > threshold):\n","      most_corr_features.append(feature)\n","    else:\n","      least_corr_features.append(feature)\n","\n","# Print the sorted features and their correlation values\n","features_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TBMl6rQLv99"},"outputs":[],"source":["# Second group of features: most correlated\n","print(f\"Number of features: {len(most_corr_features)}\")\n","print(most_corr_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kC5fAQVLv99"},"outputs":[],"source":["# Third group of features: least correlated\n","print(f\"Number of features: {len(least_corr_features)}\")\n","print(least_corr_features)"]},{"cell_type":"markdown","metadata":{"id":"Q1dizTOFLv99"},"source":["# Saving selected features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qs3tXlH3mozj"},"outputs":[],"source":["# Save all the features and their correlation value\n","with open(FEATURES_CORRELATION, 'w') as file:\n","    json.dump(features_dict, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dpvj9VWzLv99"},"outputs":[],"source":["# Save sll the features\n","with open(ALL_FEATURES, 'w') as file:\n","    json.dump(all_features, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InZ_DK88Lv9-"},"outputs":[],"source":["# Save the most correlated features\n","with open(MOST_CORR_FEATURES, 'w') as file:\n","    json.dump(most_corr_features, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAX8fLBcLv9-"},"outputs":[],"source":["# Save the least correlated features\n","with open(LEAST_CORR_FEATURES, 'w') as file:\n","    json.dump(least_corr_features, file)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
