{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Bitcoin price prediction - Random Forest Regressor**\n","### Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n","\n","\n","---\n","\n","\n","Description: executing the chosen model, first with default values, then by choosing the best parameters by performing hyperparameter tuning with cross validation and performance evaluation. Finally validate the tuned model and train it on the whole train /validation set"]},{"cell_type":"markdown","metadata":{},"source":["# Global constants, dependencies, libraries and tools"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Main constants\n","LOCAL_RUNNING = True\n","SLOW_OPERATIONS = True # Decide whether or not to use operations that might slow down notebook execution\n","MODEL_NAME = \"RandomForestRegressor\"\n","MAIN_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING: \n","    # Point Colaboratory to Google Drive\n","    from google.colab import drive\n","\n","    # Define GDrive paths\n","    drive.mount(MAIN_DIR, force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set main dir\n","MAIN_DIR = MAIN_DIR + \"\" if LOCAL_RUNNING else \"/MyDrive/BDC/project\"\n","\n","###################\n","# --- DATASET --- #\n","###################\n","\n","# Datasets dirs\n","DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n","\n","# Datasets names\n","DATASET_TRAIN_VALID_NAME = \"bitcoin_blockchain_data_30min_train_valid\"\n","\n","# Datasets paths\n","DATASET_TRAIN_VALID  = DATASET_OUTPUT_DIR + \"/\" + DATASET_TRAIN_VALID_NAME + \".parquet\"\n","\n","####################\n","# --- FEATURES --- #\n","####################\n","\n","# Features dir\n","FEATURES_DIR = MAIN_DIR + \"/features\"\n","\n","# Features labels\n","FEATURES_LABEL = \"features\"\n","TARGET_LABEL = \"next-market-price\"\n","\n","# Features names\n","ALL_FEATURES_NAME = \"all_features\"\n","MOST_CORR_FEATURES_NAME = \"most_corr_features\"\n","LEAST_CORR_FEATURES_NAME = \"least_corr_features\"\n","\n","# Features paths\n","ALL_FEATURES = FEATURES_DIR + \"/\" + ALL_FEATURES_NAME + \".json\"\n","MOST_CORR_FEATURES = FEATURES_DIR + \"/\" + MOST_CORR_FEATURES_NAME + \".json\"\n","LEAST_CORR_FEATURES = FEATURES_DIR + \"/\" + LEAST_CORR_FEATURES_NAME + \".json\"\n","\n","##################\n","# --- MODELS --- #\n","##################\n","\n","# Model dir\n","MODELS_DIR = MAIN_DIR + \"/models\"\n","\n","# Model path\n","MODEL = MODELS_DIR + \"/\" + MODEL_NAME\n","\n","#####################\n","# --- UTILITIES --- #\n","#####################\n","\n","# Utilities dir\n","UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n","\n","###################\n","# --- RESULTS --- #\n","###################\n","\n","# Results dir\n","RESULTS_DIR = MAIN_DIR + \"/results\"\n","\n","# Results path\n","MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \".csv\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Suppression of warnings for better reading\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING:\n","    # Install Spark and related dependencies\n","    !pip install pyspark\n","    !pip install -U -q PyDrive -qq\n","    !apt install openjdk-8-jdk-headless -qq"]},{"cell_type":"markdown","metadata":{},"source":["# Import files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import my files\n","import sys\n","sys.path.append(UTILITIES_DIR)\n","\n","from imports import *\n","import utilities, parameters\n","\n","importlib.reload(utilities)\n","importlib.reload(parameters)"]},{"cell_type":"markdown","metadata":{},"source":["# Create the pyspark session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '12G').\\\n","                set('spark.driver.memory', '12G').\\\n","                set('spark.driver.maxResultSize', '109G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPricePrediction\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load train / validation set into pyspark dataset objects\n","df = spark.read.load(DATASET_TRAIN_VALID,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dataset_info(dataset):\n","  # Print dataset\n","  dataset.show(3)\n","\n","  # Get the number of rows\n","  num_rows = dataset.count()\n","\n","  # Get the number of columns\n","  num_columns = len(dataset.columns)\n","\n","  # Print the shape of the dataset\n","  print(\"Shape:\", (num_rows, num_columns))\n","\n","  # Print the schema of the dataset\n","  dataset.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(df)"]},{"cell_type":"markdown","metadata":{},"source":["# Loading features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading all the features\n","with open(ALL_FEATURES, \"r\") as f:\n","    ALL_FEATURES = json.load(f)\n","print(ALL_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the most correlated features\n","with open(MOST_CORR_FEATURES, \"r\") as f:\n","    MOST_CORR_FEATURES = json.load(f)\n","print(MOST_CORR_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading least correlated features\n","with open(LEAST_CORR_FEATURES, \"r\") as f:\n","    LEAST_CORR_FEATURES = json.load(f)\n","print(LEAST_CORR_FEATURES)"]},{"cell_type":"markdown","metadata":{},"source":["# Model train / validation ❗\n","In order to train and validate the model, I'll try several approaches:\n","- **Simple:** Make predictions using the chosen base model\n","- **Simple with normalization:** Like the previous one but features are normalized\n","\n","At this point, the features that gave on average the most satisfactory results (for each model) are chosen and proceeded with:\n","\n","- **Hyperparameter tuning:** model validation to find the best parameters to use\n","- **Cross Validation:** validate the performance of the model with the chosen parameters\n","- **Validate final model:** validate the model with the chosen parameters\n","- **Train final model:** train the final model on the whole train / validation set to be ready to make predictions on market price"]},{"cell_type":"markdown","metadata":{},"source":["## Simple ❗\n","The train / validation set will be splitted so that the model performance can be seen without any tuning by using different features (normalized and non)"]},{"cell_type":"markdown","metadata":{},"source":["### Simple model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"simple\"\n","FEATURES_NORMALIZATION = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get default parameters\n","params = parameters.get_defaults_model_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cv_info = parameters.get_cross_validation_params('multi_splits')\n","cv_info = parameters.get_cross_validation_params('block_splits')\n","# cv_info = parameters.get_cross_validation_params('walk_forward_splits')\n","cv_info"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Takes the total number of samples, the minimum number of observations, and the sliding window size as input \n","# and returns a list of tuples containing the start, split, and end positions for each walk-forward split. \n","# We then add an index column to the dataset using the monotonically_increasing_id function and calculate \n","# the total number of samples. Finally, we iterate over the generated split positions, create training and \n","# validation datasets, and train and evaluate the model on each split.\n","def walk_forward_splits_new(num, min_obser, sliding_window):\n","    split_positions = []\n","    start = 0\n","    while start + min_obser + sliding_window <= num:\n","        split_positions.append((start, start + min_obser, start + min_obser + sliding_window))\n","        start += sliding_window\n","\n","    split_position_df = pd.DataFrame(split_positions, columns=['start', 'split', 'end'])\n","\n","    return split_position_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Description: Cross validation on time series data\n","Args:\n","    dataset: The dataset which needs to be splited\n","    params: Parameters which want to test \n","    cv_info: The type of cross validation [multi_splits | block_splits]\n","    model_name: Model name selected\n","    features_normalization: Indicates whether features should be normalized (True) or not (False)\n","    features: Features to be used to make predictions\n","    features_name: Name of features used\n","    features_label: The column name of features\n","    target_label: The column name of target variable\n","Return: \n","    results_lst_df: All the splits performances in a pandas dataset\n","'''\n","def cross_validation(dataset, params, cv_info, model_name, model_type, features_normalization, features, features_name, features_label, target_label):\n","    # Select the type of features to be used\n","    dataset = utilities.select_features(dataset, features_normalization, features, features_label, target_label)\n","\n","    # Get the number of samples\n","    num = dataset.count()\n","    \n","    # Save results in a list\n","    results_lst = []\n","\n","    # Initialize an empty list to store predictions\n","    predictions_list = []  \n","\n","    # Identify the type of cross validation \n","    if cv_info['cv_type'] == 'multi_splits':\n","        split_position_df = utilities.multi_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'block_splits':\n","        split_position_df = utilities.block_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'walk_forward_splits':\n","        split_position_df = walk_forward_splits_new(num, cv_info['min_obser'], cv_info['sliding_window'])\n","\n","    for position in split_position_df.itertuples():\n","        # Get the start/split/end position based on the type of cross validation\n","        start = getattr(position, 'start')\n","        splits = getattr(position, 'split')\n","        end = getattr(position, 'end')\n","        idx  = getattr(position, 'Index')\n","        \n","        # Train / validation size\n","        train_size = splits - start\n","        valid_size = end - splits\n","\n","        # Get training data and validation data\n","        train_data = dataset.filter(dataset['id'].between(start, splits-1))\n","        valid_data = dataset.filter(dataset['id'].between(splits, end-1))\n","\n","        # Cache them\n","        train_data.cache()\n","        valid_data.cache()\n","        \n","        # All combination of params\n","        param_lst = [dict(zip(params, param)) for param in product(*params.values())]\n","\n","        for param in param_lst:\n","            # Chosen Model\n","            model = utilities.model_selection(model_name, param, features_label, target_label)\n","\n","            # Chain assembler and model in a Pipeline\n","            pipeline = Pipeline(stages=[model])\n","\n","            # Train a model and calculate running time\n","            start = time.time()\n","            pipeline_model = pipeline.fit(train_data)\n","            end = time.time()\n","\n","            # Make predictions\n","            predictions = pipeline_model.transform(valid_data).select(target_label, \"prediction\", 'timestamp')\n","            \n","            # Append predictions to the list\n","            predictions_list.append(predictions)  \n","\n","            # Compute validation error by several evaluators\n","            eval_res = utilities.model_evaluation(target_label, predictions)\n","\n","            # Use dict to store each result\n","            results = {\n","                \"Model\": model_name,\n","                \"Type\": model_type,\n","                \"Cv\": cv_info['cv_type'],\n","                \"Features\": features_name,\n","                \"Splits\": idx + 1,\n","                \"Train&Validation\": (train_size,valid_size),                \n","                \"Parameters\": list(param.values()),\n","                \"RMSE\": eval_res['rmse'],\n","                \"MSE\": eval_res['mse'],\n","                \"MAE\": eval_res['mae'],\n","                \"MAPE\": eval_res['mape'],\n","                \"R2\": eval_res['r2'],\n","                \"Adjusted_R2\": eval_res['adj_r2'],\n","                \"Time\": end - start,\n","            }\n","\n","            # Store results for each split\n","            results_lst.append(results)\n","            print(results)\n","\n","        # Release Cache\n","        train_data.unpersist()\n","        valid_data.unpersist()\n","\n","    # Transform dict to pandas dataset\n","    results_lst_df = pd.DataFrame(results_lst)\n","\n","    # Create an empty DataFrame with the same schema as the predictions dataset\n","    final_predictions = spark.createDataFrame([], schema=predictions_list[0].schema)\n","\n","    # Iterate over the list of DataFrames and union them with the merged DataFrame\n","    for pred in predictions_list:\n","        final_predictions = final_predictions.union(pred)\n","\n","    return results_lst_df, final_predictions.toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using all the features\n","simple_res_all, simple_pred_all = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_res_all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_pred_all, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most correlated features\n","simple_res_most_corr, simple_pred_most_corr = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_CORR_FEATURES, MOST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_res_most_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_pred_most_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least correlated features\n","simple_res_least_corr, simple_pred_least_corr = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_CORR_FEATURES, LEAST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_res_least_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_pred_least_corr, MODEL_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["### Simple model (with normalization)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"simple_norm\"\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Valid performances with all the features\n","simple_norm_res_all, simple_norm_pred_all = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_norm_res_all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_norm_pred_all, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most the features\n","simple_norm_res_most_corr, simple_norm_pred_most_corr = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_CORR_FEATURES, MOST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_norm_res_most_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_norm_pred_most_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least the features\n","simple_norm_res_least_corr, simple_norm_pred_least_corr = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_CORR_FEATURES, LEAST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","simple_norm_res_least_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(simple_norm_pred_least_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model information and evaluators to show\n","model_info = ['Model', 'Type', 'Cv', 'Features', 'Parameters']\n","evaluator_lst = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adjusted_R2', 'Time']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","simple_comparison_lst = [simple_res_all, simple_res_most_corr, simple_res_least_corr,simple_norm_res_all, simple_norm_res_most_corr, simple_norm_res_least_corr]\n","\n","# Show the comparison table\n","simple_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in simple_comparison_lst])\n","simple_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["## Tuned ❗\n","Once the features and execution method are selected, the model will undergo hyperparameter tuning and cross validation to find the best configuration."]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter tuning with cross validation\n","The train / validation set is divided based on a portion list which will split the dataset into several splits.\n","\n","For each split, all combinations of the model parameters are tested and those that return a lower RMSE are considered.\n","\n","Using the previously selected parameters, the model undergoes two types of cross validation:"]},{"cell_type":"markdown","metadata":{},"source":["**Multiple splits**\n","\n","The idea is to divide the dataset into two folds at each iteration on condition that the validation set is always ahead of the training set. This way dependence is respected."]},{"cell_type":"markdown","metadata":{},"source":["**Blocked time series**\n","\n","It works by adding margins at two positions. The first is between the training and validation folds in order to prevent the model from observing lag values which are used twice, once as a regressor and another as a response. The second is between the folds used at each iteration in order to prevent the model from memorizing patterns from an iteration to the next."]},{"cell_type":"markdown","metadata":{},"source":["**Walk forward time series**\n","\n","The basic idea behind walk-forward validation is to iteratively train and evaluate the model using a sliding window approach. Here's how it works:\n","*  Split the time series data into a training set and a test set. The training set contains the initial portion of the data, while the test set contains the subsequent portion.\n","* Train the model on the training set and make predictions on the test set.\n","Evaluate the performance of the model on the test set using appropriate evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE).\n","* Move the sliding window forward by one step, incorporating the next data point into the training set and shifting the test set accordingly.\n","* Repeat steps 2-4 until the entire time series has been used for testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# From now on, only selected and normalized features will be considered\n","MODEL_TYPE = \"hyp_tuning\"\n","CHOSEN_FEATURES = ALL_FEATURES\n","CHOSEN_FEATURES_LABEL = ALL_FEATURES_NAME\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get model grid parameters\n","params = parameters.get_model_grid_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Description: Cross validation on time series data\n","Args:\n","    dataset: The dataset which needs to be splited\n","    params: Parameters which want to test \n","    cv_info: The type of cross validation [multi_splits | block_splits]\n","    model_name: Model name selected\n","    features_normalization: Indicates whether features should be normalized (True) or not (False)\n","    features: Features to be used to make predictions\n","    features_name: Name of features used\n","    features_label: The column name of features\n","    target_label: The column name of target variable\n","Return: \n","    results_lst_df: All the splits performances in a pandas dataset\n","'''\n","def hyperparameter_tuning(dataset, params, cv_info, model_name, model_type, features_normalization, features, features_name, features_label, target_label):\n","    # Select the type of features to be used\n","    dataset = utilities.select_features(dataset, features_normalization, features, features_label, target_label)\n","\n","    best_split_result = []\n","\n","    # Get the number of samples\n","    num = dataset.count()\n","\n","    # Identify the type of cross validation \n","    if cv_info['cv_type'] == 'multi_splits':\n","        split_position_df = utilities.multi_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'block_splits':\n","        split_position_df = utilities.block_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'walk_forward_splits':\n","        split_position_df = walk_forward_splits_new(num, cv_info['min_obser'], cv_info['sliding_window'])\n","\n","    for position in split_position_df.itertuples():\n","        best_result = {\"RMSE\": float('inf')}\n","\n","        # Get the start/split/end position based on the type of cross validation\n","        start = getattr(position, 'start')\n","        splits = getattr(position, 'split')\n","        end = getattr(position, 'end')\n","        idx  = getattr(position, 'Index')\n","        \n","        # Train / validation size\n","        train_size = splits - start\n","        valid_size = end - splits\n","\n","        # Get training data and validation data\n","        train_data = dataset.filter(dataset['id'].between(start, splits-1))\n","        valid_data = dataset.filter(dataset['id'].between(splits, end-1))\n","\n","        # Cache them\n","        train_data.cache()\n","        valid_data.cache()\n","\n","        # All combination of params\n","        param_lst = [dict(zip(params, param)) for param in product(*params.values())]\n","\n","        for param in param_lst:\n","            # Chosen Model\n","            model = utilities.model_selection(model_name, param, features_label, target_label)\n","\n","            # Chain assembler and model in a Pipeline\n","            pipeline = Pipeline(stages=[model])\n","\n","            # Train a model and calculate running time\n","            start = time.time()\n","            pipeline_model = pipeline.fit(train_data)\n","            end = time.time()\n","\n","            # Make predictions\n","            predictions = pipeline_model.transform(valid_data).select(target_label, \"prediction\", 'timestamp')\n","\n","            # Compute validation error by several evaluators\n","            eval_res = utilities.model_evaluation(target_label, predictions)\n","\n","            # Use dict to store each result\n","            results = {\n","                \"Model\": model_name,\n","                \"Type\": model_type,\n","                \"Cv\": cv_info['cv_type'],\n","                \"Features\": features_name,\n","                \"Splits\": idx + 1,\n","                \"Train&Validation\": (train_size,valid_size),                \n","                \"Parameters\": list(param.values()),\n","                \"RMSE\": eval_res['rmse'],\n","                \"MSE\": eval_res['mse'],\n","                \"MAE\": eval_res['mae'],\n","                \"MAPE\": eval_res['mape'],\n","                \"R2\": eval_res['r2'],\n","                \"Adjusted_R2\": eval_res['adj_r2'],\n","                \"Time\": end - start,\n","            }\n","            # Store the result with the lowest RMSE and the associated parameters\n","            if results['RMSE'] < best_result['RMSE']:\n","                best_result = results\n","\n","        # Release Cache\n","        train_data.unpersist()\n","        valid_data.unpersist()\n","\n","        best_split_result.append(best_result) \n","        print(best_result)\n","\n","    # Transform dict to pandas dataset\n","    best_split_result_df = pd.DataFrame(best_split_result)\n","\n","    return best_split_result_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter tuning\n","hyp_res = hyperparameter_tuning(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","hyp_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Count the occurrences of each value in the Parameters column\n","counts = hyp_res[\"Parameters\"].value_counts()\n","\n","# Display the counts\n","print(counts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"cross_val\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_best_model_params(model_name):\n","    if (model_name == 'LinearRegression'):\n","        params = {\n","            'maxIter' : [5],\n","            'regParam' : [0.8],\n","            'elasticNetParam' : [0.0]\n","        }   \n","    if (model_name == 'GeneralizedLinearRegression'):\n","        params = {\n","            'maxIter' : [5],\n","            'regParam' : [0.2],\n","            'family': ['gaussian'],\n","            'link': ['log']\n","        }\n","    elif (model_name == 'RandomForestRegressor'):\n","        params = {\n","            'numTrees' : [3],\n","            'maxDepth' : [10],\n","            'seed' : [42]\n","            }\n","    elif (model_name == 'GBTRegressor'):\n","        params = {\n","            'maxIter' : [30],\n","            'maxDepth' : [3],\n","            'stepSize': [0.4],\n","            'seed' : [42]\n","        }\n","        \n","    return params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get tuned parameters\n","params = get_best_model_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform cross validation\n","cv_res, cv_pred = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","cv_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(cv_pred, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","tuned_comparison_lst = [cv_res]\n","\n","# Show the comparison table\n","tuned_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in tuned_comparison_lst])\n","tuned_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["## Final ❗\n","Finally, the configuration found will be validated and then the model will be trained one last time on the entire train / validation set, ready to make predictions."]},{"cell_type":"markdown","metadata":{},"source":["### Validate final model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"final_validated\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Performances on validated final model\n","final_valid_res, final_valid_pred = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","final_valid_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(final_valid_pred, MODEL_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["### Train model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"final_trained\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Description: Cross validation on time series data\n","Args:\n","    dataset: The dataset which needs to be splited\n","    params: Parameters which want to test \n","    model_name: Model name selected\n","    model_type: Model type [simple | simple_norm | hyp_tuning | final_validated | final_trained]\n","    features_normalization: Indicates whether features should be normalized (True) or not (False)\n","    features: Features to be used to make predictions\n","    features_name: Name of features used\n","    features_label: The column name of features\n","    target_label: The column name of target variable\n","Return: \n","    results_df: Results obtained from the evaluation\n","    pipeline_model: Final trained model\n","    predictions: Predictions obtained from the model\n","'''\n","def evaluate_trained_model(dataset, params, model_name, model_type, features_normalization, features, features_name, features_label, target_label):    \n","    # Select the type of features to be used\n","    dataset = utilities.select_features(dataset, features_normalization, features, features_label, target_label)\n","  \n","    # All combination of params\n","    param_lst = [dict(zip(params, param)) for param in product(*params.values())]\n","    \n","    for param in param_lst:\n","        # Chosen Model\n","        model = utilities.model_selection(model_name, param, features_label, target_label)\n","        \n","        # Chain assembler and model in a Pipeline\n","        pipeline = Pipeline(stages=[model])\n","\n","        # Train a model and calculate running time\n","        start = time.time()\n","        pipeline_model = pipeline.fit(dataset)\n","        end = time.time()\n","\n","        # Make predictions\n","        predictions = pipeline_model.transform(dataset).select(target_label, \"prediction\", 'timestamp')\n","\n","        # Compute validation error by several evaluators\n","        eval_res = utilities.model_evaluation(target_label, predictions)\n","\n","        # Use dict to store each result\n","        results = {\n","            \"Model\": model_name,\n","            \"Type\": model_type,\n","            \"Cv\": \"none\",\n","            \"Features\": features_name,\n","            \"Parameters\": [list(param.values())],\n","            \"RMSE\": eval_res['rmse'],\n","            \"MSE\": eval_res['mse'],\n","            \"MAE\": eval_res['mae'],\n","            \"MAPE\": eval_res['mape'],\n","            \"R2\": eval_res['r2'],\n","            \"Adjusted_R2\": eval_res['adj_r2'],\n","            \"Time\": end - start,\n","        }\n","\n","    # Transform dict to pandas dataset\n","    results_df = pd.DataFrame(results)\n","        \n","    return results_df, pipeline_model, predictions.toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model on the whole train / validation set\n","final_train_res, final_train_model, final_train_pred = evaluate_trained_model(df, params, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","final_train_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(final_train_pred, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","valid_comparison_lst = [final_valid_res, final_train_res]\n","\n","# Show the comparison table\n","valid_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in valid_comparison_lst])\n","valid_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["# Comparison table\n","Visualization of model performance at various stages of train / validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate simple results into Pandas Dataframe\n","final_comparison_lst_df = pd.DataFrame(pd.concat([simple_comparison_lst_df, tuned_comparison_lst_df , valid_comparison_lst_df], ignore_index=True))\n","final_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["# Model accuracy ❗"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# cv_info = parameters.get_cross_validation_params('multi_splits')\n","cv_info = parameters.get_cross_validation_params('block_splits')\n","# cv_info = parameters.get_cross_validation_params('walk_forward_splits')\n","cv_info\n","\n","# From now on, only selected and normalized features will be considered\n","MODEL_TYPE = \"hyp_tuning\"\n","CHOSEN_FEATURES = ALL_FEATURES\n","CHOSEN_FEATURES_LABEL = ALL_FEATURES_NAME\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def get_best_model_params(model_name):\n","    if (model_name == 'LinearRegression'):\n","        params = {\n","            'maxIter' : [5],\n","            'regParam' : [0.8],\n","            'elasticNetParam' : [0.0]\n","        }   \n","    if (model_name == 'GeneralizedLinearRegression'):\n","        params = {\n","            'maxIter' : [5],\n","            'regParam' : [0.2],\n","            'family': ['gaussian'],\n","            'link': ['log']\n","        }\n","    elif (model_name == 'RandomForestRegressor'):\n","        params = {\n","            'numTrees' : [3],\n","            'maxDepth' : [10],\n","            'seed' : [42]\n","            }\n","    elif (model_name == 'GBTRegressor'):\n","        params = {\n","            'maxIter' : [30],\n","            'maxDepth' : [3],\n","            'stepSize': [0.4],\n","            'seed' : [42]\n","        }\n","        \n","    return params"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["{'numTrees': [3], 'maxDepth': [10], 'seed': [42]}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Get tuned parameters\n","params = get_best_model_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["'''\n","Description: Return the dataset with the selected features\n","Args:\n","    dataset: The dataset from which to extract the features\n","    features_normalization: Indicates whether features should be normalized (True) or not (False)\n","    features: list of features to be extracted\n","    features_label: The column name of features\n","    target_label: The column name of target variable\n","Return: \n","    dataset: Dataset with the selected features\n","'''\n","def select_features(dataset, features_normalization, features, features_label, target_label):\n","    if features_normalization:\n","        # Assemble the columns into a vector column\n","        assembler = VectorAssembler(inputCols = features, outputCol = \"raw_features\")\n","        df_vector  = assembler.transform(dataset).select(\"timestamp\", \"id\", \"market-price\", \"raw_features\", target_label)\n","\n","        # Create a Normalizer instance\n","        normalizer = Normalizer(inputCol=\"raw_features\", outputCol=features_label)\n","\n","        # Fit and transform the data\n","        dataset = normalizer.transform(df_vector).select(\"timestamp\", \"id\", \"market-price\",features_label, target_label)\n","    else:\n","        # Assemble the columns into a vector column\n","        vectorAssembler = VectorAssembler(inputCols = features, outputCol = features_label)\n","        dataset = vectorAssembler.transform(dataset).select(\"timestamp\", \"id\", \"market-price\", features_label, target_label)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["'''\n","Description: Cross validation on time series data\n","Args:\n","    dataset: The dataset which needs to be splited\n","    params: Parameters which want to test \n","    cv_info: The type of cross validation [multi_splits | block_splits]\n","    model_name: Model name selected\n","    features_normalization: Indicates whether features should be normalized (True) or not (False)\n","    features: Features to be used to make predictions\n","    features_name: Name of features used\n","    features_label: The column name of features\n","    target_label: The column name of target variable\n","Return: \n","    results_lst_df: All the splits performances in a pandas dataset\n","'''\n","def cross_validation(dataset, params, cv_info, model_name, model_type, features_normalization, features, features_name, features_label, target_label):\n","    # Select the type of features to be used\n","    dataset = select_features(dataset, features_normalization, features, features_label, target_label)\n","\n","    # Get the number of samples\n","    num = dataset.count()\n","    \n","    # Save results in a list\n","    results_lst = []\n","\n","    # Initialize an empty list to store predictions\n","    predictions_list = []  \n","\n","    # Identify the type of cross validation \n","    if cv_info['cv_type'] == 'multi_splits':\n","        split_position_df = utilities.multi_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'block_splits':\n","        split_position_df = utilities.block_splits(num, cv_info['splits'])\n","    elif cv_info['cv_type'] == 'walk_forward_splits':\n","        split_position_df = walk_forward_splits_new(num, cv_info['min_obser'], cv_info['sliding_window'])\n","\n","    for position in split_position_df.itertuples():\n","        # Get the start/split/end position based on the type of cross validation\n","        start = getattr(position, 'start')\n","        splits = getattr(position, 'split')\n","        end = getattr(position, 'end')\n","        idx  = getattr(position, 'Index')\n","        \n","        # Train / validation size\n","        train_size = splits - start\n","        valid_size = end - splits\n","\n","        # Get training data and validation data\n","        train_data = dataset.filter(dataset['id'].between(start, splits-1))\n","        valid_data = dataset.filter(dataset['id'].between(splits, end-1))\n","\n","        # Cache them\n","        train_data.cache()\n","        valid_data.cache()\n","        \n","        # All combination of params\n","        param_lst = [dict(zip(params, param)) for param in product(*params.values())]\n","\n","        for param in param_lst:\n","            # Chosen Model\n","            model = utilities.model_selection(model_name, param, features_label, target_label)\n","\n","            # Chain assembler and model in a Pipeline\n","            pipeline = Pipeline(stages=[model])\n","\n","            # Train a model and calculate running time\n","            start = time.time()\n","            pipeline_model = pipeline.fit(train_data)\n","            end = time.time()\n","\n","            # Make predictions\n","            predictions = pipeline_model.transform(valid_data).select(target_label, \"market-price\", \"prediction\", 'timestamp')\n","            \n","            # Append predictions to the list\n","            predictions_list.append(predictions)  \n","\n","            # Compute validation error by several evaluators\n","            eval_res = utilities.model_evaluation(target_label, predictions)\n","\n","            # Use dict to store each result\n","            results = {\n","                \"Model\": model_name,\n","                \"Type\": model_type,\n","                \"Cv\": cv_info['cv_type'],\n","                \"Features\": features_name,\n","                \"Splits\": idx + 1,\n","                \"Train&Validation\": (train_size,valid_size),                \n","                \"Parameters\": list(param.values()),\n","                \"RMSE\": eval_res['rmse'],\n","                \"MSE\": eval_res['mse'],\n","                \"MAE\": eval_res['mae'],\n","                \"MAPE\": eval_res['mape'],\n","                \"R2\": eval_res['r2'],\n","                \"Adjusted_R2\": eval_res['adj_r2'],\n","                \"Time\": end - start,\n","            }\n","\n","            # Store results for each split\n","            results_lst.append(results)\n","            print(results)\n","\n","        # Release Cache\n","        train_data.unpersist()\n","        valid_data.unpersist()\n","\n","    # Transform dict to pandas dataset\n","    results_lst_df = pd.DataFrame(results_lst)\n","\n","    # Create an empty DataFrame with the same schema as the predictions dataset\n","    final_predictions = spark.createDataFrame([], schema=predictions_list[0].schema)\n","\n","    # Iterate over the list of DataFrames and union them with the merged DataFrame\n","    for pred in predictions_list:\n","        final_predictions = final_predictions.union(pred)\n","\n","    return results_lst_df, final_predictions.toPandas()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 1, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 48.02398289916037, 'MSE': 2306.302933498848, 'MAE': 34.140728130413514, 'MAPE': 0.05798092849637084, 'R2': -8.852521975525253, 'Adjusted_R2': -8.86773235294791, 'Time': 5.2688515186309814}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 2, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 955.093230262068, 'MSE': 912203.0784924317, 'MAE': 847.3023256716388, 'MAPE': 0.3564685760850903, 'R2': -3.645366558354824, 'Adjusted_R2': -3.652538100706587, 'Time': 2.5356874465942383}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 3, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 1912.8355236380391, 'MSE': 3658939.7404916114, 'MAE': 1533.2528326703302, 'MAPE': 0.16447343197824496, 'R2': -1.8424621181976462, 'Adjusted_R2': -1.8468503267938603, 'Time': 2.810882568359375}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 4, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 1750.9038537700644, 'MSE': 3065664.305146863, 'MAE': 1328.7457161098741, 'MAPE': 0.3435827189306715, 'R2': -0.9677025040459188, 'Adjusted_R2': -0.9707402539556769, 'Time': 2.8718111515045166}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 5, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 959.0565344431087, 'MSE': 919789.4362580258, 'MAE': 836.7934593348224, 'MAPE': 0.07947827843155324, 'R2': -1.366147478752961, 'Adjusted_R2': -1.3698003501983536, 'Time': 3.363723039627075}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 6, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 1021.5893059572937, 'MSE': 1043644.7100463051, 'MAE': 859.4297389065267, 'MAPE': 0.09238685953789391, 'R2': -0.238883088851086, 'Adjusted_R2': -0.24079568335336465, 'Time': 2.6190757751464844}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 7, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 8459.716422447606, 'MSE': 71566801.9482297, 'MAE': 6794.8802088287175, 'MAPE': 0.14844955645089492, 'R2': -0.22299027784967085, 'Adjusted_R2': -0.224878336943225, 'Time': 2.6120445728302}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 8, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 6703.827210312581, 'MSE': 44941299.26572736, 'MAE': 6326.364172011644, 'MAPE': 0.10328713485310963, 'R2': -3.735910711166621, 'Adjusted_R2': -3.743222036077724, 'Time': 2.3877570629119873}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 9, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 1485.0504518766177, 'MSE': 2205374.8446189463, 'MAE': 1100.3987991335155, 'MAPE': 0.04861772475682614, 'R2': -0.1687679528136783, 'Adjusted_R2': -0.17057230318467576, 'Time': 2.2787275314331055}\n","{'Model': 'RandomForestRegressor', 'Type': 'final_validated', 'Cv': 'block_splits', 'Features': 'all_features', 'Splits': 10, 'Train&Validation': (10383, 2596), 'Parameters': [3, 10, 42], 'RMSE': 1138.7293330714685, 'MSE': 1296704.4939973915, 'MAE': 950.7753328254711, 'MAPE': 0.03302694388642647, 'R2': 0.04379616248936524, 'Adjusted_R2': 0.04231996976453212, 'Time': 2.1337835788726807}\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Type</th>\n","      <th>Cv</th>\n","      <th>Features</th>\n","      <th>Splits</th>\n","      <th>Train&amp;Validation</th>\n","      <th>Parameters</th>\n","      <th>RMSE</th>\n","      <th>MSE</th>\n","      <th>MAE</th>\n","      <th>MAPE</th>\n","      <th>R2</th>\n","      <th>Adjusted_R2</th>\n","      <th>Time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>1</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>48.023983</td>\n","      <td>2.306303e+03</td>\n","      <td>34.140728</td>\n","      <td>0.057981</td>\n","      <td>-8.852522</td>\n","      <td>-8.867732</td>\n","      <td>5.268852</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>2</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>955.093230</td>\n","      <td>9.122031e+05</td>\n","      <td>847.302326</td>\n","      <td>0.356469</td>\n","      <td>-3.645367</td>\n","      <td>-3.652538</td>\n","      <td>2.535687</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>3</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>1912.835524</td>\n","      <td>3.658940e+06</td>\n","      <td>1533.252833</td>\n","      <td>0.164473</td>\n","      <td>-1.842462</td>\n","      <td>-1.846850</td>\n","      <td>2.810883</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>4</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>1750.903854</td>\n","      <td>3.065664e+06</td>\n","      <td>1328.745716</td>\n","      <td>0.343583</td>\n","      <td>-0.967703</td>\n","      <td>-0.970740</td>\n","      <td>2.871811</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>5</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>959.056534</td>\n","      <td>9.197894e+05</td>\n","      <td>836.793459</td>\n","      <td>0.079478</td>\n","      <td>-1.366147</td>\n","      <td>-1.369800</td>\n","      <td>3.363723</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>6</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>1021.589306</td>\n","      <td>1.043645e+06</td>\n","      <td>859.429739</td>\n","      <td>0.092387</td>\n","      <td>-0.238883</td>\n","      <td>-0.240796</td>\n","      <td>2.619076</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>7</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>8459.716422</td>\n","      <td>7.156680e+07</td>\n","      <td>6794.880209</td>\n","      <td>0.148450</td>\n","      <td>-0.222990</td>\n","      <td>-0.224878</td>\n","      <td>2.612045</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>8</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>6703.827210</td>\n","      <td>4.494130e+07</td>\n","      <td>6326.364172</td>\n","      <td>0.103287</td>\n","      <td>-3.735911</td>\n","      <td>-3.743222</td>\n","      <td>2.387757</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>9</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>1485.050452</td>\n","      <td>2.205375e+06</td>\n","      <td>1100.398799</td>\n","      <td>0.048618</td>\n","      <td>-0.168768</td>\n","      <td>-0.170572</td>\n","      <td>2.278728</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>RandomForestRegressor</td>\n","      <td>final_validated</td>\n","      <td>block_splits</td>\n","      <td>all_features</td>\n","      <td>10</td>\n","      <td>(10383, 2596)</td>\n","      <td>[3, 10, 42]</td>\n","      <td>1138.729333</td>\n","      <td>1.296704e+06</td>\n","      <td>950.775333</td>\n","      <td>0.033027</td>\n","      <td>0.043796</td>\n","      <td>0.042320</td>\n","      <td>2.133784</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   Model             Type            Cv      Features  Splits  \\\n","0  RandomForestRegressor  final_validated  block_splits  all_features       1   \n","1  RandomForestRegressor  final_validated  block_splits  all_features       2   \n","2  RandomForestRegressor  final_validated  block_splits  all_features       3   \n","3  RandomForestRegressor  final_validated  block_splits  all_features       4   \n","4  RandomForestRegressor  final_validated  block_splits  all_features       5   \n","5  RandomForestRegressor  final_validated  block_splits  all_features       6   \n","6  RandomForestRegressor  final_validated  block_splits  all_features       7   \n","7  RandomForestRegressor  final_validated  block_splits  all_features       8   \n","8  RandomForestRegressor  final_validated  block_splits  all_features       9   \n","9  RandomForestRegressor  final_validated  block_splits  all_features      10   \n","\n","  Train&Validation   Parameters         RMSE           MSE          MAE  \\\n","0    (10383, 2596)  [3, 10, 42]    48.023983  2.306303e+03    34.140728   \n","1    (10383, 2596)  [3, 10, 42]   955.093230  9.122031e+05   847.302326   \n","2    (10383, 2596)  [3, 10, 42]  1912.835524  3.658940e+06  1533.252833   \n","3    (10383, 2596)  [3, 10, 42]  1750.903854  3.065664e+06  1328.745716   \n","4    (10383, 2596)  [3, 10, 42]   959.056534  9.197894e+05   836.793459   \n","5    (10383, 2596)  [3, 10, 42]  1021.589306  1.043645e+06   859.429739   \n","6    (10383, 2596)  [3, 10, 42]  8459.716422  7.156680e+07  6794.880209   \n","7    (10383, 2596)  [3, 10, 42]  6703.827210  4.494130e+07  6326.364172   \n","8    (10383, 2596)  [3, 10, 42]  1485.050452  2.205375e+06  1100.398799   \n","9    (10383, 2596)  [3, 10, 42]  1138.729333  1.296704e+06   950.775333   \n","\n","       MAPE        R2  Adjusted_R2      Time  \n","0  0.057981 -8.852522    -8.867732  5.268852  \n","1  0.356469 -3.645367    -3.652538  2.535687  \n","2  0.164473 -1.842462    -1.846850  2.810883  \n","3  0.343583 -0.967703    -0.970740  2.871811  \n","4  0.079478 -1.366147    -1.369800  3.363723  \n","5  0.092387 -0.238883    -0.240796  2.619076  \n","6  0.148450 -0.222990    -0.224878  2.612045  \n","7  0.103287 -3.735911    -3.743222  2.387757  \n","8  0.048618 -0.168768    -0.170572  2.278728  \n","9  0.033027  0.043796     0.042320  2.133784  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["MODEL_TYPE = \"final_validated\"\n","\n","# Performances on validated final model\n","final_valid_res, final_valid_pred = cross_validation(df, params, cv_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","final_valid_res"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["'''\n","Description: How good the models are at predicting whether the price will go up or down\n","\n","Dato un dataset che contiene le colonne timestamp, market-price, next-market-price, prediction\n","Per ogni riga prendo in considerazione il valore di market-price, \n","next-market-price e di prediction, se market-price < next-market-price -> il prezzo originale sale, \n","se anche la prediction per quel giorno prevede che market-price < prediction -> 1, ho previsto correttamente che il prezzo è salito (stessa cosa se il prezzo scende)\n","\n","Mentre se market-price < next-market-price -> il prezzo sale, se market-price > prediction-> 0, ho sbagliato la prediction\n","E così via finché non ho terminato tutto il set preso in considerazione, una volta finito mostro in percentuale \n","quanti 1 e quanti 0 ho ottenuto e, se la percentuale supera il 50%, potrei dire di essere stato bravo\n","\n","Args:\n","    dataset: The dataset which needs to be splited\n","    model_name: Model name selected\n","    model_type: Model type [simple | simple_norm | hyp_tuning | final_validated | final_trained]\n","Return: \n","    accuracy: Return the percentage of correct predictions\n","'''\n","def model_accuracy(dataset, model_name, model_type):    \n","    # Compute the number of total rows in the DataFrame.\n","    total_rows = dataset.count()\n","\n","    # Create a column \"correct_prediction\" which is worth 1 if the prediction is correct, otherwise 0\n","    dataset = dataset.withColumn(\n","        \"correct_prediction\",\n","        (\n","            (col(\"market-price\") < col(\"next-market-price\")) & (col(\"market-price\") < col(\"prediction\"))\n","        ) | (\n","            (col(\"market-price\") > col(\"next-market-price\")) & (col(\"market-price\") > col(\"prediction\"))\n","        )\n","    )\n","\n","    # Count the number of correct predictions\n","    correct_predictions = dataset.filter(col(\"correct_prediction\")).count()\n","\n","    # Compite percentage of correct predictions\n","    accuracy = (correct_predictions / total_rows) * 100\n","        \n","    return accuracy"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Percentage of correct predictions: 45.33%\n"]}],"source":["accuracy = model_accuracy(spark.createDataFrame(final_valid_pred), MODEL_NAME, MODEL_TYPE)\n","print(f\"Percentage of correct predictions: {accuracy:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["# Saving trained model\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'final_comparison_lst_df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\Documents\\Repository\\BDC\\project\\notebooks\\5. Random Forest Regressor_ACCURACY.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/5.%20Random%20Forest%20Regressor_ACCURACY.ipynb#Y323sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Saving final model results\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Repository/BDC/project/notebooks/5.%20Random%20Forest%20Regressor_ACCURACY.ipynb#Y323sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m final_comparison_lst_df\u001b[39m.\u001b[39mto_csv(MODEL_RESULTS, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'final_comparison_lst_df' is not defined"]}],"source":["# Saving final model results\n","final_comparison_lst_df.to_csv(MODEL_RESULTS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the trained model\n","final_train_model.write().overwrite().save(MODEL)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"}}},"nbformat":4,"nbformat_minor":0}
