{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Bitcoin price prediction - Generalized Linear Regression**\n","### Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n","\n","\n","---\n","\n","\n","Description: executing the chosen model, first with default values, then by choosing the best parameters by performing hyperparameter tuning with cross validation and performance evaluation. Finally validate the tuned model and train it on the whole train / validation set"]},{"cell_type":"markdown","metadata":{},"source":["# Global constants, dependencies, libraries and tools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Main constants\n","LOCAL_RUNNING = True\n","SLOW_OPERATIONS = True # Decide whether or not to use operations that might slow down notebook execution\n","MODEL_NAME = \"GeneralizedLinearRegression\"\n","MAIN_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING: \n","    # Point Colaboratory to Google Drive\n","    from google.colab import drive\n","\n","    # Define GDrive paths\n","    drive.mount(MAIN_DIR, force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set main dir\n","MAIN_DIR = MAIN_DIR + \"\" if LOCAL_RUNNING else \"/MyDrive/BDC/project\"\n","\n","###################\n","# --- DATASET --- #\n","###################\n","\n","# Datasets dirs\n","DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n","\n","# Datasets names\n","DATASET_TRAIN_VALID_NAME = \"bitcoin_blockchain_data_15min_train_valid\"\n","\n","# Datasets paths\n","DATASET_TRAIN_VALID  = DATASET_OUTPUT_DIR + \"/\" + DATASET_TRAIN_VALID_NAME + \".parquet\"\n","\n","####################\n","# --- FEATURES --- #\n","####################\n","\n","# Features dir\n","FEATURES_DIR = MAIN_DIR + \"/features\"\n","\n","# Features labels\n","FEATURES_LABEL = \"features\"\n","TARGET_LABEL = \"next-market-price\"\n","\n","# Features names\n","ALL_FEATURES_NAME = \"all_features\"\n","MOST_CORR_FEATURES_NAME = \"most_corr_features\"\n","LEAST_CORR_FEATURES_NAME = \"least_corr_features\"\n","\n","# Features paths\n","ALL_FEATURES = FEATURES_DIR + \"/\" + ALL_FEATURES_NAME + \".json\"\n","MOST_CORR_FEATURES = FEATURES_DIR + \"/\" + MOST_CORR_FEATURES_NAME + \".json\"\n","LEAST_CORR_FEATURES = FEATURES_DIR + \"/\" + LEAST_CORR_FEATURES_NAME + \".json\"\n","\n","##################\n","# --- MODELS --- #\n","##################\n","\n","# Model dir\n","MODELS_DIR = MAIN_DIR + \"/models\"\n","\n","# Model path\n","MODEL = MODELS_DIR + \"/\" + MODEL_NAME\n","\n","#####################\n","# --- UTILITIES --- #\n","#####################\n","\n","# Utilities dir\n","UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n","\n","###################\n","# --- RESULTS --- #\n","###################\n","\n","# Results dir\n","RESULTS_DIR = MAIN_DIR + \"/results/short_term_split\"\n","\n","# Results path\n","MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \".csv\"\n","MODEL_ACCURACY_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_accuracy.json\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Suppression of warnings for better reading\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING:\n","    # Install Spark and related dependencies\n","    !pip install pyspark\n","    !pip install -U -q PyDrive -qq\n","    !apt install openjdk-8-jdk-headless -qq"]},{"cell_type":"markdown","metadata":{},"source":["# Import files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import my files\n","import sys\n","sys.path.append(UTILITIES_DIR)\n","\n","from imports import *\n","import utilities, parameters\n","\n","importlib.reload(utilities)\n","importlib.reload(parameters)"]},{"cell_type":"markdown","metadata":{},"source":["# Create the pyspark session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '12G').\\\n","                set('spark.driver.memory', '12G').\\\n","                set('spark.driver.maxResultSize', '109G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPricePrediction\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load train / validation set into pyspark dataset objects\n","df = spark.read.load(DATASET_TRAIN_VALID,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dataset_info(dataset):\n","  # Print dataset\n","  dataset.show(3)\n","\n","  # Get the number of rows\n","  num_rows = dataset.count()\n","\n","  # Get the number of columns\n","  num_columns = len(dataset.columns)\n","\n","  # Print the shape of the dataset\n","  print(\"Shape:\", (num_rows, num_columns))\n","\n","  # Print the schema of the dataset\n","  dataset.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(df)"]},{"cell_type":"markdown","metadata":{},"source":["# Loading features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading all the features\n","with open(ALL_FEATURES, \"r\") as f:\n","    ALL_FEATURES = json.load(f)\n","print(ALL_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the most correlated features\n","with open(MOST_CORR_FEATURES, \"r\") as f:\n","    MOST_CORR_FEATURES = json.load(f)\n","print(MOST_CORR_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading least correlated features\n","with open(LEAST_CORR_FEATURES, \"r\") as f:\n","    LEAST_CORR_FEATURES = json.load(f)\n","print(LEAST_CORR_FEATURES)"]},{"cell_type":"markdown","metadata":{},"source":["# ❗Model train / validation\n","In order to train and validate the model, I'll try several approaches:\n","- **Default without normalization:** Make predictions using the chosen base model\n","- **Default with normalization:** Like the previous one but features are normalized\n","\n","At this point, the features that gave on average the most satisfactory results (for each model) are chosen and proceeded with:\n","\n","- **Hyperparameter tuning:** Researching the best parameters to use\n","- **Cross Validation:** validate the performance of the model with the chosen parameters\n","---\n","For each approach the train / validation set will be split according to the chosen splitting method (in order to figure out which one works best for our problem):\n","\n","**Block time series split**\n","\n","Involves dividing the time series into blocks of equal length, and then using each block as a separate fold for cross-validation.\n","\n","❗TODO: INSERT IMAGE HERE\n","\n","**Walk forward time series split**\n","\n","Involves using a sliding window approach to create the training and validation sets for each fold. The model is trained on a fixed window of historical data, and then tested on the next observation in the time series. This process is repeated for each subsequent observation, with the window sliding forward one step at a time. \n","\n","❗TODO: INSERT IMAGE HERE\n","\n","**Short term time series split**\n","\n","Involves dividing the time series considering as validation set a narrow period of time (3 months in our case) and as train set everything that happened before this period, in such a way as to best benefit from the trend in the short term.\n","\n","❗TODO: INSERT IMAGE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get splitting parameters based on the choosen splitting method\n","splitting_info = parameters.get_splitting_params('short_term_split')\n","splitting_info"]},{"cell_type":"markdown","metadata":{},"source":["## Default\n","The train / validation set will be splitted Based on the splitting method chosen so that the model performance can be seen without any tuning by using different features (normalized and non)"]},{"cell_type":"markdown","metadata":{},"source":["### Without normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"default\"\n","FEATURES_NORMALIZATION = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get default parameters\n","params = parameters.get_defaults_model_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using all the features\n","default_res_all, default_pred_all = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_res_all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_pred_all, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most correlated features\n","default_res_most_corr, default_pred_most_corr = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_CORR_FEATURES, MOST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_res_most_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_pred_most_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least correlated features\n","default_res_least_corr, default_pred_least_corr = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_CORR_FEATURES, LEAST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_res_least_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_pred_least_corr, MODEL_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["### With normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"default_norm\"\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Valid performances with all the features\n","default_norm_res_all, default_norm_pred_all = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_norm_res_all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_norm_pred_all, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most the features\n","default_norm_res_most_corr, default_norm_pred_most_corr = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, MOST_CORR_FEATURES, MOST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_norm_res_most_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_norm_pred_most_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least the features\n","default_norm_res_least_corr, default_norm_pred_least_corr = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_CORR_FEATURES, LEAST_CORR_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)\n","default_norm_res_least_corr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(default_norm_pred_least_corr, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model information and evaluators to show\n","model_info = ['Model', 'Type', 'Splitting', 'Features', 'Parameters']\n","evaluator_lst = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adjusted_R2', 'Time']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","default_comparison_lst = [default_res_all, default_res_most_corr, default_res_least_corr, default_norm_res_all, default_norm_res_most_corr, default_norm_res_least_corr]\n","\n","# Show the comparison table\n","default_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in default_comparison_lst])\n","default_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["❗TODO: INSERT DEFAULT RECAP HERE"]},{"cell_type":"markdown","metadata":{},"source":["## ❗Tuned\n","Once the features and execution method are selected, the model will undergo hyperparameter tuning and cross validation to find the best configuration. \n","\n","For each split, all combinations of the model parameters are tested and those that return a lower RMSE are considered. Using the previously selected parameters, cross valdiation is performed to see overall performance."]},{"cell_type":"markdown","metadata":{},"source":["### ❗Hyperparameter tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# From now on, only selected and normalized features will be considered\n","MODEL_TYPE = \"hyp_tuning\"\n","CHOSEN_FEATURES = MOST_CORR_FEATURES\n","CHOSEN_FEATURES_LABEL = MOST_CORR_FEATURES_NAME\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get model grid parameters\n","params = parameters.get_model_grid_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter tuning\n","hyp_res = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","hyp_res"]},{"cell_type":"markdown","metadata":{},"source":["❗TODO: INSERT HYP TUNING RECAP HERE"]},{"cell_type":"markdown","metadata":{},"source":["---\n","To select the best parameters to be used in the final model I assign a score to each value in the \"Parameters\" column based on the following criteria:\n","* Calculate the frequency of each unique value in the \"Parameters\" column.\n","* Normalize the frequencies to a scale of 0 to 1, where 1 represents the highest frequency.\n","* Calculate the split weight for each value in the \"Parameters\" column, where a higher split number corresponds to a higher weight.\n","* Normalize the split weights to a scale of 0 to 1, where 1 represents the highest split weight.\n","* Calculate the RMSE weight for each value in the \"Parameters\" column, where a lower RMSE value corresponds to a higher weight.\n","* Normalize the RMSE weights to a scale of 0 to 1, where 1 represents the highest RMSE weight.\n","* Calculate the overall score for each value in the \"Parameters\" column by combining the normalized frequency, split weight, and RMSE weight.\n","\n","I take into consideration the parameters that have the highest score."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grouped_scores, best_params = parameters.choose_best_params(hyp_res)\n","grouped_scores"]},{"cell_type":"markdown","metadata":{},"source":["### ❗Cross validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"cross_val\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get tuned parameters\n","params = parameters.get_best_model_params(best_params, MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform cross validation\n","cv_res, cv_pred = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","cv_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(cv_pred, MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","tuned_comparison_lst = [cv_res]\n","\n","# Show the comparison table\n","tuned_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in tuned_comparison_lst])\n","tuned_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["# Train final model\n","At this point it is possible to tow the final model over the entire train/validation set in order to use it in the final step, that is, to make predictions about the data contained in the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"final_trained\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the final model on the whole train / validation set\n","train_res, final_model, train_pred = utilities.evaluate_trained_model(df, params, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","train_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["utilities.show_results(train_pred, MODEL_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["# Comparison table\n","Visualization of model performance at various stages of train / validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate final results into Pandas dataset\n","final_comparison_lst_df = pd.DataFrame(pd.concat([default_comparison_lst_df, tuned_comparison_lst_df, train_res], ignore_index=True))\n","final_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["# ❗Model accuracy\n","\n","❗TODO: INSERT FORMULA AND EXPLANATION HERE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the pandas dataset to a PySpark dataset\n","final_valid_pred_spark = spark.createDataFrame(cv_pred)\n","\n","# Compute model accuracy\n","accuracy = utilities.model_accuracy(final_valid_pred_spark)\n","accuracy = f\"Percentage of correct predictions for {MODEL_NAME, MODEL_TYPE}: {accuracy:.2f}%\"\n","\n","print(accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["❗TODO: INSERT FINAL CONSIDERATION HERE"]},{"cell_type":"markdown","metadata":{},"source":["# Saving final results\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving final model\n","final_model.write().overwrite().save(MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving final model results\n","final_comparison_lst_df.to_csv(MODEL_RESULTS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving final accuracy results\n","with open(MODEL_ACCURACY_RESULTS, 'w') as file:\n","    json.dump(accuracy, file)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"}}},"nbformat":4,"nbformat_minor":0}
