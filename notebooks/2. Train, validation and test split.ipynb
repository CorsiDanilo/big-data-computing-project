{"cells":[{"cell_type":"markdown","metadata":{"id":"e452Ydj7chNV"},"source":["# Bitcoin price forecasting with PySpark\n","## Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author\n","Corsi Danilo - corsi.1742375@studenti.uniroma1.it\n","\n"]},{"cell_type":"markdown","source":["Description: In this notebook I am going to split the dataset into train and test sets by saving them separately on the Google Drive."],"metadata":{"id":"C2jVGnfOWo2t"}},{"cell_type":"markdown","metadata":{"id":"AkQaY6VAf4v_"},"source":["# Dependencies, Libraries and Tools"]},{"cell_type":"code","source":["JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","SLOW_OPERATION = False"],"metadata":{"id":"8o8VQRGycO4U","executionInfo":{"status":"ok","timestamp":1691560251172,"user_tz":-120,"elapsed":3,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PX7xDYw4SvrB","executionInfo":{"status":"ok","timestamp":1691560257655,"user_tz":-120,"elapsed":6486,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"outputs":[],"source":["import requests\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from itertools import cycle\n","\n","import plotly.express as px\n","\n","import plotly.graph_objs as go\n","from plotly.offline import init_notebook_mode, iplot\n","import gc\n","\n","#General System Utilities\n","import sys\n","from datetime import datetime\n","import pickle\n","\n","#Data Processing Libraries\n","import numpy as np\n","import pandas as pd\n","from pandas import concat\n","import matplotlib.pyplot as plt\n","from fastai.tabular import *\n","import six\n","\n","#DS/DL Libs\n","import sklearn\n","from sklearn.linear_model import LinearRegression as sklearnLR\n","from sklearn.feature_selection import RFE\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM, GRU\n","from keras import optimizers\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66910,"status":"ok","timestamp":1691560324562,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"},"user_tz":-120},"id":"Si82CaUYSvrA","outputId":"7647a034-2a19-4200-8e31-67c49adb550c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=d1b9e8829229d39e6906ac565646d5f3360d831a2030a7682f932b1beb00c369\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n","The following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n","Need to get 39.7 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 120511 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n"]}],"source":["!pip install pyspark\n","# Alternatively, if you want to install a specific version of pyspark:\n","#!pip install pyspark==3.2.1\n","!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = JAVA_HOME\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","\n","from pyspark.sql import functions as F"]},{"cell_type":"markdown","metadata":{"id":"YtmWjQUVSvq2"},"source":["# Link to Google Drive"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Fhi5bmOeSvrB","executionInfo":{"status":"ok","timestamp":1691560332071,"user_tz":-120,"elapsed":7513,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '4G').\\\n","                set('spark.driver.memory', '45G').\\\n","                set('spark.driver.maxResultSize', '10G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPriceForecasting\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","source":["GDRIVE_DIR = \"/content/drive\"\n","\n","GDRIVE_DATASET_RAW_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/raw\"\n","GDRIVE_DATASET_TEMP_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/temp\"\n","GDRIVE_DATASET_OUTPUT_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/output\"\n","\n","GDRIVE_DATASET_NAME = \"bitcoin_blockchain_data_1h\"\n","GDRIVE_DATASET_NAME_EXT = \"/\" + GDRIVE_DATASET_NAME + \".parquet\"\n","\n","GDRIVE_DATASET = GDRIVE_DATASET_RAW_DIR + GDRIVE_DATASET_NAME_EXT"],"metadata":{"id":"ioke-KeTcJtu","executionInfo":{"status":"ok","timestamp":1691560332071,"user_tz":-120,"elapsed":9,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79882,"status":"ok","timestamp":1691560411945,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"},"user_tz":-120},"id":"au2-MqC-SvrB","outputId":"21ba7faf-554d-439d-a1d3-045077181ab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Point Colaboratory to our Google Drive\n","\n","from google.colab import drive\n","\n","drive.mount(GDRIVE_DIR, force_remount=True)"]},{"cell_type":"markdown","source":["# **Model preparation** ❗\n"],"metadata":{"id":"Kx-BUiTt1HJ9"}},{"cell_type":"code","source":["# load dataset into pyspark dataframe objects\n","df = spark.read.load(GDRIVE_DATASET,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )"],"metadata":{"id":"B5y1hZlB4IBF","executionInfo":{"status":"ok","timestamp":1691560421781,"user_tz":-120,"elapsed":9838,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["After loading the dataset we can store the index of an 80/20 split in a variable called “valid_index” and partition the data accordingly:"],"metadata":{"id":"arEQ1qLV6cn1"}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n","window = Window.orderBy(\"timestamp\")\n","\n","df = df.withColumn(\"index\", row_number().over(window) - 1)\n","\n","# Calculates the total number of rows in the DataFrame\n","total_rows = df.count()\n","\n","# Calculates the index corresponding to 80% of the rows (train)\\\n","train_rows = int(total_rows * 0.8)\n","# Calculates the index corresponding to 15% of the rows (test)\n","valid_rows = int(total_rows * 0.15)\n","# Calculates the index corresponding to 5% of the rows (valid)\n","test_rows = total_rows - train_rows - valid_rows\n","\n","# Dividi il dataset in base all'indice\n","train_df = df.filter(df.index < train_rows)\n","valid_df = df.filter((df.index >= train_rows) & (df.index < train_rows + valid_rows))\n","test_df = df.filter(df.index >= train_rows + valid_rows)\n","\n","if SLOW_OPERATION:\n","  print(\"The shape of the train dataset is {:d} rows by {:d} columns\".format(train_df.count(), len(train_df.columns)))\n","  train_df.show(3)\n","  print(\"The shape of the valid dataset is {:d} rows by {:d} columns\".format(valid_df.count(), len(valid_df.columns)))\n","  valid_df.show(3)\n","  print(\"The shape of the test dataset is {:d} rows by {:d} columns\".format(test_df.count(), len(test_df.columns)))\n","  test_df.show(3)"],"metadata":{"id":"Wpq8hCKTbOX8","executionInfo":{"status":"ok","timestamp":1691560424099,"user_tz":-120,"elapsed":2321,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing train / test set ❗"],"metadata":{"id":"ikvt9XOqY00r"}},{"cell_type":"markdown","source":["In this section we are going to display the division of the dataset we just made based on the market price (value on which we will then go on to make our forecast)"],"metadata":{"id":"w18d_MDBZMux"}},{"cell_type":"code","source":["def show_train_test(train, valid, test):\n","  trace1 = go.Scatter(\n","      x = train['timestamp'],\n","      y = train['market-price'].astype(float),\n","      mode = 'lines',\n","      name = 'Train'\n","  )\n","\n","  trace2 = go.Scatter(\n","      x = valid['timestamp'],\n","      y = valid['market-price'].astype(float),\n","      mode = 'lines',\n","      name = 'Validation'\n","  )\n","\n","  trace3 = go.Scatter(\n","      x = test['timestamp'],\n","      y = test['market-price'].astype(float),\n","      mode = 'lines',\n","      name = 'Test'\n","  )\n","\n","  layout = dict(\n","      title='Train, test and validation set with Rangeslider',\n","      xaxis=dict(\n","          rangeselector=dict(\n","              buttons=list([\n","                  #change the count to desired amount of months.\n","                  dict(count=1,\n","                      label='1m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=6,\n","                      label='6m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=12,\n","                      label='1y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=36,\n","                      label='3y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(step='all')\n","              ])\n","          ),\n","          rangeslider=dict(\n","              visible = True\n","          ),\n","          type='date'\n","      )\n","  )\n","\n","  data = [trace1,trace2,trace3]\n","  fig = dict(data=data, layout=layout)\n","  iplot(fig, filename = \"Train, test and validation set with Rangeslider\")"],"metadata":{"id":"C0lN-g97DsFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_train_test(train_df.toPandas(), valid_df.toPandas(), test_df.toPandas())"],"metadata":{"id":"wWBo9-uBEoKT","colab":{"base_uri":"https://localhost:8080/","height":750,"output_embedded_package_id":"1zF-IJ_d6g0tH88Q_BhC3zjgfNMCQuhIE"},"executionInfo":{"status":"ok","timestamp":1691483727935,"user_tz":-120,"elapsed":14088,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}},"outputId":"be245b58-29ee-4a26-9775-8f9e79c53f21"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["# Output"],"metadata":{"id":"HHoQJdCYGygi"}},{"cell_type":"markdown","metadata":{"id":"4-owrgEPHQ7K"},"source":["In this last section we are going to save the final training and test datasets."]},{"cell_type":"code","source":["def output(dataset, type):\n","  from pyspark.sql.functions import date_format, to_timestamp, col\n","\n","  dataset.write.parquet(GDRIVE_DATASET_TEMP_DIR, mode='overwrite')\n","\n","  import os\n","  import glob\n","  import time\n","\n","  while True:\n","      parquet_files = glob.glob(os.path.join(GDRIVE_DATASET_TEMP_DIR, \"part*.parquet\"))\n","      if len(parquet_files) > 0:\n","          # .parquet file found!\n","          file_path = parquet_files[0]\n","          break\n","      else:\n","          print(\".parquet file not found. I'll try again after 1 second...\")\n","          time.sleep(1)\n","\n","  print(\".parquet file found:\", file_path)\n","\n","  new_file_path = GDRIVE_DATASET_OUTPUT_DIR + \"/\" + GDRIVE_DATASET_NAME + \"_\" + type + \".parquet\"\n","\n","  import shutil\n","\n","  # rename and move the file\n","  shutil.move(file_path, new_file_path)\n","\n","  print(\"File renamed and moved successfully!\")"],"metadata":{"id":"2hVcPL2ceRt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2654,"status":"ok","timestamp":1691483730579,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"},"user_tz":-120},"id":"G8mHxapmSAC7","outputId":"363c04f6-39fb-40ce-e996-ccaffcd8baa1"},"outputs":[{"output_type":"stream","name":"stdout","text":[".parquet file found: /content/drive/MyDrive/BDC/project/datasets/temp/part-00000-4971fc65-8415-4702-bf6e-7f950fa31e2c-c000.snappy.parquet\n","File renamed and moved successfully!\n",".parquet file found: /content/drive/MyDrive/BDC/project/datasets/temp/part-00000-380d666d-743a-44b4-92a6-ded09ea5e5bb-c000.snappy.parquet\n","File renamed and moved successfully!\n",".parquet file found: /content/drive/MyDrive/BDC/project/datasets/temp/part-00000-f85f07c9-a8dd-4a6f-8551-333963ea9521-c000.snappy.parquet\n","File renamed and moved successfully!\n"]}],"source":["output(train_df, \"train\")\n","output(valid_df, \"valid\")\n","output(test_df, \"test\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1lg_X0P096b94i0oHy3o3UI_tycwaqlAg","timestamp":1686125895263}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"}}},"nbformat":4,"nbformat_minor":0}