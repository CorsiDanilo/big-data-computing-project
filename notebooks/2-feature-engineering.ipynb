{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwl6Bf0Lv9D"
      },
      "source": [
        "# **Bitcoin price prediction - Feature Engineering**\n",
        "## Big Data Computing final project - A.Y. 2022 - 2023\n",
        "Prof. Gabriele Tolomei\n",
        "\n",
        "MSc in Computer Science\n",
        "\n",
        "La Sapienza, University of Rome\n",
        "\n",
        "### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Description: adding useful features regardings the price of Bitcoin, visualizing data and performing feature selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIbEK2pDLv9x"
      },
      "source": [
        "# Global constants, dependencies, libraries and tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUpNH3eIQPR4"
      },
      "outputs": [],
      "source": [
        "# Main constants\n",
        "LOCAL_RUNNING = True\n",
        "SLOW_OPERATIONS = True # Decide whether or not to use operations that might slow down notebook execution\n",
        "ROOT_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKk35k0MLv9y",
        "outputId": "511694c8-1ade-4e88-e63d-89c6a7d5bc66"
      },
      "outputs": [],
      "source": [
        "if not LOCAL_RUNNING:\n",
        "    # Point Colaboratory to Google Drive\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Define GDrive paths\n",
        "    drive.mount(ROOT_DIR, force_remount=True)\n",
        "\n",
        "    # Install Spark and related dependencies\n",
        "    !pip install pyspark\n",
        "    !pip install -U -q PyDrive -qq\n",
        "    !apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVR_k1G7gcyI"
      },
      "source": [
        "## Import my utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlMz0SgfgcyI"
      },
      "outputs": [],
      "source": [
        "# Set main dir\n",
        "MAIN_DIR = ROOT_DIR + \"\" if LOCAL_RUNNING else ROOT_DIR + \"/MyDrive/BDC/project\"\n",
        "\n",
        "# Utilities dir\n",
        "UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n",
        "\n",
        "# Import my utilities\n",
        "import sys\n",
        "sys.path.append(UTILITIES_DIR)\n",
        "\n",
        "from imports import *\n",
        "import feature_engineering_utilities\n",
        "\n",
        "importlib.reload(feature_engineering_utilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ8YvlaZLv9x"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# --- DATASET --- #\n",
        "###################\n",
        "\n",
        "# Datasets dirs\n",
        "DATASET_RAW_DIR = MAIN_DIR + \"/datasets/raw\"\n",
        "DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n",
        "DATASET_TEMP_DIR = MAIN_DIR + \"/datasets/temp\"\n",
        "\n",
        "# Datasets names\n",
        "DATASET_NAME = \"bitcoin_blockchain_data_15min\"\n",
        "\n",
        "# Datasets paths\n",
        "DATASET_RAW = DATASET_RAW_DIR + \"/\" + DATASET_NAME + \".parquet\"\n",
        "\n",
        "####################\n",
        "# --- FEATURES --- #\n",
        "####################\n",
        "\n",
        "# Features dir\n",
        "FEATURES_DIR = MAIN_DIR + \"/features\"\n",
        "\n",
        "# Features names\n",
        "FEATURES_CORRELATION_LABEL = \"features_correlation\"\n",
        "BASE_FEATURES_LABEL = \"base_features\"\n",
        "BASE_AND_MOST_CORR_FEATURES_LABEL = \"base_and_most_corr_features\"\n",
        "BASE_AND_LEAST_CORR_FEATURES_LABEL = \"base_and_least_corr_features\"\n",
        "\n",
        "# Features paths\n",
        "FEATURES_CORRELATION = FEATURES_DIR + \"/\" + FEATURES_CORRELATION_LABEL + \".json\"\n",
        "BASE_FEATURES = FEATURES_DIR + \"/\" + BASE_FEATURES_LABEL + \".json\"\n",
        "BASE_AND_MOST_CORR_FEATURES = FEATURES_DIR + \"/\" + BASE_AND_MOST_CORR_FEATURES_LABEL + \".json\"\n",
        "BASE_AND_LEAST_CORR_FEATURES = FEATURES_DIR + \"/\" + BASE_AND_LEAST_CORR_FEATURES_LABEL + \".json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvKa3zyWLv9y"
      },
      "outputs": [],
      "source": [
        "# Suppression of warnings for better reading\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "if LOCAL_RUNNING: pio.renderers.default='notebook' # To correctly export the notebook in html format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUI9DnQVLv9z"
      },
      "source": [
        "# Create the pyspark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLwcEWvYLv9z"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '12G').\\\n",
        "                set('spark.driver.memory', '12G').\\\n",
        "                set('spark.driver.maxResultSize', '109G').\\\n",
        "                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n",
        "                setAppName(\"BitcoinPricePrediction\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7IjRUGdLv9z"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugtTXvpBLv9z"
      },
      "outputs": [],
      "source": [
        "# Load datasets into pyspark dataset objects\n",
        "df = spark.read.load(DATASET_RAW,\n",
        "                         format=\"parquet\",\n",
        "                         sep=\",\",\n",
        "                         inferSchema=\"true\",\n",
        "                         header=\"true\"\n",
        "                    ) \\\n",
        "                     .withColumn(\"id\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))-1) # Adding \"id\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s76xwenILv9z"
      },
      "outputs": [],
      "source": [
        "feature_engineering_utilities.dataset_info(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1v6rsUjLv9z"
      },
      "source": [
        "# Adding useful features\n",
        "Here I am going to add some features that could help us predict the Bitcoin price:\n",
        "\n",
        "*   **next-market-price:** represents the price of Bitcoin for the next 15 minutes (this will be the target variable on which to make predictions)\n",
        "*   **sma-x-days:** indicators that calculate the average price over a specified number of days. They are commonly used by traders to identify trends and potential buy or sell signals.\n",
        "*   **avg-ohlc-price:** indicators that calculate the open, high, low and close average price over 7 days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WML0CKelLv9z"
      },
      "outputs": [],
      "source": [
        "# Creation of a new dataset for the new features\n",
        "new_features_df = df.select(\"timestamp\", \"id\", \"market-price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j4GFmpHLv9z"
      },
      "outputs": [],
      "source": [
        "# Adding 'next-market-price' column\n",
        "new_features_df = new_features_df.withColumn(\"next-market-price\", F.lag(\"market-price\", offset=-1) \\\n",
        "        .over(Window.orderBy(\"id\"))) \\\n",
        "        .dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6MPud1wLv90"
      },
      "outputs": [],
      "source": [
        "def simple_moving_average(dataset, period, days, col=\"market-price\", orderby=\"id\"):\n",
        "    dataset = dataset.withColumn(f\"sma-{days}-days\", F.avg(col) \\\n",
        "          .over(Window.orderBy(orderby) \\\n",
        "          .rowsBetween(-period,0)))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OBOZbvILv90"
      },
      "outputs": [],
      "source": [
        "# Moving averages days (5/7/10/20/50/100)\n",
        "MA5 = 60 * 24 * 5\n",
        "MA7 = 60 * 24 * 7\n",
        "MA10 = 60 * 24 * 10\n",
        "MA20 = 60 * 24 * 20\n",
        "MA50 = 60 * 24 * 50\n",
        "MA100 = 60 * 24 * 100\n",
        "moving_averages = [MA5, MA7, MA10, MA20, MA50, MA100]\n",
        "days_list = [5, 7, 10, 20, 50, 100]\n",
        "\n",
        "# Computing SMAs\n",
        "for i, moving_avg in enumerate(moving_averages):\n",
        "    new_features_df = simple_moving_average(new_features_df, moving_avg, days_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_info(new_features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL_5JLHyLv90"
      },
      "outputs": [],
      "source": [
        "# Drop \"market-price\" column\n",
        "new_features_df = new_features_df.drop(\"market-price\")\n",
        "\n",
        "# Merge original dataset with the one with the new features\n",
        "merged_df = df.join(new_features_df, on=['timestamp','id'], how='inner')\n",
        "\n",
        "# Persist the dataframe\n",
        "merged_df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_info(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q74GqOn_Lv90"
      },
      "outputs": [],
      "source": [
        "# Rearranges columns\n",
        "new_columns = [\"timestamp\", \"id\"] + [col for col in merged_df.columns if col not in [\"timestamp\", \"id\", \"next-market-price\"]] + [\"next-market-price\"]\n",
        "merged_df = merged_df.select(*new_columns)\n",
        "\n",
        "# Set the \"timestamp\" column as the index of the Pandas dataset\n",
        "merged_df.toPandas().set_index(\"timestamp\", inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxb4eZExLv91"
      },
      "source": [
        "# Splitting dataset\n",
        "Here I am going to split the dataset into two sets:\n",
        "* **Train / validation set:** will be used to train the models and validate the performances\n",
        "* **Test set:** will be used to perform price prediction on never-before-seen data (the last 3 months of the original dataset will be used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5TAxfkk-v8c"
      },
      "outputs": [],
      "source": [
        "# Retrieve the last timestamp value\n",
        "last_value = merged_df.agg(last(\"timestamp\")).collect()[0][0]\n",
        "\n",
        "# Subtract three month from the last timestamp value\n",
        "split_date = last_value - relativedelta(months=3)\n",
        "\n",
        "# Split the dataset based on the desired date\n",
        "train_valid_df = merged_df[merged_df['timestamp'] <= split_date]\n",
        "test_df = merged_df[merged_df['timestamp'] > split_date]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ_OPmiwLv91"
      },
      "outputs": [],
      "source": [
        "if SLOW_OPERATIONS:\n",
        "    feature_engineering_utilities.dataset_visualization(train_valid_df.toPandas(), test_df.toPandas(), \"Train / Validation and Test sets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO07pajqLv91"
      },
      "source": [
        "# Saving datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0s5qokVLv91"
      },
      "outputs": [],
      "source": [
        "def output(dataset, dataset_type):\n",
        "  dataset.write.parquet(DATASET_TEMP_DIR, mode='overwrite')\n",
        "\n",
        "  while True:\n",
        "      parquet_files = glob.glob(os.path.join(DATASET_TEMP_DIR, \"part*.parquet\"))\n",
        "      if len(parquet_files) > 0:\n",
        "          # .parquet file found!\n",
        "          file_path = parquet_files[0]\n",
        "          break\n",
        "      else:\n",
        "          print(\".parquet file not found. I'll try again after 1 second...\")\n",
        "          time.sleep(1)\n",
        "\n",
        "  print(\".parquet file found:\", file_path)\n",
        "\n",
        "  new_file_path = DATASET_OUTPUT_DIR + \"/\" + DATASET_NAME + \"_\" + dataset_type + \".parquet\"\n",
        "\n",
        "  # Rename and move the file\n",
        "  shutil.move(file_path, new_file_path)\n",
        "\n",
        "  print(\"File renamed and moved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss2HR4JpLv91"
      },
      "outputs": [],
      "source": [
        "# Save the train / validation set\n",
        "output(train_valid_df, \"train_valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtPmzkfwLv96"
      },
      "outputs": [],
      "source": [
        "# Save the test set\n",
        "output(test_df, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbFTB-2aLv96"
      },
      "source": [
        "# Data visualization\n",
        "\n",
        "Here I am going to display the features taken under consideration according to their categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui5CtB0sLv96"
      },
      "outputs": [],
      "source": [
        "# Convert the PySpark dataset into Pandas\n",
        "merged_df_pd = merged_df.toPandas()\n",
        "merged_df_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcnp_SJNLv96"
      },
      "outputs": [],
      "source": [
        "# List of features according to categories\n",
        "ohlcv_statistics = {'Opening price (USD)':'opening-price', 'Highest price (USD)':'highest-price', 'Lowest price (USD)':'lowest-price', 'Closing price (USD)':'closing-price', 'Trade volume (BTC)':'trade-volume-btc'}\n",
        "currency_statistics = {'Market price (USD)':'market-price', 'Market cap (USD)':'market-cap', 'N. total bitcoins':'total-bitcoins', 'Trade volume (USD)':'trade-volume-usd'}\n",
        "block_details = {'Blocks size (MB)':'blocks-size', 'Avg. block size (MB)':'avg-block-size', 'N. total transactions':'n-transactions-total', 'N. transactions per block':'n-transactions-per-block'}\n",
        "mining_information = {'Hash rate (TH/s)':'hash-rate', 'Difficulty (T)':'difficulty', 'Miners revenue (USD)':'miners-revenue', 'Transaction fees (USD)':'transaction-fees-usd'}\n",
        "network_activity = {\"N. unique addresses\":'n-unique-addresses', 'N. transactions':'n-transactions', 'Estimated transaction volume (USD)':'estimated-transaction-volume-usd'}\n",
        "simple_moving_avg = {\"Simple moving avg. (5d)\":\"sma-5-days\", \"Simple moving avg. (7d)\":\"sma-7-days\", \"Simple moving avg. (10d)\":\"sma-10-days\", \"Simple moving avg. (20d)\":\"sma-20-days\", \"Simple moving avg. (50d)\":\"sma-50-days\", \"Simple moving avg. (100d)\":\"sma-100-days\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OHLC Statistics\n",
        "ohlc_statistics = list(ohlcv_statistics.items())[:4]\n",
        "print(ohlc_statistics)\n",
        "\n",
        "# Volume Statistics\n",
        "volume_statistics = list(ohlcv_statistics.items())[4:]\n",
        "print(volume_statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OHLCV Statistics\n",
        "if SLOW_OPERATIONS:\n",
        "  feature_engineering_utilities.ohlcv_visualization(merged_df_pd, ohlc_statistics, \"OHLC Statistics (USD)\")\n",
        "  feature_engineering_utilities.features_visualization(merged_df_pd, volume_statistics[0][0], volume_statistics[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The OHLCV stastistics chart is a type of bar chart that shows the open, high, low, close and volume values for each period. They are useful because they show the five main points of a period, with the closing price being considered the most important by many traders. Due to an Algo Bug on Binance's U.S. Exchange I have a [strange dump on 21 October 2021](https://www.bloomberg.com/news/articles/2021-10-21/bitcoin-appears-to-crash-87-on-binance-in-apparent-mistake#xj4y7vzkg) regarding the lower price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeBnhpcSLv98"
      },
      "outputs": [],
      "source": [
        "# Currency Statistics\n",
        "if SLOW_OPERATIONS:\n",
        "  for key, value in currency_statistics.items():\n",
        "    feature_engineering_utilities.features_visualization(merged_df_pd, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4s2g3s4gcyT"
      },
      "source": [
        "Concerning currency statistics, we can see that in the period from late 2020 to mid-2022 there has been a rise in the price of Bitcoin, while the amount of Bitcoins issued is slowly peaking (i.e. 21 million), it is thought that the last BTC will be mined in 2140."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9PPFCl9Lv98"
      },
      "outputs": [],
      "source": [
        "# Block Details\n",
        "if SLOW_OPERATIONS:\n",
        "  for key, value in block_details.items():\n",
        "    feature_engineering_utilities.features_visualization(merged_df_pd, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HApd6t9TgcyT"
      },
      "source": [
        "Concerning block details, we can see that over time the number of transactions has increased exponentially, along with the size of the blocks. The peak around the end of January 2023 is due to the creation of the Ordinals protocol that allows the creation of 'digital artefacts' on the Bitcoin network (These can include JPEG images, PDFs and audio and video files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMJYU9RGLv98"
      },
      "outputs": [],
      "source": [
        "# Mining Information\n",
        "if SLOW_OPERATIONS:\n",
        "  for key, value in mining_information.items():\n",
        "    feature_engineering_utilities.features_visualization(merged_df_pd, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWh82QHgcyT"
      },
      "source": [
        "Regarding mining information, we can see how the difficulty of the network along with the hash rate has also increased exponentially, the greater the hashing (computing) power in the network, the greater its security and resistance to attacks. While the miners revenue more or less follows the price trend of Bitcoin itself (this is also thanks to the transaction fees that are distributed to the miners). The two biggest spikes in transaction fees are due to a combination of ASIC shortages, huge price increases of BTC outpacing difficulty and the sudden hashrate drop, resulting in slower block times, backlog of transactions and extra fees per block ([20 - 21 April 2021](https://www.coindesk.com/markets/2021/04/21/bitcoin-transactions-are-more-expensive-than-ever/)) and the increase in demand for block space attributed to the increase in Ordinals ([8 May 2023](https://www.coindesk.com/tech/2023/05/08/ordinals-upend-bitcoin-mining-pushing-transaction-fees-above-mining-reward-for-first-time-in-years/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb-RqdrfLv98"
      },
      "outputs": [],
      "source": [
        "# Network Activity\n",
        "if SLOW_OPERATIONS:\n",
        "  for key, value in network_activity.items():\n",
        "    feature_engineering_utilities.features_visualization(merged_df_pd, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxPPZ8RZgcyU"
      },
      "source": [
        "Regarding Network Activity, we can see how this also increases as time goes by, a symbol that the Bitcoin protocol is becoming more and more popular and people are willing to pay to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVsE5Ed_aZap"
      },
      "outputs": [],
      "source": [
        "# Extract the short term SMA\n",
        "short_term_sma = list(simple_moving_avg.items())[:3]\n",
        "print(short_term_sma)\n",
        "\n",
        "# Extract the long term SMA\n",
        "long_term_sma = list(simple_moving_avg.items())[3:]\n",
        "print(long_term_sma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O45Snn4nLv99"
      },
      "outputs": [],
      "source": [
        "# Additional Features: Short term SMA\n",
        "if SLOW_OPERATIONS:\n",
        "  feature_engineering_utilities.sma_visualization(merged_df_pd, short_term_sma, \"Short term SMA (usd)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFPGKfPXaXPb"
      },
      "outputs": [],
      "source": [
        "# Additional Features: Long term SMA\n",
        "if SLOW_OPERATIONS:\n",
        "  feature_engineering_utilities.sma_visualization(merged_df_pd,long_term_sma, \"Long term SMA (usd)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY3pzsf1gcyU"
      },
      "source": [
        "Taking into consideration the Simple Moving Averages (which instead give us a more medium to long term view of the price) we can see that the main price variations occur precisely in the latter, this tells us that although Bitcoin has high price volatility this often occurs days or even months later, except in some cases where unpredictable pumps or dumps can occur due to sudden news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Fc7fl2Lv99"
      },
      "source": [
        "#  Feature selection\n",
        "Here I am going to select blockchain features plus features added at the beginning to be eventually added to the main ones, i.e. those dedicated to currency based on their correlation and importance with respect to the market price using the Pearson method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of features\n",
        "all_features = [ohlcv_statistics, currency_statistics, block_details, mining_information, network_activity, simple_moving_avg]\n",
        "\n",
        "# Count occurrences\n",
        "count = 0\n",
        "for features in all_features:\n",
        "    for key, value in features.items():\n",
        "        count += 1\n",
        "\n",
        "print(f\"Number of features: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of columns\n",
        "num_columns = len(merged_df.columns)\n",
        "print(f\"Number of columns in the dataset: {num_columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoG9Hf-vgcyV"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset to feature selection\n",
        "new_columns = [\"next-market-price\"] + [\n",
        "    col for col in merged_df.columns if col not in \n",
        "    [\n",
        "    'timestamp',\n",
        "    'id',\n",
        "    'next-market-price',\n",
        "    'opening-price',\n",
        "    'highest-price',\n",
        "    'lowest-price',\n",
        "    'closing-price',\n",
        "    'trade-volume-btc',\n",
        "    'market-price',\n",
        "    'market-cap',\n",
        "    'total-bitcoins',\n",
        "    'trade-volume-usd'\n",
        " ]]\n",
        "merged_df_only_blockchain_data = merged_df.select(*new_columns)\n",
        "merged_df_only_blockchain_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uF8wkgFgcyV"
      },
      "outputs": [],
      "source": [
        "# Assemble the data to apply PySpark methods\n",
        "assembler = VectorAssembler(inputCols=merged_df_only_blockchain_data.columns, outputCol='features')\n",
        "assembled_data = assembler.transform(merged_df_only_blockchain_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_nWs3HPgcyV"
      },
      "outputs": [],
      "source": [
        "# Compute the correlation matrix\n",
        "correlation_matrix = Correlation.corr(assembled_data, 'features').head()\n",
        "\n",
        "# Get the highest correlated features\n",
        "correlation_scores = correlation_matrix[0].toArray()\n",
        "feature_names = merged_df_only_blockchain_data.columns\n",
        "feature_correlations = sorted([(feature_names[i], str(correlation_scores[i][0])) for i in range(len(feature_names))], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the results\n",
        "for label, value in feature_correlations:\n",
        "    print(f\"Feature: {label}, Correlation: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoOg-6CsgcyV"
      },
      "source": [
        "Finally, I decided to divide the features into two distinct groups:\n",
        "- **Base features:** contains ohlcv and currency statistics\n",
        "- **Base and additional features:** contains the Base features plus the additional features divided based on their correlation value: \n",
        "    - If >= 0.6, then then they will be considered the **most correlated**\n",
        "    - If < 0.6, then then they will be considered the **least correlated**\n",
        "\n",
        "The strategy for the next notebooks will be as follows:\n",
        "- Test models with base features\n",
        "- See if by adding the additional most and least correlated features to them improves the situation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29kHzxhUgcyV"
      },
      "outputs": [],
      "source": [
        "base_features = list(ohlcv_statistics.values()) + list(currency_statistics.values())\n",
        "most_corr_features = [x[0] for x in feature_correlations[1:] if float(x[1]) >= 0.6]\n",
        "base_and_most_corr_features = base_features + most_corr_features\n",
        "least_corr_features = [x[0] for x in feature_correlations[1:] if float(x[1]) < 0.6]\n",
        "base_and_least_corr_features = base_features + least_corr_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDAHGGxUgcyW"
      },
      "outputs": [],
      "source": [
        "most_corr_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_and_most_corr_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "least_corr_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_and_least_corr_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1dizTOFLv99"
      },
      "source": [
        "# Saving selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save currency and ohlcv features\n",
        "with open(BASE_FEATURES, 'w') as file:\n",
        "    json.dump(base_features, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InZ_DK88Lv9-"
      },
      "outputs": [],
      "source": [
        "# Save currency, ohlcv and blockchain most correlated features\n",
        "with open(BASE_AND_MOST_CORR_FEATURES, 'w') as file:\n",
        "    json.dump(base_and_most_corr_features, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save currency, ohlcv and blockchain least correlated features\n",
        "with open(BASE_AND_LEAST_CORR_FEATURES, 'w') as file:\n",
        "    json.dump(base_and_least_corr_features, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm-om-aegcyW"
      },
      "outputs": [],
      "source": [
        "# Export notebook in html format (remember to save the notebook and change the model name)\n",
        "if LOCAL_RUNNING:\n",
        "    !jupyter nbconvert --to html 2-feature-engineering.ipynb --output 2-feature-engineering --output-dir='./exports'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
