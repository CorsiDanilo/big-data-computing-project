{"cells":[{"cell_type":"markdown","metadata":{"id":"e452Ydj7chNV"},"source":["# Bitcoin price forecasting with PySpark\n","## Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author\n","Corsi Danilo - corsi.1742375@studenti.uniroma1.it\n","\n"]},{"cell_type":"markdown","source":["# Global Constants\n"],"metadata":{"id":"AkQaY6VAf4v_"}},{"cell_type":"code","source":["JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","GDRIVE_DIR = \"/content/drive\"\n","\n","GDRIVE_DATASET_RAW_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/raw\"\n","GDRIVE_DATASET_TEMP_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/temp\"\n","GDRIVE_DATASET_OUTPUT_DIR = GDRIVE_DIR + \"/MyDrive/BDC/project/datasets/output\"\n","\n","GDRIVE_DATASET_NAME = \"bitcoin_blockchain_data_1h\"\n","GDRIVE_DATASET_NAME_TRAIN = GDRIVE_DATASET_NAME + \"_train\"\n","GDRIVE_DATASET_NAME_TEST = GDRIVE_DATASET_NAME + \"_test\"\n","\n","GDRIVE_DATASET_NAME_EXT_TRAIN  = \"/\" + GDRIVE_DATASET_NAME_TRAIN + \".parquet\"\n","GDRIVE_DATASET_NAME_EXT_TEST = \"/\" + GDRIVE_DATASET_NAME_TEST + \".parquet\"\n","\n","GDRIVE_DATASET_TRAIN = GDRIVE_DATASET_OUTPUT_DIR + GDRIVE_DATASET_NAME_EXT_TRAIN\n","GDRIVE_DATASET_TEST = GDRIVE_DATASET_OUTPUT_DIR + GDRIVE_DATASET_NAME_EXT_TEST\n","\n","SLOW_OPERATION = False"],"metadata":{"id":"L7-8QQvKf5CR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  Import Python packages"],"metadata":{"id":"IXTGTgzEqE3x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX7xDYw4SvrB"},"outputs":[],"source":["import requests\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from itertools import cycle\n","\n","import plotly.express as px\n","\n","import plotly.graph_objs as go\n","from plotly.offline import init_notebook_mode, iplot\n","import gc"]},{"cell_type":"markdown","metadata":{"id":"YtmWjQUVSvq2"},"source":["# Spark + Google Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si82CaUYSvrA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691079579481,"user_tz":-120,"elapsed":49947,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}},"outputId":"245ce733-c5ca-49e8-a9ad-dcbb2d1498f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=79bb77ec48798752b65a8491106d6e45caaf591ad04450d4065b7f790874cd96\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n","The following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 39.7 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 120500 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n"]}],"source":["!pip install pyspark\n","# Alternatively, if you want to install a specific version of pyspark:\n","#!pip install pyspark==3.2.1\n","!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = JAVA_HOME\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","\n","from pyspark.sql import functions as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fhi5bmOeSvrB"},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '4G').\\\n","                set('spark.driver.memory', '45G').\\\n","                set('spark.driver.maxResultSize', '10G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPriceForecasting\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"au2-MqC-SvrB","executionInfo":{"status":"ok","timestamp":1691079601912,"user_tz":-120,"elapsed":16688,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df0fc2d1-7d56-41c8-c2ff-864e3d337e65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Point Colaboratory to our Google Drive\n","\n","from google.colab import drive\n","\n","drive.mount(GDRIVE_DIR, force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"xMCpzz2fe9Vi"},"source":["# Random Forest"]},{"cell_type":"markdown","source":["## Model preparation ❗"],"metadata":{"id":"f0yi3Fcth9fi"}},{"cell_type":"code","source":["# load dataset into pyspark dataframe objects\n","train_df = spark.read.load(GDRIVE_DATASET_TRAIN,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )\n","\n","test_df = spark.read.load(GDRIVE_DATASET_TEST,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )"],"metadata":{"id":"yiF49OXz4-i7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After loading the dataset we need to vectorize all of our features into a single column.\n","\n","To build and compare performance of our three feature set sizes:\n","* All features, our baseline\n","* Relevant features\n","* RFE-selected features"],"metadata":{"id":"4Fg6qN6mgctD"}},{"cell_type":"code","source":["# TODO: import feature selected form JSON ❗\n","all_columns = ['market-cap', 'total-bitcoins', 'trade-volume', 'blocks-size', 'avg-block-size', 'n-transactions-total', 'n-transactions-per-block', 'hash-rate', 'difficulty', 'miners-revenue', 'transaction-fees-usd', 'n-unique-addresses', 'n-transactions', 'estimated-transaction-volume-usd']\n","rel_columns = ['market-cap', 'estimated-transaction-volume-usd', 'blocks-size', 'n-unique-addresses']\n","sel_columns = ['total-bitcoins', 'blocks-size', 'avg-block-size', 'n-transactions-per-block', 'miners-revenue', 'n-unique-addresses', 'n-transactions']\n","\n","dep_var = 'market-price'"],"metadata":{"id":"nUSCqkaWgZDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll start by assembling 3 independent VectorAssemblers, 1 for each feature list. To get the 3 Vectorized RDDs, we apply the transform on the data using each:"],"metadata":{"id":"BpI95vSsggL0"}},{"cell_type":"code","source":["def select_features(dataset, features):\n","  vectorAssembler = VectorAssembler(\n","    inputCols = features,\n","    outputCol = 'features')\n","\n","  dataset = vectorAssembler.transform(dataset)\n","  dataset = dataset.select(['timestamp','index', 'features', dep_var])\n","  return dataset"],"metadata":{"id":"d85eehUnkeht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","\n","#All columns featurized\n","all_train_df = select_features(train_df, all_columns)\n","all_test_df = select_features(test_df, all_columns)\n","\n","# #Relevant columns featurized\n","# rel_train_df = select_features(train_df, rel_columns)\n","# rel_test_df = select_features(test_df, rel_columns)\n","\n","# #RFE-selected columns featurized\n","# sel_train_df = select_features(train_df, sel_columns)\n","# sel_test_df = select_features(test_df, sel_columns)"],"metadata":{"id":"uj108A-gghMq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we call the show method on each of the resulting RDDs yields the following vectorized inputs (X) and targets (y)."],"metadata":{"id":"_xnSqT2ugnYx"}},{"cell_type":"code","source":["def print_df_info(dataset, type):\n","  print(\"The shape of the {:s} dataset is {:d} rows by {:d} columns\".format(type, dataset.count(), len(dataset.columns)))\n","  dataset.show(3)"],"metadata":{"id":"M59XsdpCioMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if SLOW_OPERATION:\n","  print_df_info(all_train_df, \"all columns train\")\n","  print_df_info(all_test_df, \"all columns test\")\n","\n","  # print_df_info(rel_train_df, \"relevant columns train\")\n","  # print_df_info(rel_test_df, \"relevant columns test\")\n","\n","  # print_df_info(sel_train_df, \"selected columns train\")\n","  # print_df_info(sel_test_df, \"selected columns test\")"],"metadata":{"id":"kRNytZU9gjoJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predictions ❗"],"metadata":{"id":"YxnTVkVkwxSK"}},{"cell_type":"code","source":["def random_forest(train, test):\n","  from pyspark.ml.regression import RandomForestRegressor\n","  from pyspark.ml.feature import StringIndexer, VectorAssembler\n","\n","  # Crea un oggetto RandomForestRegressor: Puoi impostare i parametri desiderati per il modello Random Forest come numero di alberi (numTrees), profondità massima degli alberi (maxDepth), numero massimo di bin per il partizionamento delle features (maxBins), ecc.\n","  rf = RandomForestRegressor(\n","      featuresCol=\"features\",  # Colonna vettoriale delle features\n","      labelCol=\"market-price\",  # Colonna delle etichette di output\n","  )\n","\n","  # Addestra il modello: Utilizza il metodo fit() per addestrare il modello sulla tua dataset di addestramento.\n","  model = rf.fit(train)\n","\n","  # Effettua le previsioni: Utilizza il modello addestrato per fare previsioni sul tuo dataset di test o su nuovi dati.\n","  predictions = model.transform(test)\n","\n","  return predictions, model"],"metadata":{"id":"S9_owIn8rEw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_predictions_df, all_model = random_forest(all_train_df, all_test_df)\n","# rel_predictions_df, rel_model = random_forest(rel_train_df, rel_test_df)\n","# sel_predictions_df, sel_model = random_forest(sel_train_df, sel_test_df)\n","\n","if SLOW_OPERATION:\n","  print_df_info(all_predictions_df, \"all columns prediction\")\n","  # print_df_info(rel_predictions_df, \"relevant columns prediction\")\n","  # print_df_info(sel_predictions_df, \"selected columns prediction\")"],"metadata":{"id":"LLIcmyCDoYtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_results(train, test, pred):\n","  trace1 = go.Scatter(\n","      x = train['timestamp'],\n","      y = train['market-price'].astype(float),\n","      mode = 'lines',\n","      name = 'Train'\n","  )\n","\n","  trace2 = go.Scatter(\n","      x = test['timestamp'],\n","      y = test['market-price'].astype(float),\n","      mode = 'lines',\n","      name = 'Test'\n","  )\n","\n","  trace3 = go.Scatter(\n","      x = pred['timestamp'],\n","      y = pred['prediction'].astype(float),\n","      mode = 'lines',\n","      name = 'Prediction'\n","  )\n","\n","  layout = dict(\n","      title='Train, test and prediction set  with Rangeslider',\n","      xaxis=dict(\n","          rangeselector=dict(\n","              buttons=list([\n","                  #change the count to desired amount of months.\n","                  dict(count=1,\n","                      label='1m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=6,\n","                      label='6m',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=12,\n","                      label='1y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(count=36,\n","                      label='3y',\n","                      step='month',\n","                      stepmode='backward'),\n","                  dict(step='all')\n","              ])\n","          ),\n","          rangeslider=dict(\n","              visible = True\n","          ),\n","          type='date'\n","      )\n","  )\n","\n","  data = [trace1, trace2, trace3]\n","  fig = dict(data=data, layout=layout)\n","  iplot(fig, filename = \"Train, test and prediction set  with Rangeslider\")"],"metadata":{"id":"vxycP9puFjTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_results(train_df.toPandas(), test_df.toPandas(), all_predictions_df.toPandas())"],"metadata":{"id":"pl9FRfnIFk5D","executionInfo":{"status":"ok","timestamp":1691079629598,"user_tz":-120,"elapsed":9493,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}},"colab":{"base_uri":"https://localhost:8080/","height":750,"output_embedded_package_id":"17oYMSG2c7TPklAp6TAi7LO0lHjzjpkRk"},"outputId":"3fe42082-6dc0-4e8e-d82d-f70464a0c66a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Evaluation ❗"],"metadata":{"id":"Vpvbfulhwndf"}},{"cell_type":"code","source":["# Function to evaluate a model\n","def evaluation(pred, n_features):\n","  from pyspark.ml.evaluation import RegressionEvaluator\n","\n","  evaluator = RegressionEvaluator(\n","      predictionCol=\"prediction\",  # Colonna delle previsioni\n","      labelCol=\"market-price\",  # Colonna delle etichette di output\n","  )\n","\n","  mse = evaluator.evaluate(pred, {evaluator.metricName: \"mse\"})\n","  rmse = evaluator.evaluate(pred, {evaluator.metricName: \"rmse\"})\n","  r2 = evaluator.evaluate(pred, {evaluator.metricName: \"r2\"})\n","  mae = evaluator.evaluate(pred, {evaluator.metricName: \"mae\"})\n","\n","  from pyspark.sql.functions import abs, col\n","  from pyspark.sql import functions as F\n","  from pyspark.ml.evaluation import RegressionEvaluator\n","\n","  # Calcola il MAPE\n","  mape = pred.withColumn(\"error\", abs(col(\"market-price\") - col(\"prediction\")) / col(\"market-price\")) \\\n","          .selectExpr(\"avg(error) * 100 as mape\") \\\n","          .collect()[0][\"mape\"]\n","\n","  # adj_r2\n","  n = pred.count()  # Numero di osservazioni\n","  p = n_features # Numero di predittori nel modello\n","  adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n","\n","  print(\"MSE = %s\" % (mse)) # deve essere un valore non negativo, dove un valore di 0 indica una perfetta corrispondenza tra i valori predetti e quelli di riferimento\n","  print(\"RMSE = %s\" % (rmse)) # dovresti considerare il valore di RMSE in relazione al range dei valori target nel tuo problema specifico\n","  print(\"R2 = %s\" % (r2)) # piú é vicino ad 1 meglio é\n","  print(\"MAE = %s\" % (mae)) # può essere utile confrontare il valore di MAE con quello di altri modelli o con il range dei valori target per valutare la sua precisione\n","  print(\"MAPE = %s\" % (mape)) # di solito viene utilizzato come misura relativa per confrontare la precisione di modelli diversi\n","  print(\"ADJ R2 = %s\" % (adj_r2)) # piú é vicino ad 1 meglio é"],"metadata":{"id":"7pL9uDhPvmz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(all_predictions_df, len(all_columns))\n","# evaluation(rel_predictions_df)\n","# evaluation(sel_predictions_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSsgF_D0tgB9","executionInfo":{"status":"ok","timestamp":1690913396310,"user_tz":-120,"elapsed":3511,"user":{"displayName":"Danilo Corsi","userId":"10800270661719155402"}},"outputId":"b8fc7f35-1ef7-4cd2-fa34-207c0402bcf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE = 77058739.66501749\n","RMSE = 8778.310752361042\n","R2 = 0.5875016663735204\n","MAE = 7484.627081452254\n","MAPE = 27.13897711289007\n","ADJ R2 = 0.5872147822865472\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1lg_X0P096b94i0oHy3o3UI_tycwaqlAg","timestamp":1686125902527}],"collapsed_sections":["e452Ydj7chNV"],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"}}},"nbformat":4,"nbformat_minor":0}