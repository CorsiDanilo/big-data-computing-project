{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Bitcoin price prediction - Gradient Boosting Tree Regressor**\n","### Big Data Computing final project - A.Y. 2022 - 2023\n","Prof. Gabriele Tolomei\n","\n","MSc in Computer Science\n","\n","La Sapienza, University of Rome\n","\n","### Author: Corsi Danilo (1742375) - corsi.1742375@studenti.uniroma1.it\n","\n","\n","---\n","\n","\n","Description: executing the chosen model, first with default values, then by choosing the best parameters by performing hyperparameter tuning with cross validation and performance evaluation."]},{"cell_type":"markdown","metadata":{},"source":["# Global constants, dependencies, libraries and tools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Main constants\n","LOCAL_RUNNING = True\n","SLOW_OPERATIONS = True # Decide whether or not to use operations that might slow down notebook execution\n","SPLITTING_METHOD = \"single_split\"\n","MODEL_NAME = \"GradientBoostingTreeRegressor\"\n","ROOT_DIR = \"D:/Documents/Repository/BDC/project\" if LOCAL_RUNNING else \"/content/drive\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING:\n","    # Point Colaboratory to Google Drive\n","    from google.colab import drive\n","\n","    # Define GDrive paths\n","    drive.mount(ROOT_DIR, force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set main dir\n","MAIN_DIR = ROOT_DIR + \"\" if LOCAL_RUNNING else ROOT_DIR + \"/MyDrive/BDC/project\"\n","\n","###################\n","# --- DATASET --- #\n","###################\n","\n","# Datasets dirs\n","DATASET_OUTPUT_DIR = MAIN_DIR + \"/datasets/output\"\n","\n","# Datasets names\n","DATASET_TRAIN_VALID_NAME = \"bitcoin_blockchain_data_15min_train_valid\"\n","\n","# Datasets paths\n","DATASET_TRAIN_VALID  = DATASET_OUTPUT_DIR + \"/\" + DATASET_TRAIN_VALID_NAME + \".parquet\"\n","\n","####################\n","# --- FEATURES --- #\n","####################\n","\n","# Features dir\n","FEATURES_DIR = MAIN_DIR + \"/features\"\n","\n","# Features labels\n","FEATURES_LABEL = \"features\"\n","TARGET_LABEL = \"next-market-price\"\n","\n","# Features names\n","ALL_FEATURES_NAME = \"all_features\"\n","MOST_REL_FEATURES_NAME = \"most_rel_features\"\n","LEAST_REL_FEATURES_NAME = \"least_rel_features\"\n","\n","# Features paths\n","ALL_FEATURES = FEATURES_DIR + \"/\" + ALL_FEATURES_NAME + \".json\"\n","MOST_REL_FEATURES = FEATURES_DIR + \"/\" + MOST_REL_FEATURES_NAME + \".json\"\n","LEAST_REL_FEATURES = FEATURES_DIR + \"/\" + LEAST_REL_FEATURES_NAME + \".json\"\n","\n","##################\n","# --- MODELS --- #\n","##################\n","\n","# Model dir\n","MODELS_DIR = MAIN_DIR + \"/models\"\n","\n","# Model path\n","MODEL = MODELS_DIR + \"/\" + MODEL_NAME\n","\n","#####################\n","# --- UTILITIES --- #\n","#####################\n","\n","# Utilities dir\n","UTILITIES_DIR = MAIN_DIR + \"/utilities\"\n","\n","###################\n","# --- RESULTS --- #\n","###################\n","\n","# Results dir\n","RESULTS_DIR = MAIN_DIR + \"/results/\" + SPLITTING_METHOD\n","\n","# Results path\n","ALL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_all.csv\"\n","REL_MODEL_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_rel.csv\"\n","\n","MODEL_ACCURACY_RESULTS  = RESULTS_DIR + \"/\" + MODEL_NAME + \"_accuracy.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Suppression of warnings for better reading\n","import warnings\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not LOCAL_RUNNING:\n","    # Install Spark and related dependencies\n","    !pip install pyspark\n","    !pip install -U -q PyDrive -qq\n","    !apt install openjdk-8-jdk-headless -qq"]},{"cell_type":"markdown","metadata":{},"source":["# Import files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import my files\n","import sys\n","sys.path.append(UTILITIES_DIR)\n","\n","from imports import *\n","import utilities, parameters\n","\n","importlib.reload(utilities)\n","importlib.reload(parameters)"]},{"cell_type":"markdown","metadata":{},"source":["# Create the pyspark session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the session\n","conf = SparkConf().\\\n","                set('spark.ui.port', \"4050\").\\\n","                set('spark.executor.memory', '12G').\\\n","                set('spark.driver.memory', '12G').\\\n","                set('spark.driver.maxResultSize', '109G').\\\n","                set(\"spark.kryoserializer.buffer.max\", \"1G\").\\\n","                setAppName(\"BitcoinPricePrediction\").\\\n","                setMaster(\"local[*]\")\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load train / validation set into pyspark dataset objects\n","df = spark.read.load(DATASET_TRAIN_VALID,\n","                         format=\"parquet\",\n","                         sep=\",\",\n","                         inferSchema=\"true\",\n","                         header=\"true\"\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dataset_info(dataset):\n","  # Print dataset\n","  dataset.show(3)\n","\n","  # Get the number of rows\n","  num_rows = dataset.count()\n","\n","  # Get the number of columns\n","  num_columns = len(dataset.columns)\n","\n","  # Print the shape of the dataset\n","  print(\"Shape:\", (num_rows, num_columns))\n","\n","  # Print the schema of the dataset\n","  dataset.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(df)"]},{"cell_type":"markdown","metadata":{},"source":["Instead of considering the entire dataset, I will only consider the last two years from today's date in order to train / validate the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve the last timestamp value\n","last_value = df.agg(last(\"timestamp\")).collect()[0][0]\n","\n","# Consider only the last two years as of today\n","split_date = last_value - relativedelta(years=2)\n","\n","# Split the dataset based on the desired date and drop the \"id\" column\n","df = df[df['timestamp'] > split_date].drop(col(\"id\"))\n","\n","# Recompute id column\n","df = df.withColumn(\"id\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))-1)\n","# Rearranges columns\n","new_columns = [\"timestamp\", \"id\"] + [col for col in df.columns if col not in [\"timestamp\", \"id\", \"next-market-price\"]] + [\"next-market-price\"]\n","df = df.select(*new_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if SLOW_OPERATIONS:\n","  dataset_info(df)"]},{"cell_type":"markdown","metadata":{},"source":["# Loading features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading all the features\n","with open(ALL_FEATURES, \"r\") as f:\n","    ALL_FEATURES = json.load(f)\n","print(ALL_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the most relevant features\n","with open(MOST_REL_FEATURES, \"r\") as f:\n","    MOST_REL_FEATURES = json.load(f)\n","print(MOST_REL_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading least relevant features\n","with open(LEAST_REL_FEATURES, \"r\") as f:\n","    LEAST_REL_FEATURES = json.load(f)\n","print(LEAST_REL_FEATURES)"]},{"cell_type":"markdown","metadata":{},"source":["# Model train / validation\n","In order to train and validate the model I'll try several approaches:\n","- **Default without normalization:** Make predictions using the chosen base model\n","- **Default with normalization:** Like the previous one but features are normalized\n","\n","At this point, the features that gave on average the most satisfactory results (for each model) are chosen and proceeded with:\n","\n","- **Hyperparameter tuning:** Researching the best parameters to use\n","- **Cross Validation:** Validate the performance of the model with the chosen parameters\n","\n","If the final results are satisfactory, the model will be trained on the whole train / validation set and saved to later make predictions on the test set.\n","\n","---\n","\n","For each approach the train / validation set will be split according to the chosen splitting method (in order to figure out which one works best for our problem):\n","\n","- **Block time series splits:** Involves dividing the time series into blocks of equal length, and then using each block as a separate fold for cross-validation.\n","\n","    ![block-splits.png](https://drive.google.com/uc?id=1SPT133HO1VdWYZZv6GeknFY3xX3T2tvL)\n","\n","- **Walk forward time series splits:** Involves using a sliding window approach to create the training and validation sets for each fold. The model is trained on a fixed window of historical data, and then validated on the next observation in the time series. This process is repeated for each subsequent observation, with the window sliding forward one step at a time. \n","\n","    ![walk-forward-splits.png](https://drive.google.com/uc?id=1SNdq-kjbv4MXtdBj3EOJ2dmQpbbPStJi)\n","\n","- **Single time series split:** Involves dividing the time series considering as validation set a narrow period of time and as train set everything that happened before this period, in such a way as to best benefit from the trend in the short term.\n","\n","    ![single-split.png](https://drive.google.com/uc?id=1SODyQLolK4zn9lFGnNaqnMBZrHn3OsVn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get splitting parameters based on the choosen splitting method\n","splitting_info = parameters.get_splitting_params(SPLITTING_METHOD)\n","splitting_info"]},{"cell_type":"markdown","metadata":{},"source":["## Default\n","The train / validation set will be splitted based on the splitting method chosen so that the model performance can be seen without any tuning by using different features (normalized and non)"]},{"cell_type":"markdown","metadata":{},"source":["### Without normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"default\"\n","FEATURES_NORMALIZATION = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get default parameters\n","params = parameters.get_defaults_model_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using all the features\n","default_train_results_all_features, default_valid_results_all_features, default_train_pred_all_features, default_valid_pred_all_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_train_results_all_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_valid_results_all_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most relevant features\n","default_train_results_most_rel_features, default_valid_results_most_rel_features, default_train_pred_most_rel_features, default_valid_pred_most_rel_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_train_results_most_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_valid_results_most_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least relevant features\n","default_train_results_least_rel_features, default_valid_results_least_rel_features, default_train_pred_least_rel_features, default_valid_pred_least_rel_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_train_results_least_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_valid_results_least_rel_features"]},{"cell_type":"markdown","metadata":{},"source":["### With normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model and features type\n","MODEL_TYPE = \"default_norm\"\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using all the features\n","default_norm_train_results_all_features, default_norm_valid_results_all_features, default_norm_train_pred_all_features, default_norm_valid_pred_all_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_train_results_all_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_valid_results_all_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the most relevant features\n","default_norm_train_results_most_rel_features, default_norm_valid_results_most_rel_features, default_norm_train_pred_most_rel_features, default_norm_valid_pred_most_rel_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_train_results_most_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_valid_results_most_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions by using the least relevant features\n","default_norm_train_results_least_rel_features, default_norm_valid_results_least_rel_features, default_norm_train_pred_least_rel_features, default_norm_valid_pred_least_rel_features = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, ALL_FEATURES, ALL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_train_results_least_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_norm_valid_results_least_rel_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define model information and evaluators to show\n","model_info = ['Model', 'Type', 'Splitting', 'Features', 'Parameters']\n","evaluator_lst = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adjusted_R2', 'Time']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","default_comparison_lst = [default_valid_results_all_features, default_valid_results_most_rel_features, default_valid_results_least_rel_features, default_norm_valid_results_all_features, default_norm_valid_results_most_rel_features, default_norm_valid_results_least_rel_features]\n","\n","# Show the comparison table\n","default_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in default_comparison_lst])\n","default_comparison_lst_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the best default model results and predicitons\n","best_default_results = default_norm_valid_results_all_features\n","best_default_predictions = default_norm_valid_pred_all_features"]},{"cell_type":"markdown","metadata":{},"source":["## Tuned\n","Once the features and execution method are selected, the model will undergo hyperparameter tuning and cross validation to find the best configuration."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Using one of the multiple splitting methods to tune the model\n","SPLITTING_METHOD = \"walk_forward_splits\"\n","\n","# Get splitting parameters based on the choosen splitting method\n","splitting_info = parameters.get_splitting_params(SPLITTING_METHOD)\n","splitting_info"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Select the type of feature to be used\n","MODEL_TYPE = \"hyp_tuning\"\n","CHOSEN_FEATURES = ALL_FEATURES\n","CHOSEN_FEATURES_LABEL = ALL_FEATURES_NAME\n","FEATURES_NORMALIZATION = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get model grid parameters\n","params = parameters.get_model_grid_params(MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform hyperparameter tuning\n","hyp_res = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)\n","hyp_res"]},{"cell_type":"markdown","metadata":{},"source":["---\n","To select the best parameters to be used in the final model I assign a score to each value in the \"Parameters\" column based on the following criteria:\n","* Calculate the frequency of each unique value in the \"Parameters\" column.\n","* Normalize the frequencies to a scale of 0 to 1, where 1 represents the highest frequency.\n","* Calculate the split weight for each value in the \"Parameters\" column, where a higher split number corresponds to a higher weight.\n","* Normalize the split weights to a scale of 0 to 1, where 1 represents the highest split weight.\n","* Calculate the RMSE weight for each value in the \"Parameters\" column, where a lower RMSE value corresponds to a higher weight.\n","* Normalize the RMSE weights to a scale of 0 to 1, where 1 represents the highest RMSE weight.\n","\n","Then calculate the overall score for each value in the \"Parameters\" column by combining the normalized frequency, split weight, and RMSE weight and take into consideration the parameters that have the highest score."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Show parameters score\n","grouped_scores, best_params = parameters.choose_best_params(hyp_res)\n","grouped_scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print best parameters\n","print(f\"Best parameters: {best_params}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Cross validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"cross_val\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get tuned parameters\n","params = parameters.get_best_model_params(best_params, MODEL_NAME)\n","params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform cross validation\n","cv_train_result, cv_valid_result, cv_train_pred, cv_valid_pred = utilities.multiple_splits(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_train_result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_valid_result"]},{"cell_type":"markdown","metadata":{},"source":["Let's see if the situation has improved by validating the model with the newly found parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Using single split method to validate the model\n","SPLITTING_METHOD = \"single_split\"\n","\n","# Get splitting parameters based on the choosen splitting method\n","splitting_info = parameters.get_splitting_params(SPLITTING_METHOD)\n","splitting_info"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"tuned\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Validate the model with the best parameters\n","tuned_train_results, tuned_valid_results, tuned_train_pred, tuned_valid_pred = utilities.single_split(df, params, splitting_info, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, LEAST_REL_FEATURES, LEAST_REL_FEATURES_NAME, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tuned_train_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tuned_valid_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results to show\n","tuned_comparison_lst = [cv_valid_result, tuned_valid_results]\n","\n","# Show the comparison table\n","tuned_comparison_lst_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in tuned_comparison_lst])\n","tuned_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["## Trained\n","At this point it is possible to train the final model over the entire train / validation set in order to use it in the final step, that is, to make predictions about the data contained in the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_TYPE = \"final\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the final model on the whole train / validation set\n","final_train_results, final_model, final_train_pred = utilities.evaluate_trained_model(df, params, MODEL_NAME, MODEL_TYPE, FEATURES_NORMALIZATION, CHOSEN_FEATURES, CHOSEN_FEATURES_LABEL, FEATURES_LABEL, TARGET_LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["final_train_results"]},{"cell_type":"markdown","metadata":{},"source":["# Comparison table\n","Visualization of model performance at various stages of train / validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate final results into Pandas dataset\n","final_comparison_lst_df = pd.DataFrame(pd.concat([default_comparison_lst_df, tuned_comparison_lst_df, final_train_results], ignore_index=True))\n","final_comparison_lst_df"]},{"cell_type":"markdown","metadata":{},"source":["# Model accuracy\n","\n","Since predicting the price accurately is very difficult let's se how good the model is at predicting whether the price will go up or down.\n","\n","For each row in our predictions let's consider the actual market-price, next-market-price and our predicted next-market-price (prediction).\n","We compute whether the current prediction is correct (1) or not (0):\n","\n","$$\n","prediction\\_is\\_correct\n","=\n","\\begin{cases}\n","0 \\text{ if [(market-price > next-market-price) and (market-price < prediction)] or [(market-price < next-market-price) and (market-price > prediction)]} \\\\\n","1 \\text{ if [(market-price > next-market-price) and (market-price > prediction)] or [(market-price < next-market-price) and (market-price < prediction)]}\n","\\end{cases}\n","$$\n","\n","After that we count the number of correct prediction:\n","$$\n","correct\\_predictions\n","=\n","\\sum_{i=0}^{total\\_rows} prediction\\_is\\_correct\n","$$\n","\n","Finally we compute the percentage of accuracy of the model:\n","$$\n","\\\\\n","accuracy\n","=\n","(correct\\_predictions / total\\_rows)\n","* 100\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the pandas dataset to a PySpark dataset\n","best_default_pred_spark = spark.createDataFrame(best_default_predictions)\n","validated_pred_spark = spark.createDataFrame(tuned_valid_pred)\n","\n","# Compute model accuracy\n","default_accuracy = utilities.model_accuracy(best_default_pred_spark)\n","validated_accuracy = utilities.model_accuracy(validated_pred_spark)\n","\n","# Saving accuracy data into dataframe\n","accuracy_data = {\n","    'Model': MODEL_NAME,\n","    'Features': CHOSEN_FEATURES_LABEL,\n","    'Splitting': SPLITTING_METHOD,\n","    'Accuracy (default)': default_accuracy,\n","    'Accuracy (validated)': validated_accuracy\n","}\n","accuracy_data_df = pd.DataFrame(accuracy_data, index=['Model'])\n","\n","print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (default): {default_accuracy:.2f}%\")\n","print(f\"Percentage of correct predictions for {MODEL_NAME} with {CHOSEN_FEATURES_LABEL} and {SPLITTING_METHOD} (validated): {validated_accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate default and tuned results\n","default_tuned_results = [best_default_results, tuned_valid_results]\n","default_tuned_results_df = pd.concat([utilities.model_comparison(results, model_info, evaluator_lst) for results in default_tuned_results])\n","default_tuned_results_df"]},{"cell_type":"markdown","metadata":{},"source":["# Saving final results\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save all final comparison results\n","final_comparison_lst_df.to_csv(ALL_MODEL_RESULTS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save relevant results (default and tuned results)\n","default_tuned_results_df.to_csv(REL_MODEL_RESULTS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving accuracy results\n","accuracy_data_df.to_csv(MODEL_ACCURACY_RESULTS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Saving final model\n","final_model.write().overwrite().save(MAIN_DIR + '/models/' + MODEL_NAME)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"vscode":{"interpreter":{"hash":"303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"}}},"nbformat":4,"nbformat_minor":0}
