{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorsiDanilo/big-data-computing-project/blob/main/BDC_Project_Bitcoin_price_forecasting_(Model_preparation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e452Ydj7chNV"
      },
      "source": [
        "# Bitcoin price forecasting with PySpark\n",
        "## Big Data Computing final project - A.Y. 2022 - 2023\n",
        "Prof. Gabriele Tolomei\n",
        "\n",
        "MSc in Computer Science\n",
        "\n",
        "La Sapienza, University of Rome\n",
        "\n",
        "### Author\n",
        "Corsi Danilo - corsi.1742375@studenti.uniroma1.it\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFEbOtkdTI0"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The cryptocurrency Bitcoin has attracted the attention of many people in recent years. However, it's\n",
        "price fluctuation can be extremely unpredictable, which makes it difficult to predict when the right\n",
        "time to buy or sell this digital currency will be. In this context, forecasting Bitcoin prices can be a\n",
        "competitive advantage for investors and traders, as it could allow them to make informed decisions\n",
        "on the right time to enter or exit the market. In this project, I will analyze some machine learning\n",
        "techniques to understand, through the processing of historical data, how accurately the price of Bitcoin\n",
        "can be predicted and whether this can provide added value to cryptocurrency investors and traders.\n",
        "### Dataset\n",
        "I chose to use the following dataset from Kaggle Bitcoin Historical Dataset, more specifically those\n",
        "containing minute-by-minute updates of the Bitcoin price from 2017 to 2021 (period for which there\n",
        "were moments of high volatility but also a lot of price lateralisation). The columns (features) contained\n",
        "in it, in addition to the timestamp of each transaction, are the opening, closing, highest and lowest\n",
        "price and the corresponding trading volume in Bitcoin and Dollars.\n",
        "### Methods (TODO: da scegliere per bene)\n",
        "The methods I will test will be Linear Regression (simple and multiple) and Random Forest. Further\n",
        "comparisons with other classification models are planned in the course of development. Moreover, I\n",
        "would also like to try to understand what the differences are between these methods and the imple-\n",
        "mentation of a state-of-the-art neural network such as Long-Short Term Memory.\n",
        "### Evaluation framework (TODO: vedi quali usare in base ai paper/esempi e ai modelli utilizzati)\n",
        "As evaluation framework I will use R-square (R²), Mean Square Error (MSE) and Mean Absolute\n",
        "Error (MAE) to get a complete picture of the performance of the various models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtmWjQUVSvq2"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkQaY6VAf4v_"
      },
      "source": [
        "## Global Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L7-8QQvKf5CR"
      },
      "outputs": [],
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_DATASET_DIR = GDRIVE_DIR + \"/MyDrive/Computer_Science/BDC/project/datasets\"\n",
        "GDRIVE_DATASET = GDRIVE_DATASET_DIR + \"/output.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRW_JLjSvrA"
      },
      "source": [
        "## Install PySpark and related dependencies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si82CaUYSvrA",
        "outputId": "dbca218f-c7e2-4f58-80f5-bef34ade5836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=61a5f4b8901a9faa9579db18da8218f3d8711c0edb500592fab876c08b83bce6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1\n",
        "!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oP4l33G44Xj",
        "outputId": "016994b8-a2ef-4d3e-9370-4ef1a1b97bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting koalas\n",
            "  Downloading koalas-0.32.0-py3-none-any.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.2/593.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.10/dist-packages (from koalas) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from koalas) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.2->koalas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->koalas) (1.16.0)\n",
            "Installing collected packages: koalas\n",
            "Successfully installed koalas-0.32.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hyper\n",
            "  Downloading hyper-0.7.0-py2.py3-none-any.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<3.0,>=2.4 (from hyper)\n",
            "  Downloading h2-2.6.2-py2.py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<4.0,>=3.2 (from hyper)\n",
            "  Downloading hyperframe-3.2.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting hpack<4,>=2.2 (from h2<3.0,>=2.4->hyper)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Installing collected packages: hyperframe, hpack, h2, hyper\n",
            "Successfully installed h2-2.6.2 hpack-3.0.0 hyper-0.7.0 hyperframe-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install koalas\n",
        "!pip install hyper\n",
        "\n",
        "import collections.abc\n",
        "# hyper needs the four following aliases to be done manually.\n",
        "collections.Iterable = collections.abc.Iterable\n",
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableSet = collections.abc.MutableSet\n",
        "collections.MutableMapping = collections.abc.MutableMapping\n",
        "collections.Callable = collections.abc.Callable\n",
        "# now import hyper\n",
        "import hyper\n",
        "\n",
        "import databricks.koalas as ks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXTGTgzEqE3x"
      },
      "source": [
        "##  Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PX7xDYw4SvrB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvwgAvGSvrB"
      },
      "source": [
        "##  Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fhi5bmOeSvrB"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"BitcoinPriceForecasting\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWSfSKX6SvrB"
      },
      "source": [
        "##  Create <code>ngrok</code> tunnel to check the Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9luPyVNSvrB",
        "outputId": "dcace995-a46f-4096-a368-850c73184c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=a591bbf2cd79340bd6f50e04a930fed695431c3359a77e65064c42ae19fd7cfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKGaSV0F7Sh5",
        "outputId": "f4557217-ea39-4c47-8499-5a6191b4737e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Saving authtoken \n",
        "!ngrok authtoken 2PKOO1E6Ghw65dpEG4QNSVzu9PY_GufsrTiussGmBxF664RD "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROQzVuNu7ZE_",
        "outputId": "66333ce0-6d8d-408a-8758-48fba57ea47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-07T09:29:27+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwwPauep7bm4",
        "outputId": "50b7f870-85d2-47ed-9879-4767b6f5ee7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the Spark Web UI console, please click on the following link to the ngrok tunnel \"https://6774-34-90-39-46.ngrok-free.app\" -> \"http://127.0.0.1:4050\"\n"
          ]
        }
      ],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9prfkJSvrB"
      },
      "source": [
        "##  Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "au2-MqC-SvrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65dffe0-bbdd-4ae5-bd1e-93f7e623a7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsgE-WfSvrB"
      },
      "source": [
        "##  Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8X312aarSvrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "858fb22a-7eaf-4bd1-f231-c7b93c14198f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8b54491510>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7ee4a3276cac:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BitcoinPriceForecasting</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GEvgsXRkSvrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cfce73-bd38-49e6-f996-4e4b2816c806"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.port', '42139'),\n",
              " ('spark.driver.memory', '45G'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.app.submitTime', '1686130151057'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.name', 'BitcoinPriceForecasting'),\n",
              " ('spark.driver.host', '7ee4a3276cac'),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.app.startTime', '1686130151365'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.app.id', 'local-1686130153609'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpaf8RMIcP4a"
      },
      "source": [
        "# **Model preparation**\n",
        "\n",
        "Prepara i dati: Assicurati che il tuo dataset sia in un formato adatto per l'addestramento del modello. Dovresti avere una colonna di etichette di output (variabile di risposta) e le features (variabili indipendenti) in colonne separate.\n",
        "\n",
        "Crea un VectorAssembler: Un VectorAssembler è utilizzato per combinare le features in una singola colonna vettoriale. Questo passaggio è necessario poiché PySpark richiede che le features siano in un unico vettore per l'addestramento del modello Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Agk8Cum2OdQz"
      },
      "outputs": [],
      "source": [
        "# load dataset into pyspark dataframe objects\n",
        "df = spark.read.load(GDRIVE_DATASET, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "h5-oVeurnfGf"
      },
      "outputs": [],
      "source": [
        "def model_preparation(dataframe):  \n",
        "  from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "  assembler = VectorAssembler(\n",
        "      inputCols=[\"close\"],  # Colonna del prezzo di chiusura\n",
        "      outputCol=\"features\"  # Colonna vettoriale risultante\n",
        "  )\n",
        "\n",
        "  dataframe = assembler.transform(dataframe)\n",
        "\n",
        "  from pyspark.sql.functions import date_format, to_timestamp\n",
        "\n",
        "  # transform date column into string\n",
        "  dataframe = dataframe.withColumn(\"date_str\", date_format(to_timestamp(\"date\", \"yyyy-MM-dd HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        " # encode the date to a column of label indicies\n",
        "  from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "  label_stringIdx = StringIndexer(inputCol = 'date_str', outputCol = 'labelIndex')\n",
        "  dataframe = label_stringIdx.fit(dataframe).transform(dataframe)\n",
        "\n",
        "  # dividi il dataset in train set e test set\n",
        "  from pyspark.sql.functions import percent_rank\n",
        "  from pyspark.sql import Window\n",
        "\n",
        "  dataframe = dataframe.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"date_str\")))\n",
        "  train_df = dataframe.where(\"rank <= .8\").drop(\"rank\", \"date_str\")\n",
        "  test_df = dataframe.where(\"rank > .8\").drop(\"rank\", \"date_str\")\n",
        "\n",
        "  print(\"The shape of the train set is {:d} rows by {:d} columns\".format(train_df.count(), len(train_df.columns)))\n",
        "  train_df.printSchema()\n",
        "  train_df.show(5)\t\n",
        "\n",
        "  print(\"The shape of the test set is {:d} rows by {:d} columns\".format(test_df.count(), len(test_df.columns)))\n",
        "  test_df.printSchema()\n",
        "  test_df.show(5)\t\n",
        "\n",
        "  return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = model_preparation(df)"
      ],
      "metadata": {
        "id": "Fe1u7N0RNTc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJPBT9vs_zRZ"
      },
      "outputs": [],
      "source": [
        "def compute_avg_train_test(dataframe):\n",
        "  dataframe = dataframe.drop(\"features\", \"labelIndex\")\n",
        "  \n",
        "  dataframe = dataframe.withColumn(\"date\", date_format(dataframe.date, \"yyyy-MM-dd\")).groupBy(\"date\").agg(\n",
        "      avg(\"close\").alias(\"avg_close\")\n",
        "  ).sort(\"date\")\n",
        "\n",
        "  dataframe = dataframe.withColumn(\"avg_close\", round(dataframe[\"avg_close\"], 2))\n",
        "\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGlxw9wPW77V"
      },
      "outputs": [],
      "source": [
        "def show_avg_train_test(train_df, test_df):\n",
        "  avg_train_df_pandas = compute_avg_train_test(train_df).toPandas()\n",
        "  avg_test_df_pandas = compute_avg_train_test(test_df).toPandas()\n",
        "\n",
        "  trace1 = go.Scatter(\n",
        "      x = avg_train_df_pandas['date'],\n",
        "      y = avg_train_df_pandas['avg_close'].astype(float),\n",
        "      mode = 'lines',\n",
        "      name = 'Train set'\n",
        "  )\n",
        "\n",
        "  trace2 = go.Scatter(\n",
        "      x = avg_test_df_pandas['date'],\n",
        "      y = avg_test_df_pandas['avg_close'].astype(float),\n",
        "      mode = 'lines',\n",
        "      name = 'Test set'\n",
        "  )\n",
        "  \n",
        "  layout = dict(\n",
        "      title='Train and Test set with the Slider ',\n",
        "      xaxis=dict(\n",
        "          rangeselector=dict(\n",
        "              buttons=list([\n",
        "                  #change the count to desired amount of months.\n",
        "                  dict(count=1,\n",
        "                      label='1m',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=6,\n",
        "                      label='6m',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=12,\n",
        "                      label='1y',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(count=36,\n",
        "                      label='3y',\n",
        "                      step='month',\n",
        "                      stepmode='backward'),\n",
        "                  dict(step='all')\n",
        "              ])\n",
        "          ),\n",
        "          rangeslider=dict(\n",
        "              visible = True\n",
        "          ),\n",
        "          type='date'\n",
        "      )\n",
        "  )\n",
        "\n",
        "  data = [trace1,trace2]\n",
        "  fig = dict(data=data, layout=layout)\n",
        "  iplot(fig, filename = \"Train and Test set  with Rangeslider\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uuZJSHkRY12"
      },
      "outputs": [],
      "source": [
        "show_avg_train_test(train_df, test_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e452Ydj7chNV",
        "zK1eMlqXWioB"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "303fa613b6f3e1efefe7bb28036e305e1021fa6bdb083a5f9fd57f9d9bbad8eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}